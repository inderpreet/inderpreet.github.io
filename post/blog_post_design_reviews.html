<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>What Experience Looks Like in Design Reviews | Layered Systems</title><meta name="title" content="What Experience Looks Like in Design Reviews | Layered Systems"/><meta name="description" content="Inexperienced engineers talk a lot in design reviews. Experienced ones often don&#x27;t. This is frequently misinterpreted as disengagement. It isn&#x27;t. It&#x27;s pattern recognition at work."/><meta name="keywords" content="design, experience, reviews, engineering"/><meta name="author" content="Inderpreet Singh"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:type" content="website"/><meta property="og:url" content="https://inderpreet.github.io"/><meta property="og:title" content="What Experience Looks Like in Design Reviews | Layered Systems"/><meta property="og:description" content="Inexperienced engineers talk a lot in design reviews. Experienced ones often don&#x27;t. This is frequently misinterpreted as disengagement. It isn&#x27;t. It&#x27;s pattern recognition at work."/><meta property="og:image" content="/og-image.jpg"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:url" content="https://inderpreet.github.io"/><meta property="twitter:title" content="What Experience Looks Like in Design Reviews | Layered Systems"/><meta property="twitter:description" content="Inexperienced engineers talk a lot in design reviews. Experienced ones often don&#x27;t. This is frequently misinterpreted as disengagement. It isn&#x27;t. It&#x27;s pattern recognition at work."/><meta property="twitter:image" content="/og-image.jpg"/><meta name="robots" content="index, follow"/><meta name="language" content="English"/><link rel="canonical" href="https://inderpreet.github.io"/><meta name="next-head-count" content="20"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="theme-color" content="#000000"/><link rel="preload" href="/_next/static/css/1d29b84bfc5354c8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1d29b84bfc5354c8.css" data-n-g=""/><link rel="preload" href="/_next/static/css/0fdd858d31bddfca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0fdd858d31bddfca.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-639aa3d8bfe7ee72.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-d979f5fb3aa68004.js" defer=""></script><script src="/_next/static/chunks/pages/_app-20693b9d02de5813.js" defer=""></script><script src="/_next/static/chunks/666-58cfeb78f799b3ee.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-388991efceb2e5b5.js" defer=""></script><script src="/_next/static/phl_WOpLyX2gCvGrSm1hh/_buildManifest.js" defer=""></script><script src="/_next/static/phl_WOpLyX2gCvGrSm1hh/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_container__l2gjk"><header class="Layout_header__8XhYm"><div class="Layout_headerContent__06wDW"><nav class="Layout_nav__qOLUE "><a href="/">Home</a><a href="/about">About</a><div class="Layout_dropdown__z3jRI"><button class="Layout_dropdownToggle__WdMTA" aria-expanded="false" aria-haspopup="true">Resources<svg width="12" height="12" viewBox="0 0 12 12" fill="currentColor" style="margin-left:0.25rem;transform:rotate(0deg);transition:transform 0.2s"><path d="M2 4l4 4 4-4" stroke="currentColor" stroke-width="2" fill="none"></path></svg></button><div class="Layout_dropdownMenu__zEIeU "><a href="/monitor" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üéõÔ∏è</span><div><div class="Layout_dropdownItemTitle__VcVEY">System Monitor</div><div class="Layout_dropdownItemDesc__DjI_j">Real-time dashboard</div></div></a><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üíª</span><div><div class="Layout_dropdownItemTitle__VcVEY">GitHub</div><div class="Layout_dropdownItemDesc__DjI_j">View my repositories</div></div></a><div class="Layout_dropdownDivider__ySCu0"></div><a href="/docs" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üìö</span><div><div class="Layout_dropdownItemTitle__VcVEY">Documentation</div><div class="Layout_dropdownItemDesc__DjI_j">Guides &amp; tutorials</div></div></a><a href="#" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üöÄ</span><div><div class="Layout_dropdownItemTitle__VcVEY">Projects</div><div class="Layout_dropdownItemDesc__DjI_j">Explore my work</div></div></a></div></div></nav><div class="Layout_logoSection__xngcy"><h1 class="Layout_logo__Yfd0y"><a href="/">Layered Systems</a></h1><p class="Layout_tagline__pRBfO">Engineering judgment, one layer at a time.</p></div><div class="Layout_socialIcons__8_CJc"><button class="ThemeToggle_themeToggle__Lxt_p" aria-label="Toggle theme"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="5"></circle></svg></button><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://twitter.com/ip_v1" target="_blank" rel="noopener noreferrer" aria-label="Twitter"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"></path></svg></a><a href="https://linkedin.com/in/inderpreets" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div><button class="Layout_mobileMenuButton__nLMRy" aria-label="Toggle menu"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></header><main class="Layout_main__BqQ1G"><div class="Post_postContainer__1nAIq"><article class="Post_post__1KH4p"><div class="Post_postHeader__VS2xC"><div class="Post_authorSection__JpQiF"><div class="Post_avatarLarge__YO5z5">IS</div><div><h3 class="Post_authorName__3jNxf">Inderpreet Singh</h3><time class="Post_date__YGL_Q">2025-12-21</time></div></div></div><div class="Post_featuredImage__CRWk2"><img src="/images/posts/design-review2.png" alt="What Experience Looks Like in Design Reviews"/></div><div class="Post_postContent__EmxdX"><h1 class="Post_title__pen_7">What Experience Looks Like in Design Reviews</h1><div class="Post_tags__PFaKf"><span class="Post_tag__jmLTM">#<!-- -->design</span><span class="Post_tag__jmLTM">#<!-- -->experience</span><span class="Post_tag__jmLTM">#<!-- -->reviews</span><span class="Post_tag__jmLTM">#<!-- -->engineering</span></div><div class="Post_content__aWM2C"><p>Inexperienced engineers tend to talk a lot in design reviews. Experienced ones often don&#39;t.</p>
<p>This is frequently misinterpreted as disengagement. It isn&#39;t. It&#39;s pattern recognition at work.</p>
<p>A junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There&#39;s error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.</p>
<p>The team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.</p>
<p>Thirty minutes in, the senior controls engineer‚Äîwho&#39;s been silent the entire time‚Äîasks: &quot;What happens when the press cycles faster than the sensor settling time?&quot;</p>
<p>Pause.</p>
<p>&quot;The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.&quot;</p>
<p>&quot;What happens when production pushes it to 75 strokes per minute to meet a deadline?&quot;</p>
<p>Longer pause.</p>
<p>That question wasn&#39;t on any slide. It wasn&#39;t in the requirements. But everyone in the room who&#39;s been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between &quot;rated for&quot; and &quot;tested at.&quot;</p>
<p>Experience changes what you listen for.</p>
<h2>The Questions That Matter Change Over Time</h2>
<p>Early in a career, design reviews focus on how something works.</p>
<p>Is the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?</p>
<p>These are legitimate questions. They matter. But with experience, attention shifts to different territory:</p>
<ul>
<li>What assumptions are being made about the environment?</li>
<li>Which constraints are fixed, and which are imagined?</li>
<li>What happens when this fails at the worst possible time?</li>
<li>Who will be awake at 3 AM when it does?</li>
<li>How will we know it&#39;s failing before it&#39;s catastrophic?</li>
</ul>
<p>These questions don&#39;t show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.</p>
<p>I remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.</p>
<p>What I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values‚Äîand a tank overheated. The system worked exactly as designed. The design just didn&#39;t account for sensors that fail by lying rather than going silent.</p>
<p>An experienced engineer would have asked: &quot;What does a failing sensor look like to this algorithm?&quot; Not because they&#39;re smarter, but because they&#39;ve seen that failure mode before. Probably more than once.</p>
<h2>Silence Is a Signal</h2>
<p>Experienced engineers are often quiet during the early part of a review.</p>
<p>They let the design present itself. They listen for what is emphasized‚Äîand what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?</p>
<p>When they finally speak, it is rarely to propose an alternative design. It is to test the edges:</p>
<p>&quot;What happens if Modbus requests start taking 200ms instead of 50ms?&quot;</p>
<p>&quot;How do we know this component is failing versus just slow?&quot;</p>
<p>&quot;Who owns the configuration updates when this is in production?&quot;</p>
<p>&quot;What&#39;s the plan when this needs to change and you&#39;re not here?&quot;</p>
<p>These are not clever questions. They are uncomfortable ones. They don&#39;t have satisfying technical answers because they&#39;re not really technical questions‚Äîthey&#39;re questions about risk, ownership, and operational reality.</p>
<p>The inexperienced reviewer says: &quot;Have you considered using CRC32 instead of CRC16 for better error detection?&quot;</p>
<p>The experienced reviewer says: &quot;What happens when the checksum passes but the data is still wrong?&quot;</p>
<p>Both questions are about data integrity. Only one is about what actually happens in production.</p>
<h2>Experience Recognizes Deferred Decisions</h2>
<p>Many designs appear solid because they defer hard decisions:</p>
<ul>
<li>Retry logic described as &quot;exponential backoff with appropriate limits&quot; but never defined</li>
<li>Operational ownership assumed but not assigned</li>
<li>Failure handling described vaguely as &quot;graceful degradation&quot;</li>
<li>Scaling concerns waved away with &quot;we&#39;ll monitor and adjust&quot;</li>
<li>Integration complexity acknowledged but postponed to &quot;phase two&quot;</li>
</ul>
<p>Experience recognizes deferral instantly‚Äînot because deferral is always wrong, but because deferred decisions always come back with interest.</p>
<p>&quot;We&#39;ll tune the timeouts in production&quot; sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.</p>
<p>&quot;We&#39;ll add more detailed logging if needed&quot; sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.</p>
<p>&quot;We&#39;ll clarify ownership during deployment&quot; sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.</p>
<p>Production is very patient about collecting on these debts.</p>
<p>I watched a team defer a decision about how to handle partial sensor array failures. &quot;We&#39;ll see how it behaves in production and tune the algorithm.&quot; Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn&#39;t designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.</p>
<p>The cost of deferring that decision was higher than the cost of making it wrong during design would have been.</p>
<h2>The Absence of Overconfidence</h2>
<p>One of the clearest markers of experience is restraint.</p>
<p>Experienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will‚Äîand the ways it might not.</p>
<p>This is sometimes mistaken for pessimism. It isn&#39;t. It is respect for complexity.</p>
<p>Confidence says: &quot;This will handle sensor failures.&quot;</p>
<p>Experience says: &quot;This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We&#39;ve added bounds checking for the first case. The other two will require operational awareness.&quot;</p>
<p>Confidence says: &quot;This will scale to 100 nodes.&quot;</p>
<p>Experience says: &quot;This will scale to 100 nodes if network latency stays below 50ms and nodes don&#39;t join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We&#39;ll see it as increased jitter in the timing measurements.&quot;</p>
<p>The difference is not about being negative. It&#39;s about being specific about what has been designed for and what has been assumed away.</p>
<p>When someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn&#39;t understood the problem space well enough to see the edges.</p>
<p>Most problems are not trivial.</p>
<h2>Ownership Reveals Maturity</h2>
<p>Designs that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.</p>
<p>Experienced reviewers push on this immediately.</p>
<p>Who owns this in production? Who gets paged when it misbehaves? Who can say &quot;no&quot; when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?</p>
<p>If ownership is unclear, the design is incomplete‚Äîno matter how elegant the code.</p>
<p>I&#39;ve seen systems designed by committee where every component had a different owner, and the interfaces between components were &quot;shared responsibility.&quot; Shared responsibility is another way of saying &quot;no one is responsible.&quot;</p>
<p>When that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system&#39;s behavior was the outcome of those negotiations, not of any coherent design intent.</p>
<p>The experienced engineer asks: &quot;Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?&quot;</p>
<p>If the answer is &quot;well, it depends on which part is having issues,&quot; the ownership model is broken.</p>
<p>Ownership isn&#39;t about credit. It&#39;s about who carries the mental model of the system&#39;s actual behavior‚Äînot its documented behavior, its actual behavior‚Äîand has the authority to act on that understanding.</p>
<h2>Experience Is Often Mistaken for Negativity</h2>
<p>The most experienced person in the room is often labeled &quot;difficult&quot; at least once.</p>
<p>They ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn&#39;t include time for learning what was gotten wrong.</p>
<p>This is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.</p>
<p>What they are really doing is defending the future team from the present team&#39;s optimism.</p>
<p>That is rarely popular in the moment. It is deeply appreciated later‚Äîusually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn&#39;t want to answer during the review has become an urgent problem with no good solutions.</p>
<p>I&#39;ve been that &quot;difficult&quot; person. I&#39;ve asked questions that made presenters defensive. I&#39;ve pointed out gaps that felt like criticisms. I&#39;ve watched the energy in the room shift from enthusiasm to frustration.</p>
<p>And I&#39;ve also gotten emails, months later, from those same presenters: &quot;Remember when you asked about [that thing]? It happened. We were ready because we&#39;d thought about it. Thank you.&quot;</p>
<p>The experienced reviewer is not trying to stop the design. They are trying to make it survivable.</p>
<h2>Why Design Reviews Fail</h2>
<p>Most failed design reviews fail quietly.</p>
<p>They approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.</p>
<p>And then production teaches the lesson that the review avoided.</p>
<p>Design reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.</p>
<p>Good design reviews do not prevent failure. They prevent surprise.</p>
<p>A good review leaves everyone with a shared understanding of:</p>
<ul>
<li>What has been designed for</li>
<li>What has been assumed away</li>
<li>Where the edges are</li>
<li>Who owns what happens at those edges</li>
</ul>
<p>It does not guarantee success. It makes failure less catastrophic and more recoverable.</p>
<h2>A Final Thought</h2>
<p>Experience does not make you smarter in design reviews. It makes you more selective.</p>
<p>You stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you&#39;re gone.</p>
<p>That shift is subtle. And once you see it, you can&#39;t unsee it.</p>
<p>You start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions‚Äînot because you&#39;re difficult, but because you&#39;ve seen what happens when no one asks them.</p>
<p>And you realize that the best design reviews are not the ones where everyone agrees.</p>
<p>They are the ones where everyone leaves slightly uncomfortable, but with clarity about why.</p>
</div></div><div class="Post_backLink__n_9IU"><a href="/">‚Üê Back to Feed</a></div></article></div></main><footer class="Layout_footer__3v8iv"><p>¬© 2025 Layered Systems - Engineering judgment, one layer at a time.</p><div class="Layout_socialLinks__WZeQy"><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer">GitHub</a><a href="https://twitter.com/ip_v1" target="_blank" rel="noopener noreferrer">Twitter</a><a href="https://instagram.com/ip_v1" target="_blank" rel="noopener noreferrer">Instagram</a><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer">LinkedIn</a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"blog_post_design_reviews","title":"What Experience Looks Like in Design Reviews","date":"2025-12-21","excerpt":"Inexperienced engineers talk a lot in design reviews. Experienced ones often don't. This is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/design-review2.png","tags":["design","experience","reviews","engineering"],"likes":0,"comments":0,"content":"\u003cp\u003eInexperienced engineers tend to talk a lot in design reviews. Experienced ones often don\u0026#39;t.\u003c/p\u003e\n\u003cp\u003eThis is frequently misinterpreted as disengagement. It isn\u0026#39;t. It\u0026#39;s pattern recognition at work.\u003c/p\u003e\n\u003cp\u003eA junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There\u0026#39;s error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.\u003c/p\u003e\n\u003cp\u003eThe team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.\u003c/p\u003e\n\u003cp\u003eThirty minutes in, the senior controls engineer‚Äîwho\u0026#39;s been silent the entire time‚Äîasks: \u0026quot;What happens when the press cycles faster than the sensor settling time?\u0026quot;\u003c/p\u003e\n\u003cp\u003ePause.\u003c/p\u003e\n\u003cp\u003e\u0026quot;The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;What happens when production pushes it to 75 strokes per minute to meet a deadline?\u0026quot;\u003c/p\u003e\n\u003cp\u003eLonger pause.\u003c/p\u003e\n\u003cp\u003eThat question wasn\u0026#39;t on any slide. It wasn\u0026#39;t in the requirements. But everyone in the room who\u0026#39;s been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between \u0026quot;rated for\u0026quot; and \u0026quot;tested at.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience changes what you listen for.\u003c/p\u003e\n\u003ch2\u003eThe Questions That Matter Change Over Time\u003c/h2\u003e\n\u003cp\u003eEarly in a career, design reviews focus on how something works.\u003c/p\u003e\n\u003cp\u003eIs the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?\u003c/p\u003e\n\u003cp\u003eThese are legitimate questions. They matter. But with experience, attention shifts to different territory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat assumptions are being made about the environment?\u003c/li\u003e\n\u003cli\u003eWhich constraints are fixed, and which are imagined?\u003c/li\u003e\n\u003cli\u003eWhat happens when this fails at the worst possible time?\u003c/li\u003e\n\u003cli\u003eWho will be awake at 3 AM when it does?\u003c/li\u003e\n\u003cli\u003eHow will we know it\u0026#39;s failing before it\u0026#39;s catastrophic?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese questions don\u0026#39;t show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.\u003c/p\u003e\n\u003cp\u003eI remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.\u003c/p\u003e\n\u003cp\u003eWhat I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values‚Äîand a tank overheated. The system worked exactly as designed. The design just didn\u0026#39;t account for sensors that fail by lying rather than going silent.\u003c/p\u003e\n\u003cp\u003eAn experienced engineer would have asked: \u0026quot;What does a failing sensor look like to this algorithm?\u0026quot; Not because they\u0026#39;re smarter, but because they\u0026#39;ve seen that failure mode before. Probably more than once.\u003c/p\u003e\n\u003ch2\u003eSilence Is a Signal\u003c/h2\u003e\n\u003cp\u003eExperienced engineers are often quiet during the early part of a review.\u003c/p\u003e\n\u003cp\u003eThey let the design present itself. They listen for what is emphasized‚Äîand what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?\u003c/p\u003e\n\u003cp\u003eWhen they finally speak, it is rarely to propose an alternative design. It is to test the edges:\u003c/p\u003e\n\u003cp\u003e\u0026quot;What happens if Modbus requests start taking 200ms instead of 50ms?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;How do we know this component is failing versus just slow?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;Who owns the configuration updates when this is in production?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;What\u0026#39;s the plan when this needs to change and you\u0026#39;re not here?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThese are not clever questions. They are uncomfortable ones. They don\u0026#39;t have satisfying technical answers because they\u0026#39;re not really technical questions‚Äîthey\u0026#39;re questions about risk, ownership, and operational reality.\u003c/p\u003e\n\u003cp\u003eThe inexperienced reviewer says: \u0026quot;Have you considered using CRC32 instead of CRC16 for better error detection?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe experienced reviewer says: \u0026quot;What happens when the checksum passes but the data is still wrong?\u0026quot;\u003c/p\u003e\n\u003cp\u003eBoth questions are about data integrity. Only one is about what actually happens in production.\u003c/p\u003e\n\u003ch2\u003eExperience Recognizes Deferred Decisions\u003c/h2\u003e\n\u003cp\u003eMany designs appear solid because they defer hard decisions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRetry logic described as \u0026quot;exponential backoff with appropriate limits\u0026quot; but never defined\u003c/li\u003e\n\u003cli\u003eOperational ownership assumed but not assigned\u003c/li\u003e\n\u003cli\u003eFailure handling described vaguely as \u0026quot;graceful degradation\u0026quot;\u003c/li\u003e\n\u003cli\u003eScaling concerns waved away with \u0026quot;we\u0026#39;ll monitor and adjust\u0026quot;\u003c/li\u003e\n\u003cli\u003eIntegration complexity acknowledged but postponed to \u0026quot;phase two\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExperience recognizes deferral instantly‚Äînot because deferral is always wrong, but because deferred decisions always come back with interest.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll tune the timeouts in production\u0026quot; sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll add more detailed logging if needed\u0026quot; sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll clarify ownership during deployment\u0026quot; sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.\u003c/p\u003e\n\u003cp\u003eProduction is very patient about collecting on these debts.\u003c/p\u003e\n\u003cp\u003eI watched a team defer a decision about how to handle partial sensor array failures. \u0026quot;We\u0026#39;ll see how it behaves in production and tune the algorithm.\u0026quot; Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn\u0026#39;t designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.\u003c/p\u003e\n\u003cp\u003eThe cost of deferring that decision was higher than the cost of making it wrong during design would have been.\u003c/p\u003e\n\u003ch2\u003eThe Absence of Overconfidence\u003c/h2\u003e\n\u003cp\u003eOne of the clearest markers of experience is restraint.\u003c/p\u003e\n\u003cp\u003eExperienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will‚Äîand the ways it might not.\u003c/p\u003e\n\u003cp\u003eThis is sometimes mistaken for pessimism. It isn\u0026#39;t. It is respect for complexity.\u003c/p\u003e\n\u003cp\u003eConfidence says: \u0026quot;This will handle sensor failures.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience says: \u0026quot;This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We\u0026#39;ve added bounds checking for the first case. The other two will require operational awareness.\u0026quot;\u003c/p\u003e\n\u003cp\u003eConfidence says: \u0026quot;This will scale to 100 nodes.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience says: \u0026quot;This will scale to 100 nodes if network latency stays below 50ms and nodes don\u0026#39;t join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We\u0026#39;ll see it as increased jitter in the timing measurements.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe difference is not about being negative. It\u0026#39;s about being specific about what has been designed for and what has been assumed away.\u003c/p\u003e\n\u003cp\u003eWhen someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn\u0026#39;t understood the problem space well enough to see the edges.\u003c/p\u003e\n\u003cp\u003eMost problems are not trivial.\u003c/p\u003e\n\u003ch2\u003eOwnership Reveals Maturity\u003c/h2\u003e\n\u003cp\u003eDesigns that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.\u003c/p\u003e\n\u003cp\u003eExperienced reviewers push on this immediately.\u003c/p\u003e\n\u003cp\u003eWho owns this in production? Who gets paged when it misbehaves? Who can say \u0026quot;no\u0026quot; when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?\u003c/p\u003e\n\u003cp\u003eIf ownership is unclear, the design is incomplete‚Äîno matter how elegant the code.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve seen systems designed by committee where every component had a different owner, and the interfaces between components were \u0026quot;shared responsibility.\u0026quot; Shared responsibility is another way of saying \u0026quot;no one is responsible.\u0026quot;\u003c/p\u003e\n\u003cp\u003eWhen that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system\u0026#39;s behavior was the outcome of those negotiations, not of any coherent design intent.\u003c/p\u003e\n\u003cp\u003eThe experienced engineer asks: \u0026quot;Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?\u0026quot;\u003c/p\u003e\n\u003cp\u003eIf the answer is \u0026quot;well, it depends on which part is having issues,\u0026quot; the ownership model is broken.\u003c/p\u003e\n\u003cp\u003eOwnership isn\u0026#39;t about credit. It\u0026#39;s about who carries the mental model of the system\u0026#39;s actual behavior‚Äînot its documented behavior, its actual behavior‚Äîand has the authority to act on that understanding.\u003c/p\u003e\n\u003ch2\u003eExperience Is Often Mistaken for Negativity\u003c/h2\u003e\n\u003cp\u003eThe most experienced person in the room is often labeled \u0026quot;difficult\u0026quot; at least once.\u003c/p\u003e\n\u003cp\u003eThey ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn\u0026#39;t include time for learning what was gotten wrong.\u003c/p\u003e\n\u003cp\u003eThis is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.\u003c/p\u003e\n\u003cp\u003eWhat they are really doing is defending the future team from the present team\u0026#39;s optimism.\u003c/p\u003e\n\u003cp\u003eThat is rarely popular in the moment. It is deeply appreciated later‚Äîusually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn\u0026#39;t want to answer during the review has become an urgent problem with no good solutions.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve been that \u0026quot;difficult\u0026quot; person. I\u0026#39;ve asked questions that made presenters defensive. I\u0026#39;ve pointed out gaps that felt like criticisms. I\u0026#39;ve watched the energy in the room shift from enthusiasm to frustration.\u003c/p\u003e\n\u003cp\u003eAnd I\u0026#39;ve also gotten emails, months later, from those same presenters: \u0026quot;Remember when you asked about [that thing]? It happened. We were ready because we\u0026#39;d thought about it. Thank you.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe experienced reviewer is not trying to stop the design. They are trying to make it survivable.\u003c/p\u003e\n\u003ch2\u003eWhy Design Reviews Fail\u003c/h2\u003e\n\u003cp\u003eMost failed design reviews fail quietly.\u003c/p\u003e\n\u003cp\u003eThey approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.\u003c/p\u003e\n\u003cp\u003eAnd then production teaches the lesson that the review avoided.\u003c/p\u003e\n\u003cp\u003eDesign reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.\u003c/p\u003e\n\u003cp\u003eGood design reviews do not prevent failure. They prevent surprise.\u003c/p\u003e\n\u003cp\u003eA good review leaves everyone with a shared understanding of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat has been designed for\u003c/li\u003e\n\u003cli\u003eWhat has been assumed away\u003c/li\u003e\n\u003cli\u003eWhere the edges are\u003c/li\u003e\n\u003cli\u003eWho owns what happens at those edges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt does not guarantee success. It makes failure less catastrophic and more recoverable.\u003c/p\u003e\n\u003ch2\u003eA Final Thought\u003c/h2\u003e\n\u003cp\u003eExperience does not make you smarter in design reviews. It makes you more selective.\u003c/p\u003e\n\u003cp\u003eYou stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you\u0026#39;re gone.\u003c/p\u003e\n\u003cp\u003eThat shift is subtle. And once you see it, you can\u0026#39;t unsee it.\u003c/p\u003e\n\u003cp\u003eYou start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions‚Äînot because you\u0026#39;re difficult, but because you\u0026#39;ve seen what happens when no one asks them.\u003c/p\u003e\n\u003cp\u003eAnd you realize that the best design reviews are not the ones where everyone agrees.\u003c/p\u003e\n\u003cp\u003eThey are the ones where everyone leaves slightly uncomfortable, but with clarity about why.\u003c/p\u003e\n","rawContent":"\nInexperienced engineers tend to talk a lot in design reviews. Experienced ones often don't.\n\nThis is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.\n\nA junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There's error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.\n\nThe team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.\n\nThirty minutes in, the senior controls engineer‚Äîwho's been silent the entire time‚Äîasks: \"What happens when the press cycles faster than the sensor settling time?\"\n\nPause.\n\n\"The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.\"\n\n\"What happens when production pushes it to 75 strokes per minute to meet a deadline?\"\n\nLonger pause.\n\nThat question wasn't on any slide. It wasn't in the requirements. But everyone in the room who's been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between \"rated for\" and \"tested at.\"\n\nExperience changes what you listen for.\n\n## The Questions That Matter Change Over Time\n\nEarly in a career, design reviews focus on how something works.\n\nIs the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?\n\nThese are legitimate questions. They matter. But with experience, attention shifts to different territory:\n\n- What assumptions are being made about the environment?\n- Which constraints are fixed, and which are imagined?\n- What happens when this fails at the worst possible time?\n- Who will be awake at 3 AM when it does?\n- How will we know it's failing before it's catastrophic?\n\nThese questions don't show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.\n\nI remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.\n\nWhat I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values‚Äîand a tank overheated. The system worked exactly as designed. The design just didn't account for sensors that fail by lying rather than going silent.\n\nAn experienced engineer would have asked: \"What does a failing sensor look like to this algorithm?\" Not because they're smarter, but because they've seen that failure mode before. Probably more than once.\n\n## Silence Is a Signal\n\nExperienced engineers are often quiet during the early part of a review.\n\nThey let the design present itself. They listen for what is emphasized‚Äîand what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?\n\nWhen they finally speak, it is rarely to propose an alternative design. It is to test the edges:\n\n\"What happens if Modbus requests start taking 200ms instead of 50ms?\"\n\n\"How do we know this component is failing versus just slow?\"\n\n\"Who owns the configuration updates when this is in production?\"\n\n\"What's the plan when this needs to change and you're not here?\"\n\nThese are not clever questions. They are uncomfortable ones. They don't have satisfying technical answers because they're not really technical questions‚Äîthey're questions about risk, ownership, and operational reality.\n\nThe inexperienced reviewer says: \"Have you considered using CRC32 instead of CRC16 for better error detection?\"\n\nThe experienced reviewer says: \"What happens when the checksum passes but the data is still wrong?\"\n\nBoth questions are about data integrity. Only one is about what actually happens in production.\n\n## Experience Recognizes Deferred Decisions\n\nMany designs appear solid because they defer hard decisions:\n\n- Retry logic described as \"exponential backoff with appropriate limits\" but never defined\n- Operational ownership assumed but not assigned\n- Failure handling described vaguely as \"graceful degradation\"\n- Scaling concerns waved away with \"we'll monitor and adjust\"\n- Integration complexity acknowledged but postponed to \"phase two\"\n\nExperience recognizes deferral instantly‚Äînot because deferral is always wrong, but because deferred decisions always come back with interest.\n\n\"We'll tune the timeouts in production\" sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.\n\n\"We'll add more detailed logging if needed\" sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.\n\n\"We'll clarify ownership during deployment\" sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.\n\nProduction is very patient about collecting on these debts.\n\nI watched a team defer a decision about how to handle partial sensor array failures. \"We'll see how it behaves in production and tune the algorithm.\" Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn't designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.\n\nThe cost of deferring that decision was higher than the cost of making it wrong during design would have been.\n\n## The Absence of Overconfidence\n\nOne of the clearest markers of experience is restraint.\n\nExperienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will‚Äîand the ways it might not.\n\nThis is sometimes mistaken for pessimism. It isn't. It is respect for complexity.\n\nConfidence says: \"This will handle sensor failures.\"\n\nExperience says: \"This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We've added bounds checking for the first case. The other two will require operational awareness.\"\n\nConfidence says: \"This will scale to 100 nodes.\"\n\nExperience says: \"This will scale to 100 nodes if network latency stays below 50ms and nodes don't join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We'll see it as increased jitter in the timing measurements.\"\n\nThe difference is not about being negative. It's about being specific about what has been designed for and what has been assumed away.\n\nWhen someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn't understood the problem space well enough to see the edges.\n\nMost problems are not trivial.\n\n## Ownership Reveals Maturity\n\nDesigns that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.\n\nExperienced reviewers push on this immediately.\n\nWho owns this in production? Who gets paged when it misbehaves? Who can say \"no\" when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?\n\nIf ownership is unclear, the design is incomplete‚Äîno matter how elegant the code.\n\nI've seen systems designed by committee where every component had a different owner, and the interfaces between components were \"shared responsibility.\" Shared responsibility is another way of saying \"no one is responsible.\"\n\nWhen that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system's behavior was the outcome of those negotiations, not of any coherent design intent.\n\nThe experienced engineer asks: \"Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?\"\n\nIf the answer is \"well, it depends on which part is having issues,\" the ownership model is broken.\n\nOwnership isn't about credit. It's about who carries the mental model of the system's actual behavior‚Äînot its documented behavior, its actual behavior‚Äîand has the authority to act on that understanding.\n\n## Experience Is Often Mistaken for Negativity\n\nThe most experienced person in the room is often labeled \"difficult\" at least once.\n\nThey ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn't include time for learning what was gotten wrong.\n\nThis is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.\n\nWhat they are really doing is defending the future team from the present team's optimism.\n\nThat is rarely popular in the moment. It is deeply appreciated later‚Äîusually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn't want to answer during the review has become an urgent problem with no good solutions.\n\nI've been that \"difficult\" person. I've asked questions that made presenters defensive. I've pointed out gaps that felt like criticisms. I've watched the energy in the room shift from enthusiasm to frustration.\n\nAnd I've also gotten emails, months later, from those same presenters: \"Remember when you asked about [that thing]? It happened. We were ready because we'd thought about it. Thank you.\"\n\nThe experienced reviewer is not trying to stop the design. They are trying to make it survivable.\n\n## Why Design Reviews Fail\n\nMost failed design reviews fail quietly.\n\nThey approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.\n\nAnd then production teaches the lesson that the review avoided.\n\nDesign reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.\n\nGood design reviews do not prevent failure. They prevent surprise.\n\nA good review leaves everyone with a shared understanding of:\n- What has been designed for\n- What has been assumed away\n- Where the edges are\n- Who owns what happens at those edges\n\nIt does not guarantee success. It makes failure less catastrophic and more recoverable.\n\n## A Final Thought\n\nExperience does not make you smarter in design reviews. It makes you more selective.\n\nYou stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you're gone.\n\nThat shift is subtle. And once you see it, you can't unsee it.\n\nYou start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions‚Äînot because you're difficult, but because you've seen what happens when no one asks them.\n\nAnd you realize that the best design reviews are not the ones where everyone agrees.\n\nThey are the ones where everyone leaves slightly uncomfortable, but with clarity about why."}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"blog_post_design_reviews"},"buildId":"phl_WOpLyX2gCvGrSm1hh","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
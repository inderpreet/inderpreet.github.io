<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Home | Layered Systems</title><meta name="title" content="Home | Layered Systems"/><meta name="description" content="Layered Systems - Engineering judgment, one layer at a time."/><meta name="keywords" content="blog, web development, nextjs, react, javascript, css, seo, ESP32, Raspberry Pi, Altium, KiCAD, prototyping, electronics, 3D printing, Fusion 360, microcontrollers, RTOS, embedded systems, firmware, C programming, control panels, automation"/><meta name="author" content="Inderpreet Singh"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:type" content="website"/><meta property="og:url" content="https://inderpreet.github.io"/><meta property="og:title" content="Home | Layered Systems"/><meta property="og:description" content="Layered Systems - Engineering judgment, one layer at a time."/><meta property="og:image" content="/og-image.jpg"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:url" content="https://inderpreet.github.io"/><meta property="twitter:title" content="Home | Layered Systems"/><meta property="twitter:description" content="Layered Systems - Engineering judgment, one layer at a time."/><meta property="twitter:image" content="/og-image.jpg"/><meta name="robots" content="index, follow"/><meta name="language" content="English"/><link rel="canonical" href="https://inderpreet.github.io"/><meta name="next-head-count" content="20"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="theme-color" content="#000000"/><link rel="preload" href="/_next/static/css/1d29b84bfc5354c8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1d29b84bfc5354c8.css" data-n-g=""/><link rel="preload" href="/_next/static/css/68fef7a5f0a68e02.css" as="style"/><link rel="stylesheet" href="/_next/static/css/68fef7a5f0a68e02.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-639aa3d8bfe7ee72.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-d979f5fb3aa68004.js" defer=""></script><script src="/_next/static/chunks/pages/_app-20693b9d02de5813.js" defer=""></script><script src="/_next/static/chunks/666-58cfeb78f799b3ee.js" defer=""></script><script src="/_next/static/chunks/pages/index-249c72a9378653c0.js" defer=""></script><script src="/_next/static/phl_WOpLyX2gCvGrSm1hh/_buildManifest.js" defer=""></script><script src="/_next/static/phl_WOpLyX2gCvGrSm1hh/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_container__l2gjk"><header class="Layout_header__8XhYm"><div class="Layout_headerContent__06wDW"><nav class="Layout_nav__qOLUE "><a href="/">Home</a><a href="/about">About</a><div class="Layout_dropdown__z3jRI"><button class="Layout_dropdownToggle__WdMTA" aria-expanded="false" aria-haspopup="true">Resources<svg width="12" height="12" viewBox="0 0 12 12" fill="currentColor" style="margin-left:0.25rem;transform:rotate(0deg);transition:transform 0.2s"><path d="M2 4l4 4 4-4" stroke="currentColor" stroke-width="2" fill="none"></path></svg></button><div class="Layout_dropdownMenu__zEIeU "><a href="/monitor" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üéõÔ∏è</span><div><div class="Layout_dropdownItemTitle__VcVEY">System Monitor</div><div class="Layout_dropdownItemDesc__DjI_j">Real-time dashboard</div></div></a><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üíª</span><div><div class="Layout_dropdownItemTitle__VcVEY">GitHub</div><div class="Layout_dropdownItemDesc__DjI_j">View my repositories</div></div></a><div class="Layout_dropdownDivider__ySCu0"></div><a href="/docs" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üìö</span><div><div class="Layout_dropdownItemTitle__VcVEY">Documentation</div><div class="Layout_dropdownItemDesc__DjI_j">Guides &amp; tutorials</div></div></a><a href="#" class="Layout_dropdownItem__tEDBv"><span class="Layout_dropdownIcon__fA6Ww">üöÄ</span><div><div class="Layout_dropdownItemTitle__VcVEY">Projects</div><div class="Layout_dropdownItemDesc__DjI_j">Explore my work</div></div></a></div></div></nav><div class="Layout_logoSection__xngcy"><h1 class="Layout_logo__Yfd0y"><a href="/">Layered Systems</a></h1><p class="Layout_tagline__pRBfO">Engineering judgment, one layer at a time.</p></div><div class="Layout_socialIcons__8_CJc"><button class="ThemeToggle_themeToggle__Lxt_p" aria-label="Toggle theme"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="5"></circle></svg></button><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://twitter.com/ip_v1" target="_blank" rel="noopener noreferrer" aria-label="Twitter"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"></path></svg></a><a href="https://linkedin.com/in/inderpreets" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div><button class="Layout_mobileMenuButton__nLMRy" aria-label="Toggle menu"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></header><main class="Layout_main__BqQ1G"><div class="Home_homeContainer__h7U1S"><div class="Home_mainContent__RtoDJ"><div class="Home_feed__n8ZD9"><article class="BlogCard_card__CAKq_"><a href="/post/blog_post_control_systems"><div class="BlogCard_imageContainer__ZiWI8"><img src="/images/posts/control-system.png" alt="Every Automated System Is a Control System" class="BlogCard_image__0BR8U"/></div></a><div class="BlogCard_cardBody__JuMW4"><a href="/post/blog_post_control_systems"><h2 class="BlogCard_title__0VkiB">Every Automated System Is a Control System</h2></a><p class="BlogCard_excerpt__HA7_R">Automation doesn&#x27;t remove control‚Äîit implements it. And whether that control lives in a PLC, a cloud service, or an organization chart doesn&#x27;t change the physics involved.</p><div class="BlogCard_tags__TuNF2"><span class="BlogCard_tag__rAnxm">#<!-- -->automation</span><span class="BlogCard_tag__rAnxm">#<!-- -->control-systems</span><span class="BlogCard_tag__rAnxm">#<!-- -->feedback</span><span class="BlogCard_tag__rAnxm">#<!-- -->industrial</span></div></div></article><article class="BlogCard_card__CAKq_"><a href="/post/blog_post_design_reviews"><div class="BlogCard_imageContainer__ZiWI8"><img src="/images/posts/design-review2.png" alt="What Experience Looks Like in Design Reviews" class="BlogCard_image__0BR8U"/></div></a><div class="BlogCard_cardBody__JuMW4"><a href="/post/blog_post_design_reviews"><h2 class="BlogCard_title__0VkiB">What Experience Looks Like in Design Reviews</h2></a><p class="BlogCard_excerpt__HA7_R">Inexperienced engineers talk a lot in design reviews. Experienced ones often don&#x27;t. This is frequently misinterpreted as disengagement. It isn&#x27;t. It&#x27;s pattern recognition at work.</p><div class="BlogCard_tags__TuNF2"><span class="BlogCard_tag__rAnxm">#<!-- -->design</span><span class="BlogCard_tag__rAnxm">#<!-- -->experience</span><span class="BlogCard_tag__rAnxm">#<!-- -->reviews</span><span class="BlogCard_tag__rAnxm">#<!-- -->engineering</span></div></div></article><article class="BlogCard_card__CAKq_"><a href="/post/blog_post_first_incident"><div class="BlogCard_imageContainer__ZiWI8"><img src="/images/posts/first-incident.png" alt="The First Incident Changes Everything" class="BlogCard_image__0BR8U"/></div></a><div class="BlogCard_cardBody__JuMW4"><a href="/post/blog_post_first_incident"><h2 class="BlogCard_title__0VkiB">The First Incident Changes Everything</h2></a><p class="BlogCard_excerpt__HA7_R">There&#x27;s a moment when ownership stops being theoretical and becomes visceral. It usually happens at 3 AM, with a production line stopped and people asking questions you don&#x27;t have answers to yet.</p><div class="BlogCard_tags__TuNF2"><span class="BlogCard_tag__rAnxm">#<!-- -->incidents</span><span class="BlogCard_tag__rAnxm">#<!-- -->ownership</span><span class="BlogCard_tag__rAnxm">#<!-- -->production</span><span class="BlogCard_tag__rAnxm">#<!-- -->systems</span></div></div></article><article class="BlogCard_card__CAKq_"><a href="/post/the-happy-post-lie"><div class="BlogCard_imageContainer__ZiWI8"><img src="/images/posts/design-review.png" alt="The Happy Path Is a Lie We Tell Ourselves" class="BlogCard_image__0BR8U"/></div></a><div class="BlogCard_cardBody__JuMW4"><a href="/post/the-happy-post-lie"><h2 class="BlogCard_title__0VkiB">The Happy Path Is a Lie We Tell Ourselves</h2></a><p class="BlogCard_excerpt__HA7_R">Design reviews reward optimism. Test environments validate assumptions. And somewhere between approval and deployment, we forget that we&#x27;re building systems for a world that doesn&#x27;t care about our diagrams.</p><div class="BlogCard_tags__TuNF2"><span class="BlogCard_tag__rAnxm">#<!-- -->systems</span><span class="BlogCard_tag__rAnxm">#<!-- -->design</span><span class="BlogCard_tag__rAnxm">#<!-- -->production</span><span class="BlogCard_tag__rAnxm">#<!-- -->failure</span></div></div></article><article class="BlogCard_card__CAKq_"><a href="/post/the-anchor"><div class="BlogCard_imageContainer__ZiWI8"><img src="/images/posts/post-anchor-splash-0.png" alt="Every System Tells Its Truth in Production" class="BlogCard_image__0BR8U"/></div></a><div class="BlogCard_cardBody__JuMW4"><a href="/post/the-anchor"><h2 class="BlogCard_title__0VkiB">Every System Tells Its Truth in Production</h2></a><p class="BlogCard_excerpt__HA7_R">Production strips away comfortable illusions. It&#x27;s where assumptions stop being hypothetical and start charging interest‚Äîrevealing not just what we built, but what we actually designed.</p><div class="BlogCard_tags__TuNF2"><span class="BlogCard_tag__rAnxm">#<!-- -->systems</span><span class="BlogCard_tag__rAnxm">#<!-- -->production</span><span class="BlogCard_tag__rAnxm">#<!-- -->engineering</span><span class="BlogCard_tag__rAnxm">#<!-- -->reliability</span></div></div></article></div></div><aside class="Home_sidebar__5uvgt"><div class="Home_sidebarCard__yNAi3"><div class="Home_sidebarHeader__Ke1_1"><h2 class="Home_sidebarTitle__bsU0T">About Layered Systems</h2></div><div class="Home_sidebarContent__r84ah"><p class="Home_sidebarDescription__S_hpI">Engineering judgment, one layer at a time. Exploring systems design, embedded development, and the layers that make technology work.</p></div></div><div class="Home_sidebarCard__yNAi3"><div class="Home_sidebarHeader__Ke1_1"><h2 class="Home_sidebarTitle__bsU0T">üî• Trending Topics</h2></div><div class="Home_trendingTags__DsjhM"><a href="#" class="Home_trendingTag__bobbw"><span class="Home_tagRank__CzWSR">1</span><div><div class="Home_tagName__MUp30">#webdev</div><div class="Home_tagCount__na8wO">12.5k posts</div></div></a><a href="#" class="Home_trendingTag__bobbw"><span class="Home_tagRank__CzWSR">2</span><div><div class="Home_tagName__MUp30">#javascript</div><div class="Home_tagCount__na8wO">10.2k posts</div></div></a><a href="#" class="Home_trendingTag__bobbw"><span class="Home_tagRank__CzWSR">3</span><div><div class="Home_tagName__MUp30">#react</div><div class="Home_tagCount__na8wO">8.9k posts</div></div></a><a href="#" class="Home_trendingTag__bobbw"><span class="Home_tagRank__CzWSR">4</span><div><div class="Home_tagName__MUp30">#design</div><div class="Home_tagCount__na8wO">7.3k posts</div></div></a><a href="#" class="Home_trendingTag__bobbw"><span class="Home_tagRank__CzWSR">5</span><div><div class="Home_tagName__MUp30">#nextjs</div><div class="Home_tagCount__na8wO">6.8k posts</div></div></a></div></div><div class="Home_sidebarCard__yNAi3"><div class="Home_sidebarHeader__Ke1_1"><h2 class="Home_sidebarTitle__bsU0T">Quick Links</h2></div><div class="Home_sidebarLinks__d0BOW"><a href="/about" class="Home_sidebarLink__RFrEC"><span class="Home_linkIcon__uwERA">üë§</span>About</a><a href="/docs" class="Home_sidebarLink__RFrEC"><span class="Home_linkIcon__uwERA">üìö</span>Documentation</a><a href="/monitor" class="Home_sidebarLink__RFrEC"><span class="Home_linkIcon__uwERA">üéõÔ∏è</span>System Monitor</a><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer" class="Home_sidebarLink__RFrEC"><span class="Home_linkIcon__uwERA">üíª</span>GitHub</a></div></div><div class="Home_sidebarCard__yNAi3"><div class="Home_sidebarHeader__Ke1_1"><h2 class="Home_sidebarTitle__bsU0T">Topics</h2></div><div class="Home_sidebarTopics__QtQPJ"><a href="#" class="Home_topicTag__2Yzvv">Embedded Systems</a><a href="#" class="Home_topicTag__2Yzvv">IoT</a><a href="#" class="Home_topicTag__2Yzvv">Web Development</a><a href="#" class="Home_topicTag__2Yzvv">Hardware Design</a><a href="#" class="Home_topicTag__2Yzvv">Firmware</a><a href="#" class="Home_topicTag__2Yzvv">System Design</a></div></div></aside></div></main><footer class="Layout_footer__3v8iv"><p>¬© 2025 Layered Systems - Engineering judgment, one layer at a time.</p><div class="Layout_socialLinks__WZeQy"><a href="https://github.com/inderpreet" target="_blank" rel="noopener noreferrer">GitHub</a><a href="https://twitter.com/ip_v1" target="_blank" rel="noopener noreferrer">Twitter</a><a href="https://instagram.com/ip_v1" target="_blank" rel="noopener noreferrer">Instagram</a><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer">LinkedIn</a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"blog_post_control_systems","title":"Every Automated System Is a Control System","date":"2025-12-24","excerpt":"Automation doesn't remove control‚Äîit implements it. And whether that control lives in a PLC, a cloud service, or an organization chart doesn't change the physics involved.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/control-system.png","tags":["automation","control-systems","feedback","industrial"],"content":"\u003ch1\u003eEvery Automated System Is a Control System\u003c/h1\u003e\n\u003cp\u003eA distribution warehouse automated its loading dock safety system in 2021. Smart control panels coordinated hydraulic dock levelers, truck restraints, overhead doors, and inflatable seals. The sequence was critical: restrain the truck, extend the leveler, inflate the seal, then allow the door to open. Get the order wrong and you risk crushing equipment, damaging trailers, or creating a fall hazard for forklift operators.\u003c/p\u003e\n\u003cp\u003eThe system worked flawlessly for eighteen months. Then one Tuesday morning, Bay 7 locked out. The operator pressed the button sequence. Nothing. The panel showed conflicting states‚Äîdoor closed, but also a fault. Leveler down, but restraint active. The display flickered between valid states too quickly to read.\u003c/p\u003e\n\u003cp\u003eThe truck sat in the bay. The operator called maintenance. Maintenance called the panel vendor. No one could see what the system was seeing. No logs. No remote access. No diagnostic mode that didn\u0026#39;t require physically opening the panel and connecting a laptop‚Äîwhich no one on-site knew how to do safely while the system was energized.\u003c/p\u003e\n\u003cp\u003eFour hours later, a technician traced the problem: a door interlock sensor with a loose connection. Instead of a clean on/off signal, it was chattering‚Äîsending intermittent pulses that the control logic interpreted as rapid state changes. The safety system, doing exactly what it was designed to do, refused to proceed because the door state was ambiguous.\u003c/p\u003e\n\u003cp\u003eThe fix took ten minutes once diagnosed. The diagnosis took four hours and $800 in emergency service fees. The lost productivity cost more than that.\u003c/p\u003e\n\u003cp\u003eThe system worked exactly as designed. It just had no way to tell anyone what it was experiencing.\u003c/p\u003e\n\u003cp\u003eAutomation is often described as a way to remove humans from the loop. That framing is comforting‚Äîand wrong.\u003c/p\u003e\n\u003cp\u003eAutomation does not eliminate control. It implements it. Every automated system is, at its core, a control system: something observes a process, compares it to intent, and acts to reduce the difference.\u003c/p\u003e\n\u003cp\u003eWhether that system lives in a PLC, a microcontroller, a cloud service, or an organization chart does not change the physics involved. Feedback, delay, noise, saturation, and stability still apply.\u003c/p\u003e\n\u003cp\u003eIgnoring that reality is how automated systems quietly become dangerous.\u003c/p\u003e\n\u003ch2\u003eAutomation Is Control With Commitment\u003c/h2\u003e\n\u003cp\u003eIn industrial environments, automation is not an abstraction. It closes a loop around something physical.\u003c/p\u003e\n\u003cp\u003eA valve moves. A motor spins. A door opens. A restraint engages. A process heats, cools, fills, or drains.\u003c/p\u003e\n\u003cp\u003eOnce automated, those actions happen faster than human reaction time and with more consistency than human judgment. That is the point.\u003c/p\u003e\n\u003cp\u003eBut it also means the consequences of design decisions arrive faster‚Äîand with less opportunity for intervention.\u003c/p\u003e\n\u003cp\u003eA human operator running a system manually makes continuous micro-adjustments based on observation, intuition, and context that never makes it into documentation. They notice when a sensor \u0026quot;feels sticky.\u0026quot; They remember that the interlock on Bay 3 takes an extra half-second to settle. They know to wait for the hydraulic pump to stabilize before cycling again.\u003c/p\u003e\n\u003cp\u003eAutomation doesn\u0026#39;t have intuition. It has the logic you gave it and the sensors you installed. It will faithfully execute that logic with those sensors, even when the result is obviously wrong to anyone watching.\u003c/p\u003e\n\u003cp\u003eThis is not a limitation of automation. It is the nature of it.\u003c/p\u003e\n\u003cp\u003eAutomation is control with commitment. Once the loop is closed, the system will do exactly what you encoded‚Äîno more, no less‚Äîlong after you\u0026#39;ve stopped paying attention. It won\u0026#39;t second-guess itself. It won\u0026#39;t notice that something feels off. It will enforce whatever rules you programmed, even when those rules produce unexpected behavior.\u003c/p\u003e\n\u003cp\u003eThat dock control panel didn\u0026#39;t know the door sensor was chattering. It knew the door state was changing rapidly, which violated the safety interlock logic. So it did exactly what it was supposed to: refuse to proceed. The fact that the state changes were caused by a failing sensor rather than an actual unsafe condition was invisible to the control logic.\u003c/p\u003e\n\u003cp\u003eThe system couldn\u0026#39;t distinguish between \u0026quot;door rapidly opening and closing\u0026quot; (dangerous, must prevent) and \u0026quot;door sensor failing\u0026quot; (annoying, should flag). Both looked the same from inside the control loop.\u003c/p\u003e\n\u003ch2\u003eThe Control Loop Is the Unit of Truth\u003c/h2\u003e\n\u003cp\u003eIt\u0026#39;s tempting to talk about systems in terms of components: sensors, PLCs, networks, SCADA, cloud dashboards, HMI panels.\u003c/p\u003e\n\u003cp\u003eBut components don\u0026#39;t fail in isolation. Loops do.\u003c/p\u003e\n\u003cp\u003eA control loop tells you:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat the system believes about the world\u003c/li\u003e\n\u003cli\u003eHow often it updates that belief  \u003c/li\u003e\n\u003cli\u003eHow aggressively it responds\u003c/li\u003e\n\u003cli\u003eWhat happens when measurements are missing or wrong\u003c/li\u003e\n\u003cli\u003eWhat it does when it reaches its limits\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you want to understand an automated system, you don\u0026#39;t start with the hardware list. You start by asking: where are the loops, and where aren\u0026#39;t they?\u003c/p\u003e\n\u003cp\u003eThat dock safety system had all the right components. The sensors were rated for the environment. The control panel had proper safety logic. The interlocks were correctly wired. But the loop‚Äîthe complete feedback path from sensor through logic to actuator and back‚Äîhad no provision for ambiguous sensor states.\u003c/p\u003e\n\u003cp\u003eThe loop is what matters. Not because the components aren\u0026#39;t important, but because components only make sense in the context of the loop they\u0026#39;re part of.\u003c/p\u003e\n\u003cp\u003eA binary sensor is perfect‚Äîunless environmental vibration causes intermittent connections. A safety interlock is necessary‚Äîunless it can\u0026#39;t distinguish between unsafe conditions and sensor failures. A local control panel is reliable‚Äîunless diagnosing it requires knowledge and tools that aren\u0026#39;t available on-site.\u003c/p\u003e\n\u003cp\u003eThis is why you can\u0026#39;t design automation by picking components and wiring them together. You have to design loops: understand what the system needs to observe, determine how it should respond to both normal and abnormal signals, and verify that the complete feedback path behaves as intended‚Äîincluding degraded sensor conditions.\u003c/p\u003e\n\u003cp\u003eMost automation problems are loop problems disguised as component problems.\u003c/p\u003e\n\u003ch2\u003eOpen-Loop Automation Is Optimism\u003c/h2\u003e\n\u003cp\u003eMany automation failures are not caused by bugs. They are caused by loops that were never fully closed.\u003c/p\u003e\n\u003cp\u003eThat dock control system had excellent safety interlocks‚Äîsensors, logic, actuators, all integrated. But it had no diagnostic loop. No way for the system to observe its own behavior and communicate what it was experiencing. No telemetry. No event logging. No remote visibility.\u003c/p\u003e\n\u003cp\u003eWhen the door sensor failed, the safety loop worked perfectly: detect ambiguous state, prevent operation, protect people and equipment. But there was no observability loop to help operators or technicians understand why.\u003c/p\u003e\n\u003cp\u003eThis pattern appears everywhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSystems that actuate but don\u0026#39;t verify outcome\u003c/li\u003e\n\u003cli\u003eAlarms that fire but have no authority to act  \u003c/li\u003e\n\u003cli\u003eSafety interlocks with no diagnostic telemetry\u003c/li\u003e\n\u003cli\u003eControl panels that enforce logic but can\u0026#39;t explain their state\u003c/li\u003e\n\u003cli\u003eAnalytics platforms that detect anomalies no one responds to\u003c/li\u003e\n\u003cli\u003eDashboards that inform no decisions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese are open-loop systems wearing the appearance of control. They work beautifully in steady state. They fail spectacularly during transitions‚Äîstartups, shutdowns, disturbances, sensor degradation, and edge cases‚Äîbecause there\u0026#39;s no feedback to communicate what\u0026#39;s actually happening.\u003c/p\u003e\n\u003cp\u003eOpen-loop automation is not engineering. It is optimism encoded in architecture diagrams.\u003c/p\u003e\n\u003cp\u003eThe difference between \u0026quot;automated\u0026quot; and \u0026quot;automatic\u0026quot; matters. Automated means machines do the work. Automatic means the machines adjust themselves when things change. One is a tool. The other is a control system.\u003c/p\u003e\n\u003cp\u003eBut there\u0026#39;s a third category that\u0026#39;s often forgotten: observable. A system can be automatic but opaque. It makes decisions, but you can\u0026#39;t see why. It enforces rules, but you can\u0026#39;t tell which rule is blocking you. It protects you from hazards, but also locks you out when a sensor hiccups‚Äîand offers no clue about which sensor or why.\u003c/p\u003e\n\u003cp\u003eObservability isn\u0026#39;t optional. It\u0026#39;s a feedback loop for humans trying to understand what the automated system is doing.\u003c/p\u003e\n\u003ch2\u003eRemote Monitoring Is a Supervisory Layer\u003c/h2\u003e\n\u003cp\u003eRemote monitoring is often sold as visibility. Visibility is useful, but visibility alone does not create control.\u003c/p\u003e\n\u003cp\u003eA remote system introduces latency, bandwidth constraints, loss of determinism, and unclear authority boundaries. At that point, you are no longer designing a control loop. You are designing a supervisory loop.\u003c/p\u003e\n\u003cp\u003eSupervisory control is valid‚Äîbut only when its role is clearly defined.\u003c/p\u003e\n\u003cp\u003eConsider a hierarchical control structure: local control panels handle safety interlocks (milliseconds), site management systems coordinate operations (seconds to minutes), and cloud platforms aggregate diagnostics across facilities (minutes to hours). Each layer operates at its natural timescale. Authority is clear.\u003c/p\u003e\n\u003cp\u003eThe dock safety system didn\u0026#39;t need cloud control. The safety decisions‚Äîwhen to allow the door to open, whether the truck is properly restrained‚Äîmust happen locally, instantly, deterministically. Those decisions can\u0026#39;t wait for network round-trips or cloud processing.\u003c/p\u003e\n\u003cp\u003eBut diagnostics? That\u0026#39;s different. Diagnostics operate on a slower timescale. When a sensor starts behaving erratically, you don\u0026#39;t need instant response. You need visibility‚Äîevent logs, state histories, sensor signal quality metrics. Information that helps technicians diagnose problems without being on-site with specialized tools.\u003c/p\u003e\n\u003cp\u003eThe missing piece wasn\u0026#39;t faster control. It was slower observation.\u003c/p\u003e\n\u003cp\u003eProblems arise when these boundaries blur. When cloud systems try to make real-time control decisions. When local controllers wait for remote approval before acting. When \u0026quot;monitoring\u0026quot; systems are expected to prevent problems they can only observe after the fact.\u003c/p\u003e\n\u003cp\u003eLatency turns control into observation. But observation without control is still valuable‚Äîif the system is designed to provide it.\u003c/p\u003e\n\u003ch2\u003eWhen Remote Observability Meets Local Control\u003c/h2\u003e\n\u003cp\u003eCloud platforms feel powerful because they are flexible, scalable, and abstracted from hardware. None of that exempts them from control theory.\u003c/p\u003e\n\u003cp\u003eRemote systems are best understood as high-latency, high-compute supervisory layers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExcellent for aggregation across multiple sites\u003c/li\u003e\n\u003cli\u003eExcellent for diagnostics and trend analysis\u003c/li\u003e\n\u003cli\u003eExcellent for coordination and maintenance planning\u003c/li\u003e\n\u003cli\u003eTerrible for real-time safety decisions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis isn\u0026#39;t a criticism of remote architecture. It\u0026#39;s a description of its physics. Light only travels so fast. Networks have congestion. Cellular connections drop. These aren\u0026#39;t implementation details to be optimized away‚Äîthey\u0026#39;re fundamental constraints that must be designed for.\u003c/p\u003e\n\u003cp\u003eThe dock control panel could have had both: local, deterministic safety control (no network dependency, instant response, proven logic) plus remote diagnostic telemetry (event logs, sensor states, fault histories uploaded when connectivity is available).\u003c/p\u003e\n\u003cp\u003eThis requires answering specific questions during design:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat decisions must be local and deterministic?\u003c/li\u003e\n\u003cli\u003eWhat information would help diagnose problems remotely?\u003c/li\u003e\n\u003cli\u003eWhat happens when connectivity is unavailable?\u003c/li\u003e\n\u003cli\u003eHow long can diagnostics be buffered before critical information is lost?\u003c/li\u003e\n\u003cli\u003eWho receives the diagnostic data and what do they do with it?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMost systems don\u0026#39;t answer these questions explicitly. They treat remote connectivity as either essential (everything goes to the cloud) or unnecessary (everything stays local). The middle ground‚Äîlocal control with remote observability‚Äîrequires more thought but solves real problems.\u003c/p\u003e\n\u003cp\u003eThat four-hour diagnostic delay? With basic telemetry, it could have been four minutes. Not by making the cloud control the dock‚Äîbut by making the dock tell the cloud what it was experiencing. Event timestamps showing the door sensor cycling 200 times per second. State machine logs showing repeated transitions between incompatible states. Signal quality metrics showing noise on the interlock circuit.\u003c/p\u003e\n\u003cp\u003eThe technician could have arrived knowing exactly which sensor to check, maybe even with a replacement part already in hand.\u003c/p\u003e\n\u003ch2\u003eSafety Is a Separate Loop\u003c/h2\u003e\n\u003cp\u003eOne of the earliest lessons in industrial control is also one of the most frequently forgotten: safety is not a feature of control‚Äîit is a separate control loop.\u003c/p\u003e\n\u003cp\u003eSafety systems assume the primary control loop can fail. They assume sensors can lie. They assume logic can be wrong. They assume humans can make poor decisions.\u003c/p\u003e\n\u003cp\u003eThey exist precisely because automation works so well‚Äîand so relentlessly.\u003c/p\u003e\n\u003cp\u003eA control system optimizes for performance: throughput, efficiency, convenience. A safety system optimizes for survival: keeping things within boundaries that prevent damage, injury, or environmental harm.\u003c/p\u003e\n\u003cp\u003eThese goals conflict. And when they\u0026#39;re implemented in the same system, the conflict gets resolved through implicit tradeoffs that no one agreed to.\u003c/p\u003e\n\u003cp\u003eThe dock control panel did this right: safety logic was primary. When the door sensor gave ambiguous signals, the system chose the safe response‚Äîlockout‚Äîover the convenient response‚Äîignore the noise and proceed anyway.\u003c/p\u003e\n\u003cp\u003eBut safety and observability are not the same thing. The system could safely refuse to operate while simultaneously logging why. It could maintain safety interlocks while transmitting diagnostic data. It could protect operators from hazards while giving technicians information to fix the hazard.\u003c/p\u003e\n\u003cp\u003eInstead, it protected operators but left technicians blind.\u003c/p\u003e\n\u003cp\u003eThis is a common pattern: systems designed with robust safety logic but minimal diagnostic capability. The assumption is that if safety is handled, everything else is secondary. But \u0026quot;everything else\u0026quot; includes the ability to understand failures, plan maintenance, and avoid emergency service calls.\u003c/p\u003e\n\u003cp\u003eSafety loops must be independent: separate sensors, separate logic, separate authority to shut things down. But observability loops should be pervasive: instrument everything, log state changes, buffer diagnostics locally, transmit when possible.\u003c/p\u003e\n\u003cp\u003eThese are not competing requirements. They\u0026#39;re complementary.\u003c/p\u003e\n\u003ch2\u003eOrganizations Are Control Systems Too\u003c/h2\u003e\n\u003cp\u003eThis is where automation, technology, and management converge.\u003c/p\u003e\n\u003cp\u003eOrganizations sense through metrics and reports. They decide through meetings and approvals. They actuate through people and processes. They respond with delay and distortion.\u003c/p\u003e\n\u003cp\u003eWhen feedback is slow, organizations oscillate‚Äîovercorrecting to problems that have already changed.\u003c/p\u003e\n\u003cp\u003eWhen authority is unclear, control saturates‚Äîmultiple people trying to correct the same thing in incompatible ways.\u003c/p\u003e\n\u003cp\u003eWhen incentives are misaligned, the system drives itself into failure‚Äîoptimizing locally while degrading globally.\u003c/p\u003e\n\u003cp\u003eThese are not cultural problems. They are control problems.\u003c/p\u003e\n\u003cp\u003eThat dock control panel? The decision to exclude remote diagnostics wasn\u0026#39;t technical. The panel manufacturer offered telemetry as an option. The warehouse operator decided against it to save $200 per bay‚Äî$2,400 total across twelve bays.\u003c/p\u003e\n\u003cp\u003eThat was a reasonable decision given available information: the system had worked reliably elsewhere, diagnostic issues seemed unlikely, and budget was constrained.\u003c/p\u003e\n\u003cp\u003eBut no one in the decision-making loop had experienced the cost of a four-hour diagnostic delay. No one modeled the probability of sensor degradation over time. No one considered that the warehouse was remote enough that emergency service calls were expensive. The organizational feedback loop‚Äîthe one that should learn from operational experience and adjust specifications‚Äîwas open.\u003c/p\u003e\n\u003cp\u003eThe result: $2,400 saved during installation, $800 spent on the first service call, plus lost productivity, plus the knowledge that eleven other bays could fail the same way with the same diagnostic blindness.\u003c/p\u003e\n\u003cp\u003eThe technical system was local. The organizational learning system was also local‚Äîeach facility made its own decisions based on its own experience, with no feedback loop to aggregate lessons learned across sites.\u003c/p\u003e\n\u003cp\u003eThis pattern repeats: automation projects where procurement optimizes for initial cost without visibility into operational cost. Remote monitoring proposals rejected because \u0026quot;it\u0026#39;s worked fine without it\u0026quot; right up until it doesn\u0026#39;t. Diagnostic capabilities treated as nice-to-have features instead of essential observability loops.\u003c/p\u003e\n\u003cp\u003eThe technical system can be well designed, but if the organizational system around it is open-loop, failures are inevitable.\u003c/p\u003e\n\u003ch2\u003eThe Pattern Repeats\u003c/h2\u003e\n\u003cp\u003eThis dock control failure is not unique. The specifics change‚Äîdifferent equipment, different sensors, different facilities‚Äîbut the pattern is constant.\u003c/p\u003e\n\u003cp\u003eSystems are designed with good control logic but minimal observability. Safety is prioritized (correctly) over diagnostics. Remote connectivity is seen as an unnecessary cost until the first time it would have saved multiples of that cost.\u003c/p\u003e\n\u003cp\u003eAnd the lesson gets learned locally, slowly, expensively‚Äîone emergency service call at a time.\u003c/p\u003e\n\u003cp\u003eThis is where control theory meets organizational reality. The technical feedback loop (sensor ‚Üí logic ‚Üí actuator) works fine. The diagnostic feedback loop (system behavior ‚Üí human understanding ‚Üí maintenance action) is open. And the organizational learning loop (operational experience ‚Üí design decisions ‚Üí better specifications) barely exists.\u003c/p\u003e\n\u003cp\u003eEach loop matters. Each fails in predictable ways. And understanding one helps you recognize the others.\u003c/p\u003e\n\u003ch2\u003eA Final Thought\u003c/h2\u003e\n\u003cp\u003eAutomation doesn\u0026#39;t remove humans from systems. It changes where humans intervene‚Äîand how much information they have when they do.\u003c/p\u003e\n\u003cp\u003eThe dock control panel didn\u0026#39;t need humans removed from the safety loop. It needed humans with the right information at the right time: operators with clear panel feedback about system state, technicians with diagnostic telemetry about sensor health, managers with operational data about failure patterns across sites.\u003c/p\u003e\n\u003cp\u003eWell-designed systems respect that reality. They put fast control close to the process. They add observability where diagnosis matters. They keep safety separate from optimization. They ensure someone owns the complete loop‚Äînot just the components, not just the initial installation, but the long-term operational behavior.\u003c/p\u003e\n\u003cp\u003ePoorly designed systems hide these distinctions until something breaks.\u003c/p\u003e\n\u003cp\u003eEvery automated system is a control system. Whether it behaves safely, stably, and predictably depends on whether its designers treated it like one.\u003c/p\u003e\n\u003cp\u003eThe question is not whether you have automation. The question is whether you have control‚Äîand whether you can see what that control is doing when things go wrong.\u003c/p\u003e\n","rawContent":"\n# Every Automated System Is a Control System\n\nA distribution warehouse automated its loading dock safety system in 2021. Smart control panels coordinated hydraulic dock levelers, truck restraints, overhead doors, and inflatable seals. The sequence was critical: restrain the truck, extend the leveler, inflate the seal, then allow the door to open. Get the order wrong and you risk crushing equipment, damaging trailers, or creating a fall hazard for forklift operators.\n\nThe system worked flawlessly for eighteen months. Then one Tuesday morning, Bay 7 locked out. The operator pressed the button sequence. Nothing. The panel showed conflicting states‚Äîdoor closed, but also a fault. Leveler down, but restraint active. The display flickered between valid states too quickly to read.\n\nThe truck sat in the bay. The operator called maintenance. Maintenance called the panel vendor. No one could see what the system was seeing. No logs. No remote access. No diagnostic mode that didn't require physically opening the panel and connecting a laptop‚Äîwhich no one on-site knew how to do safely while the system was energized.\n\nFour hours later, a technician traced the problem: a door interlock sensor with a loose connection. Instead of a clean on/off signal, it was chattering‚Äîsending intermittent pulses that the control logic interpreted as rapid state changes. The safety system, doing exactly what it was designed to do, refused to proceed because the door state was ambiguous.\n\nThe fix took ten minutes once diagnosed. The diagnosis took four hours and $800 in emergency service fees. The lost productivity cost more than that.\n\nThe system worked exactly as designed. It just had no way to tell anyone what it was experiencing.\n\nAutomation is often described as a way to remove humans from the loop. That framing is comforting‚Äîand wrong.\n\nAutomation does not eliminate control. It implements it. Every automated system is, at its core, a control system: something observes a process, compares it to intent, and acts to reduce the difference.\n\nWhether that system lives in a PLC, a microcontroller, a cloud service, or an organization chart does not change the physics involved. Feedback, delay, noise, saturation, and stability still apply.\n\nIgnoring that reality is how automated systems quietly become dangerous.\n\n## Automation Is Control With Commitment\n\nIn industrial environments, automation is not an abstraction. It closes a loop around something physical.\n\nA valve moves. A motor spins. A door opens. A restraint engages. A process heats, cools, fills, or drains.\n\nOnce automated, those actions happen faster than human reaction time and with more consistency than human judgment. That is the point.\n\nBut it also means the consequences of design decisions arrive faster‚Äîand with less opportunity for intervention.\n\nA human operator running a system manually makes continuous micro-adjustments based on observation, intuition, and context that never makes it into documentation. They notice when a sensor \"feels sticky.\" They remember that the interlock on Bay 3 takes an extra half-second to settle. They know to wait for the hydraulic pump to stabilize before cycling again.\n\nAutomation doesn't have intuition. It has the logic you gave it and the sensors you installed. It will faithfully execute that logic with those sensors, even when the result is obviously wrong to anyone watching.\n\nThis is not a limitation of automation. It is the nature of it.\n\nAutomation is control with commitment. Once the loop is closed, the system will do exactly what you encoded‚Äîno more, no less‚Äîlong after you've stopped paying attention. It won't second-guess itself. It won't notice that something feels off. It will enforce whatever rules you programmed, even when those rules produce unexpected behavior.\n\nThat dock control panel didn't know the door sensor was chattering. It knew the door state was changing rapidly, which violated the safety interlock logic. So it did exactly what it was supposed to: refuse to proceed. The fact that the state changes were caused by a failing sensor rather than an actual unsafe condition was invisible to the control logic.\n\nThe system couldn't distinguish between \"door rapidly opening and closing\" (dangerous, must prevent) and \"door sensor failing\" (annoying, should flag). Both looked the same from inside the control loop.\n\n## The Control Loop Is the Unit of Truth\n\nIt's tempting to talk about systems in terms of components: sensors, PLCs, networks, SCADA, cloud dashboards, HMI panels.\n\nBut components don't fail in isolation. Loops do.\n\nA control loop tells you:\n- What the system believes about the world\n- How often it updates that belief  \n- How aggressively it responds\n- What happens when measurements are missing or wrong\n- What it does when it reaches its limits\n\nIf you want to understand an automated system, you don't start with the hardware list. You start by asking: where are the loops, and where aren't they?\n\nThat dock safety system had all the right components. The sensors were rated for the environment. The control panel had proper safety logic. The interlocks were correctly wired. But the loop‚Äîthe complete feedback path from sensor through logic to actuator and back‚Äîhad no provision for ambiguous sensor states.\n\nThe loop is what matters. Not because the components aren't important, but because components only make sense in the context of the loop they're part of.\n\nA binary sensor is perfect‚Äîunless environmental vibration causes intermittent connections. A safety interlock is necessary‚Äîunless it can't distinguish between unsafe conditions and sensor failures. A local control panel is reliable‚Äîunless diagnosing it requires knowledge and tools that aren't available on-site.\n\nThis is why you can't design automation by picking components and wiring them together. You have to design loops: understand what the system needs to observe, determine how it should respond to both normal and abnormal signals, and verify that the complete feedback path behaves as intended‚Äîincluding degraded sensor conditions.\n\nMost automation problems are loop problems disguised as component problems.\n\n## Open-Loop Automation Is Optimism\n\nMany automation failures are not caused by bugs. They are caused by loops that were never fully closed.\n\nThat dock control system had excellent safety interlocks‚Äîsensors, logic, actuators, all integrated. But it had no diagnostic loop. No way for the system to observe its own behavior and communicate what it was experiencing. No telemetry. No event logging. No remote visibility.\n\nWhen the door sensor failed, the safety loop worked perfectly: detect ambiguous state, prevent operation, protect people and equipment. But there was no observability loop to help operators or technicians understand why.\n\nThis pattern appears everywhere:\n- Systems that actuate but don't verify outcome\n- Alarms that fire but have no authority to act  \n- Safety interlocks with no diagnostic telemetry\n- Control panels that enforce logic but can't explain their state\n- Analytics platforms that detect anomalies no one responds to\n- Dashboards that inform no decisions\n\nThese are open-loop systems wearing the appearance of control. They work beautifully in steady state. They fail spectacularly during transitions‚Äîstartups, shutdowns, disturbances, sensor degradation, and edge cases‚Äîbecause there's no feedback to communicate what's actually happening.\n\nOpen-loop automation is not engineering. It is optimism encoded in architecture diagrams.\n\nThe difference between \"automated\" and \"automatic\" matters. Automated means machines do the work. Automatic means the machines adjust themselves when things change. One is a tool. The other is a control system.\n\nBut there's a third category that's often forgotten: observable. A system can be automatic but opaque. It makes decisions, but you can't see why. It enforces rules, but you can't tell which rule is blocking you. It protects you from hazards, but also locks you out when a sensor hiccups‚Äîand offers no clue about which sensor or why.\n\nObservability isn't optional. It's a feedback loop for humans trying to understand what the automated system is doing.\n\n## Remote Monitoring Is a Supervisory Layer\n\nRemote monitoring is often sold as visibility. Visibility is useful, but visibility alone does not create control.\n\nA remote system introduces latency, bandwidth constraints, loss of determinism, and unclear authority boundaries. At that point, you are no longer designing a control loop. You are designing a supervisory loop.\n\nSupervisory control is valid‚Äîbut only when its role is clearly defined.\n\nConsider a hierarchical control structure: local control panels handle safety interlocks (milliseconds), site management systems coordinate operations (seconds to minutes), and cloud platforms aggregate diagnostics across facilities (minutes to hours). Each layer operates at its natural timescale. Authority is clear.\n\nThe dock safety system didn't need cloud control. The safety decisions‚Äîwhen to allow the door to open, whether the truck is properly restrained‚Äîmust happen locally, instantly, deterministically. Those decisions can't wait for network round-trips or cloud processing.\n\nBut diagnostics? That's different. Diagnostics operate on a slower timescale. When a sensor starts behaving erratically, you don't need instant response. You need visibility‚Äîevent logs, state histories, sensor signal quality metrics. Information that helps technicians diagnose problems without being on-site with specialized tools.\n\nThe missing piece wasn't faster control. It was slower observation.\n\nProblems arise when these boundaries blur. When cloud systems try to make real-time control decisions. When local controllers wait for remote approval before acting. When \"monitoring\" systems are expected to prevent problems they can only observe after the fact.\n\nLatency turns control into observation. But observation without control is still valuable‚Äîif the system is designed to provide it.\n\n## When Remote Observability Meets Local Control\n\nCloud platforms feel powerful because they are flexible, scalable, and abstracted from hardware. None of that exempts them from control theory.\n\nRemote systems are best understood as high-latency, high-compute supervisory layers:\n- Excellent for aggregation across multiple sites\n- Excellent for diagnostics and trend analysis\n- Excellent for coordination and maintenance planning\n- Terrible for real-time safety decisions\n\nThis isn't a criticism of remote architecture. It's a description of its physics. Light only travels so fast. Networks have congestion. Cellular connections drop. These aren't implementation details to be optimized away‚Äîthey're fundamental constraints that must be designed for.\n\nThe dock control panel could have had both: local, deterministic safety control (no network dependency, instant response, proven logic) plus remote diagnostic telemetry (event logs, sensor states, fault histories uploaded when connectivity is available).\n\nThis requires answering specific questions during design:\n- What decisions must be local and deterministic?\n- What information would help diagnose problems remotely?\n- What happens when connectivity is unavailable?\n- How long can diagnostics be buffered before critical information is lost?\n- Who receives the diagnostic data and what do they do with it?\n\nMost systems don't answer these questions explicitly. They treat remote connectivity as either essential (everything goes to the cloud) or unnecessary (everything stays local). The middle ground‚Äîlocal control with remote observability‚Äîrequires more thought but solves real problems.\n\nThat four-hour diagnostic delay? With basic telemetry, it could have been four minutes. Not by making the cloud control the dock‚Äîbut by making the dock tell the cloud what it was experiencing. Event timestamps showing the door sensor cycling 200 times per second. State machine logs showing repeated transitions between incompatible states. Signal quality metrics showing noise on the interlock circuit.\n\nThe technician could have arrived knowing exactly which sensor to check, maybe even with a replacement part already in hand.\n\n## Safety Is a Separate Loop\n\nOne of the earliest lessons in industrial control is also one of the most frequently forgotten: safety is not a feature of control‚Äîit is a separate control loop.\n\nSafety systems assume the primary control loop can fail. They assume sensors can lie. They assume logic can be wrong. They assume humans can make poor decisions.\n\nThey exist precisely because automation works so well‚Äîand so relentlessly.\n\nA control system optimizes for performance: throughput, efficiency, convenience. A safety system optimizes for survival: keeping things within boundaries that prevent damage, injury, or environmental harm.\n\nThese goals conflict. And when they're implemented in the same system, the conflict gets resolved through implicit tradeoffs that no one agreed to.\n\nThe dock control panel did this right: safety logic was primary. When the door sensor gave ambiguous signals, the system chose the safe response‚Äîlockout‚Äîover the convenient response‚Äîignore the noise and proceed anyway.\n\nBut safety and observability are not the same thing. The system could safely refuse to operate while simultaneously logging why. It could maintain safety interlocks while transmitting diagnostic data. It could protect operators from hazards while giving technicians information to fix the hazard.\n\nInstead, it protected operators but left technicians blind.\n\nThis is a common pattern: systems designed with robust safety logic but minimal diagnostic capability. The assumption is that if safety is handled, everything else is secondary. But \"everything else\" includes the ability to understand failures, plan maintenance, and avoid emergency service calls.\n\nSafety loops must be independent: separate sensors, separate logic, separate authority to shut things down. But observability loops should be pervasive: instrument everything, log state changes, buffer diagnostics locally, transmit when possible.\n\nThese are not competing requirements. They're complementary.\n\n## Organizations Are Control Systems Too\n\nThis is where automation, technology, and management converge.\n\nOrganizations sense through metrics and reports. They decide through meetings and approvals. They actuate through people and processes. They respond with delay and distortion.\n\nWhen feedback is slow, organizations oscillate‚Äîovercorrecting to problems that have already changed.\n\nWhen authority is unclear, control saturates‚Äîmultiple people trying to correct the same thing in incompatible ways.\n\nWhen incentives are misaligned, the system drives itself into failure‚Äîoptimizing locally while degrading globally.\n\nThese are not cultural problems. They are control problems.\n\nThat dock control panel? The decision to exclude remote diagnostics wasn't technical. The panel manufacturer offered telemetry as an option. The warehouse operator decided against it to save $200 per bay‚Äî$2,400 total across twelve bays.\n\nThat was a reasonable decision given available information: the system had worked reliably elsewhere, diagnostic issues seemed unlikely, and budget was constrained.\n\nBut no one in the decision-making loop had experienced the cost of a four-hour diagnostic delay. No one modeled the probability of sensor degradation over time. No one considered that the warehouse was remote enough that emergency service calls were expensive. The organizational feedback loop‚Äîthe one that should learn from operational experience and adjust specifications‚Äîwas open.\n\nThe result: $2,400 saved during installation, $800 spent on the first service call, plus lost productivity, plus the knowledge that eleven other bays could fail the same way with the same diagnostic blindness.\n\nThe technical system was local. The organizational learning system was also local‚Äîeach facility made its own decisions based on its own experience, with no feedback loop to aggregate lessons learned across sites.\n\nThis pattern repeats: automation projects where procurement optimizes for initial cost without visibility into operational cost. Remote monitoring proposals rejected because \"it's worked fine without it\" right up until it doesn't. Diagnostic capabilities treated as nice-to-have features instead of essential observability loops.\n\nThe technical system can be well designed, but if the organizational system around it is open-loop, failures are inevitable.\n\n## The Pattern Repeats\n\nThis dock control failure is not unique. The specifics change‚Äîdifferent equipment, different sensors, different facilities‚Äîbut the pattern is constant.\n\nSystems are designed with good control logic but minimal observability. Safety is prioritized (correctly) over diagnostics. Remote connectivity is seen as an unnecessary cost until the first time it would have saved multiples of that cost.\n\nAnd the lesson gets learned locally, slowly, expensively‚Äîone emergency service call at a time.\n\nThis is where control theory meets organizational reality. The technical feedback loop (sensor ‚Üí logic ‚Üí actuator) works fine. The diagnostic feedback loop (system behavior ‚Üí human understanding ‚Üí maintenance action) is open. And the organizational learning loop (operational experience ‚Üí design decisions ‚Üí better specifications) barely exists.\n\nEach loop matters. Each fails in predictable ways. And understanding one helps you recognize the others.\n\n## A Final Thought\n\nAutomation doesn't remove humans from systems. It changes where humans intervene‚Äîand how much information they have when they do.\n\nThe dock control panel didn't need humans removed from the safety loop. It needed humans with the right information at the right time: operators with clear panel feedback about system state, technicians with diagnostic telemetry about sensor health, managers with operational data about failure patterns across sites.\n\nWell-designed systems respect that reality. They put fast control close to the process. They add observability where diagnosis matters. They keep safety separate from optimization. They ensure someone owns the complete loop‚Äînot just the components, not just the initial installation, but the long-term operational behavior.\n\nPoorly designed systems hide these distinctions until something breaks.\n\nEvery automated system is a control system. Whether it behaves safely, stably, and predictably depends on whether its designers treated it like one.\n\nThe question is not whether you have automation. The question is whether you have control‚Äîand whether you can see what that control is doing when things go wrong."},{"slug":"blog_post_design_reviews","title":"What Experience Looks Like in Design Reviews","date":"2025-12-21","excerpt":"Inexperienced engineers talk a lot in design reviews. Experienced ones often don't. This is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/design-review2.png","tags":["design","experience","reviews","engineering"],"likes":0,"comments":0,"content":"\u003cp\u003eInexperienced engineers tend to talk a lot in design reviews. Experienced ones often don\u0026#39;t.\u003c/p\u003e\n\u003cp\u003eThis is frequently misinterpreted as disengagement. It isn\u0026#39;t. It\u0026#39;s pattern recognition at work.\u003c/p\u003e\n\u003cp\u003eA junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There\u0026#39;s error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.\u003c/p\u003e\n\u003cp\u003eThe team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.\u003c/p\u003e\n\u003cp\u003eThirty minutes in, the senior controls engineer‚Äîwho\u0026#39;s been silent the entire time‚Äîasks: \u0026quot;What happens when the press cycles faster than the sensor settling time?\u0026quot;\u003c/p\u003e\n\u003cp\u003ePause.\u003c/p\u003e\n\u003cp\u003e\u0026quot;The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;What happens when production pushes it to 75 strokes per minute to meet a deadline?\u0026quot;\u003c/p\u003e\n\u003cp\u003eLonger pause.\u003c/p\u003e\n\u003cp\u003eThat question wasn\u0026#39;t on any slide. It wasn\u0026#39;t in the requirements. But everyone in the room who\u0026#39;s been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between \u0026quot;rated for\u0026quot; and \u0026quot;tested at.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience changes what you listen for.\u003c/p\u003e\n\u003ch2\u003eThe Questions That Matter Change Over Time\u003c/h2\u003e\n\u003cp\u003eEarly in a career, design reviews focus on how something works.\u003c/p\u003e\n\u003cp\u003eIs the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?\u003c/p\u003e\n\u003cp\u003eThese are legitimate questions. They matter. But with experience, attention shifts to different territory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat assumptions are being made about the environment?\u003c/li\u003e\n\u003cli\u003eWhich constraints are fixed, and which are imagined?\u003c/li\u003e\n\u003cli\u003eWhat happens when this fails at the worst possible time?\u003c/li\u003e\n\u003cli\u003eWho will be awake at 3 AM when it does?\u003c/li\u003e\n\u003cli\u003eHow will we know it\u0026#39;s failing before it\u0026#39;s catastrophic?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese questions don\u0026#39;t show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.\u003c/p\u003e\n\u003cp\u003eI remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.\u003c/p\u003e\n\u003cp\u003eWhat I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values‚Äîand a tank overheated. The system worked exactly as designed. The design just didn\u0026#39;t account for sensors that fail by lying rather than going silent.\u003c/p\u003e\n\u003cp\u003eAn experienced engineer would have asked: \u0026quot;What does a failing sensor look like to this algorithm?\u0026quot; Not because they\u0026#39;re smarter, but because they\u0026#39;ve seen that failure mode before. Probably more than once.\u003c/p\u003e\n\u003ch2\u003eSilence Is a Signal\u003c/h2\u003e\n\u003cp\u003eExperienced engineers are often quiet during the early part of a review.\u003c/p\u003e\n\u003cp\u003eThey let the design present itself. They listen for what is emphasized‚Äîand what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?\u003c/p\u003e\n\u003cp\u003eWhen they finally speak, it is rarely to propose an alternative design. It is to test the edges:\u003c/p\u003e\n\u003cp\u003e\u0026quot;What happens if Modbus requests start taking 200ms instead of 50ms?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;How do we know this component is failing versus just slow?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;Who owns the configuration updates when this is in production?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;What\u0026#39;s the plan when this needs to change and you\u0026#39;re not here?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThese are not clever questions. They are uncomfortable ones. They don\u0026#39;t have satisfying technical answers because they\u0026#39;re not really technical questions‚Äîthey\u0026#39;re questions about risk, ownership, and operational reality.\u003c/p\u003e\n\u003cp\u003eThe inexperienced reviewer says: \u0026quot;Have you considered using CRC32 instead of CRC16 for better error detection?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe experienced reviewer says: \u0026quot;What happens when the checksum passes but the data is still wrong?\u0026quot;\u003c/p\u003e\n\u003cp\u003eBoth questions are about data integrity. Only one is about what actually happens in production.\u003c/p\u003e\n\u003ch2\u003eExperience Recognizes Deferred Decisions\u003c/h2\u003e\n\u003cp\u003eMany designs appear solid because they defer hard decisions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRetry logic described as \u0026quot;exponential backoff with appropriate limits\u0026quot; but never defined\u003c/li\u003e\n\u003cli\u003eOperational ownership assumed but not assigned\u003c/li\u003e\n\u003cli\u003eFailure handling described vaguely as \u0026quot;graceful degradation\u0026quot;\u003c/li\u003e\n\u003cli\u003eScaling concerns waved away with \u0026quot;we\u0026#39;ll monitor and adjust\u0026quot;\u003c/li\u003e\n\u003cli\u003eIntegration complexity acknowledged but postponed to \u0026quot;phase two\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExperience recognizes deferral instantly‚Äînot because deferral is always wrong, but because deferred decisions always come back with interest.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll tune the timeouts in production\u0026quot; sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll add more detailed logging if needed\u0026quot; sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.\u003c/p\u003e\n\u003cp\u003e\u0026quot;We\u0026#39;ll clarify ownership during deployment\u0026quot; sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.\u003c/p\u003e\n\u003cp\u003eProduction is very patient about collecting on these debts.\u003c/p\u003e\n\u003cp\u003eI watched a team defer a decision about how to handle partial sensor array failures. \u0026quot;We\u0026#39;ll see how it behaves in production and tune the algorithm.\u0026quot; Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn\u0026#39;t designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.\u003c/p\u003e\n\u003cp\u003eThe cost of deferring that decision was higher than the cost of making it wrong during design would have been.\u003c/p\u003e\n\u003ch2\u003eThe Absence of Overconfidence\u003c/h2\u003e\n\u003cp\u003eOne of the clearest markers of experience is restraint.\u003c/p\u003e\n\u003cp\u003eExperienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will‚Äîand the ways it might not.\u003c/p\u003e\n\u003cp\u003eThis is sometimes mistaken for pessimism. It isn\u0026#39;t. It is respect for complexity.\u003c/p\u003e\n\u003cp\u003eConfidence says: \u0026quot;This will handle sensor failures.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience says: \u0026quot;This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We\u0026#39;ve added bounds checking for the first case. The other two will require operational awareness.\u0026quot;\u003c/p\u003e\n\u003cp\u003eConfidence says: \u0026quot;This will scale to 100 nodes.\u0026quot;\u003c/p\u003e\n\u003cp\u003eExperience says: \u0026quot;This will scale to 100 nodes if network latency stays below 50ms and nodes don\u0026#39;t join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We\u0026#39;ll see it as increased jitter in the timing measurements.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe difference is not about being negative. It\u0026#39;s about being specific about what has been designed for and what has been assumed away.\u003c/p\u003e\n\u003cp\u003eWhen someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn\u0026#39;t understood the problem space well enough to see the edges.\u003c/p\u003e\n\u003cp\u003eMost problems are not trivial.\u003c/p\u003e\n\u003ch2\u003eOwnership Reveals Maturity\u003c/h2\u003e\n\u003cp\u003eDesigns that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.\u003c/p\u003e\n\u003cp\u003eExperienced reviewers push on this immediately.\u003c/p\u003e\n\u003cp\u003eWho owns this in production? Who gets paged when it misbehaves? Who can say \u0026quot;no\u0026quot; when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?\u003c/p\u003e\n\u003cp\u003eIf ownership is unclear, the design is incomplete‚Äîno matter how elegant the code.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve seen systems designed by committee where every component had a different owner, and the interfaces between components were \u0026quot;shared responsibility.\u0026quot; Shared responsibility is another way of saying \u0026quot;no one is responsible.\u0026quot;\u003c/p\u003e\n\u003cp\u003eWhen that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system\u0026#39;s behavior was the outcome of those negotiations, not of any coherent design intent.\u003c/p\u003e\n\u003cp\u003eThe experienced engineer asks: \u0026quot;Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?\u0026quot;\u003c/p\u003e\n\u003cp\u003eIf the answer is \u0026quot;well, it depends on which part is having issues,\u0026quot; the ownership model is broken.\u003c/p\u003e\n\u003cp\u003eOwnership isn\u0026#39;t about credit. It\u0026#39;s about who carries the mental model of the system\u0026#39;s actual behavior‚Äînot its documented behavior, its actual behavior‚Äîand has the authority to act on that understanding.\u003c/p\u003e\n\u003ch2\u003eExperience Is Often Mistaken for Negativity\u003c/h2\u003e\n\u003cp\u003eThe most experienced person in the room is often labeled \u0026quot;difficult\u0026quot; at least once.\u003c/p\u003e\n\u003cp\u003eThey ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn\u0026#39;t include time for learning what was gotten wrong.\u003c/p\u003e\n\u003cp\u003eThis is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.\u003c/p\u003e\n\u003cp\u003eWhat they are really doing is defending the future team from the present team\u0026#39;s optimism.\u003c/p\u003e\n\u003cp\u003eThat is rarely popular in the moment. It is deeply appreciated later‚Äîusually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn\u0026#39;t want to answer during the review has become an urgent problem with no good solutions.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve been that \u0026quot;difficult\u0026quot; person. I\u0026#39;ve asked questions that made presenters defensive. I\u0026#39;ve pointed out gaps that felt like criticisms. I\u0026#39;ve watched the energy in the room shift from enthusiasm to frustration.\u003c/p\u003e\n\u003cp\u003eAnd I\u0026#39;ve also gotten emails, months later, from those same presenters: \u0026quot;Remember when you asked about [that thing]? It happened. We were ready because we\u0026#39;d thought about it. Thank you.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe experienced reviewer is not trying to stop the design. They are trying to make it survivable.\u003c/p\u003e\n\u003ch2\u003eWhy Design Reviews Fail\u003c/h2\u003e\n\u003cp\u003eMost failed design reviews fail quietly.\u003c/p\u003e\n\u003cp\u003eThey approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.\u003c/p\u003e\n\u003cp\u003eAnd then production teaches the lesson that the review avoided.\u003c/p\u003e\n\u003cp\u003eDesign reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.\u003c/p\u003e\n\u003cp\u003eGood design reviews do not prevent failure. They prevent surprise.\u003c/p\u003e\n\u003cp\u003eA good review leaves everyone with a shared understanding of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat has been designed for\u003c/li\u003e\n\u003cli\u003eWhat has been assumed away\u003c/li\u003e\n\u003cli\u003eWhere the edges are\u003c/li\u003e\n\u003cli\u003eWho owns what happens at those edges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt does not guarantee success. It makes failure less catastrophic and more recoverable.\u003c/p\u003e\n\u003ch2\u003eA Final Thought\u003c/h2\u003e\n\u003cp\u003eExperience does not make you smarter in design reviews. It makes you more selective.\u003c/p\u003e\n\u003cp\u003eYou stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you\u0026#39;re gone.\u003c/p\u003e\n\u003cp\u003eThat shift is subtle. And once you see it, you can\u0026#39;t unsee it.\u003c/p\u003e\n\u003cp\u003eYou start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions‚Äînot because you\u0026#39;re difficult, but because you\u0026#39;ve seen what happens when no one asks them.\u003c/p\u003e\n\u003cp\u003eAnd you realize that the best design reviews are not the ones where everyone agrees.\u003c/p\u003e\n\u003cp\u003eThey are the ones where everyone leaves slightly uncomfortable, but with clarity about why.\u003c/p\u003e\n","rawContent":"\nInexperienced engineers tend to talk a lot in design reviews. Experienced ones often don't.\n\nThis is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.\n\nA junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There's error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.\n\nThe team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.\n\nThirty minutes in, the senior controls engineer‚Äîwho's been silent the entire time‚Äîasks: \"What happens when the press cycles faster than the sensor settling time?\"\n\nPause.\n\n\"The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.\"\n\n\"What happens when production pushes it to 75 strokes per minute to meet a deadline?\"\n\nLonger pause.\n\nThat question wasn't on any slide. It wasn't in the requirements. But everyone in the room who's been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between \"rated for\" and \"tested at.\"\n\nExperience changes what you listen for.\n\n## The Questions That Matter Change Over Time\n\nEarly in a career, design reviews focus on how something works.\n\nIs the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?\n\nThese are legitimate questions. They matter. But with experience, attention shifts to different territory:\n\n- What assumptions are being made about the environment?\n- Which constraints are fixed, and which are imagined?\n- What happens when this fails at the worst possible time?\n- Who will be awake at 3 AM when it does?\n- How will we know it's failing before it's catastrophic?\n\nThese questions don't show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.\n\nI remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.\n\nWhat I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values‚Äîand a tank overheated. The system worked exactly as designed. The design just didn't account for sensors that fail by lying rather than going silent.\n\nAn experienced engineer would have asked: \"What does a failing sensor look like to this algorithm?\" Not because they're smarter, but because they've seen that failure mode before. Probably more than once.\n\n## Silence Is a Signal\n\nExperienced engineers are often quiet during the early part of a review.\n\nThey let the design present itself. They listen for what is emphasized‚Äîand what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?\n\nWhen they finally speak, it is rarely to propose an alternative design. It is to test the edges:\n\n\"What happens if Modbus requests start taking 200ms instead of 50ms?\"\n\n\"How do we know this component is failing versus just slow?\"\n\n\"Who owns the configuration updates when this is in production?\"\n\n\"What's the plan when this needs to change and you're not here?\"\n\nThese are not clever questions. They are uncomfortable ones. They don't have satisfying technical answers because they're not really technical questions‚Äîthey're questions about risk, ownership, and operational reality.\n\nThe inexperienced reviewer says: \"Have you considered using CRC32 instead of CRC16 for better error detection?\"\n\nThe experienced reviewer says: \"What happens when the checksum passes but the data is still wrong?\"\n\nBoth questions are about data integrity. Only one is about what actually happens in production.\n\n## Experience Recognizes Deferred Decisions\n\nMany designs appear solid because they defer hard decisions:\n\n- Retry logic described as \"exponential backoff with appropriate limits\" but never defined\n- Operational ownership assumed but not assigned\n- Failure handling described vaguely as \"graceful degradation\"\n- Scaling concerns waved away with \"we'll monitor and adjust\"\n- Integration complexity acknowledged but postponed to \"phase two\"\n\nExperience recognizes deferral instantly‚Äînot because deferral is always wrong, but because deferred decisions always come back with interest.\n\n\"We'll tune the timeouts in production\" sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.\n\n\"We'll add more detailed logging if needed\" sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.\n\n\"We'll clarify ownership during deployment\" sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.\n\nProduction is very patient about collecting on these debts.\n\nI watched a team defer a decision about how to handle partial sensor array failures. \"We'll see how it behaves in production and tune the algorithm.\" Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn't designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.\n\nThe cost of deferring that decision was higher than the cost of making it wrong during design would have been.\n\n## The Absence of Overconfidence\n\nOne of the clearest markers of experience is restraint.\n\nExperienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will‚Äîand the ways it might not.\n\nThis is sometimes mistaken for pessimism. It isn't. It is respect for complexity.\n\nConfidence says: \"This will handle sensor failures.\"\n\nExperience says: \"This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We've added bounds checking for the first case. The other two will require operational awareness.\"\n\nConfidence says: \"This will scale to 100 nodes.\"\n\nExperience says: \"This will scale to 100 nodes if network latency stays below 50ms and nodes don't join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We'll see it as increased jitter in the timing measurements.\"\n\nThe difference is not about being negative. It's about being specific about what has been designed for and what has been assumed away.\n\nWhen someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn't understood the problem space well enough to see the edges.\n\nMost problems are not trivial.\n\n## Ownership Reveals Maturity\n\nDesigns that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.\n\nExperienced reviewers push on this immediately.\n\nWho owns this in production? Who gets paged when it misbehaves? Who can say \"no\" when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?\n\nIf ownership is unclear, the design is incomplete‚Äîno matter how elegant the code.\n\nI've seen systems designed by committee where every component had a different owner, and the interfaces between components were \"shared responsibility.\" Shared responsibility is another way of saying \"no one is responsible.\"\n\nWhen that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system's behavior was the outcome of those negotiations, not of any coherent design intent.\n\nThe experienced engineer asks: \"Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?\"\n\nIf the answer is \"well, it depends on which part is having issues,\" the ownership model is broken.\n\nOwnership isn't about credit. It's about who carries the mental model of the system's actual behavior‚Äînot its documented behavior, its actual behavior‚Äîand has the authority to act on that understanding.\n\n## Experience Is Often Mistaken for Negativity\n\nThe most experienced person in the room is often labeled \"difficult\" at least once.\n\nThey ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn't include time for learning what was gotten wrong.\n\nThis is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.\n\nWhat they are really doing is defending the future team from the present team's optimism.\n\nThat is rarely popular in the moment. It is deeply appreciated later‚Äîusually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn't want to answer during the review has become an urgent problem with no good solutions.\n\nI've been that \"difficult\" person. I've asked questions that made presenters defensive. I've pointed out gaps that felt like criticisms. I've watched the energy in the room shift from enthusiasm to frustration.\n\nAnd I've also gotten emails, months later, from those same presenters: \"Remember when you asked about [that thing]? It happened. We were ready because we'd thought about it. Thank you.\"\n\nThe experienced reviewer is not trying to stop the design. They are trying to make it survivable.\n\n## Why Design Reviews Fail\n\nMost failed design reviews fail quietly.\n\nThey approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.\n\nAnd then production teaches the lesson that the review avoided.\n\nDesign reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.\n\nGood design reviews do not prevent failure. They prevent surprise.\n\nA good review leaves everyone with a shared understanding of:\n- What has been designed for\n- What has been assumed away\n- Where the edges are\n- Who owns what happens at those edges\n\nIt does not guarantee success. It makes failure less catastrophic and more recoverable.\n\n## A Final Thought\n\nExperience does not make you smarter in design reviews. It makes you more selective.\n\nYou stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you're gone.\n\nThat shift is subtle. And once you see it, you can't unsee it.\n\nYou start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions‚Äînot because you're difficult, but because you've seen what happens when no one asks them.\n\nAnd you realize that the best design reviews are not the ones where everyone agrees.\n\nThey are the ones where everyone leaves slightly uncomfortable, but with clarity about why."},{"slug":"blog_post_first_incident","title":"The First Incident Changes Everything","date":"2025-12-20","excerpt":"There's a moment when ownership stops being theoretical and becomes visceral. It usually happens at 3 AM, with a production line stopped and people asking questions you don't have answers to yet.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/first-incident.png","tags":["incidents","ownership","production","systems"],"likes":0,"comments":0,"content":"\u003cp\u003eThe pager goes off at 2:47 AM. You\u0026#39;re the firmware lead for a new motor control system that\u0026#39;s been in production for six weeks. It\u0026#39;s been running fine. No issues. And then: \u0026quot;Line 3 emergency stop. Controllers unresponsive. Production halted.\u0026quot;\u003c/p\u003e\n\u003cp\u003eYou\u0026#39;re on a call by 2:52. The plant manager is already there‚Äîin person, at the facility, at 3 AM. The line is still down. Every minute costs money you\u0026#39;ve only thought about abstractly until now. The operators are watching. Maintenance is standing by. And everyone is looking at your system.\u003c/p\u003e\n\u003cp\u003eNot metaphorically looking. Actually standing in front of the cabinet, pointing at LEDs, asking what they mean.\u003c/p\u003e\n\u003cp\u003eYou realize, with a clarity that\u0026#39;s almost physical, that you don\u0026#39;t know. You know what they\u0026#39;re supposed to mean. You wrote the documentation. But right now, with the system in an unknown state, you\u0026#39;re not sure. You\u0026#39;re running mental simulations, trying to map LED patterns to states, wondering if the watchdog triggered or if it\u0026#39;s a CAN bus fault or if something else entirely happened.\u003c/p\u003e\n\u003cp\u003eThe plant manager asks: \u0026quot;How long until we\u0026#39;re back up?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThis is the moment. Not the moment you become an experienced engineer. The moment you realize you weren\u0026#39;t one yet.\u003c/p\u003e\n\u003ch2\u003eWhen Ownership Becomes Real\u003c/h2\u003e\n\u003cp\u003eBefore your first real incident, ownership is theoretical. You own the firmware, sure. Your name is on the commit history. You wrote the architecture doc. You presented at the design review.\u003c/p\u003e\n\u003cp\u003eBut that\u0026#39;s ownership as authorship. Ownership as credit.\u003c/p\u003e\n\u003cp\u003eReal ownership is different. Real ownership is: when this breaks, you get called. When it misbehaves, you explain why. When someone needs to make a decision about whether to push forward or shut down, they\u0026#39;re looking at you for information that might not exist yet.\u003c/p\u003e\n\u003cp\u003eThe shift happens fast. One moment you\u0026#39;re proud of what you built. The next moment you\u0026#39;re responsible for it in a way that makes pride seem quaint.\u003c/p\u003e\n\u003cp\u003eI remember my first significant incident. A PLC I\u0026#39;d written firmware for started dropping Modbus requests unpredictably. Not often‚Äîmaybe one in ten thousand. Enough to be noticeable in production but rare enough that we\u0026#39;d never caught it in testing. The system had error handling. It would retry. But the retries added latency, and that latency cascaded into other systems, and suddenly a water treatment facility was dealing with tank levels that were drifting outside normal operating ranges.\u003c/p\u003e\n\u003cp\u003eNo one was angry. Everyone was professional. But I could feel the weight of it: this wasn\u0026#39;t my code anymore. It was their water treatment system. Their compliance requirements. Their operators who had to explain to management why levels were fluctuating.\u003c/p\u003e\n\u003cp\u003eThe code was the same. The system was the same. But what it meant was different.\u003c/p\u003e\n\u003ch2\u003eHow Teams Behave Under Public Failure\u003c/h2\u003e\n\u003cp\u003eHere\u0026#39;s something they don\u0026#39;t teach you: teams change during incidents.\u003c/p\u003e\n\u003cp\u003eThe usual dynamics suspend. The person who never speaks up in meetings suddenly has crucial information. The senior engineer who usually drives decisions steps back and lets the person closest to the problem lead. Or sometimes the opposite happens‚Äîsomeone who\u0026#39;s normally collaborative becomes territorial, defensive.\u003c/p\u003e\n\u003cp\u003eYou learn who stays calm and who doesn\u0026#39;t. Who asks clarifying questions and who jumps to conclusions. Who admits when they don\u0026#39;t know something and who deflects. Who thinks about the system and who thinks about blame.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026#39;t about good people versus bad people. It\u0026#39;s about how pressure reveals what people actually optimize for when the stakes are real.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve watched a junior technician with three months on the job diagnose a problem faster than anyone else because they\u0026#39;d been the one doing the daily rounds and noticed a pattern no one else had seen. I\u0026#39;ve watched a principal engineer who designed the original system defer to an operator who knew which breaker \u0026quot;acted funny\u0026quot; in a way no documentation captured.\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve also watched teams spiral. Someone suggests a theory, and instead of testing it, everyone starts building on it. Someone makes a change without announcing it, and now two people are modifying the same system with different assumptions. Someone gets frustrated and starts blaming the vendor, the previous team, the budget constraints‚Äîanything except the current problem.\u003c/p\u003e\n\u003cp\u003eThe best incident response I\u0026#39;ve seen wasn\u0026#39;t the fastest. It was the most deliberate. Someone‚Äîit happened to be a controls engineer who\u0026#39;d been with the company for fifteen years‚Äîsaid: \u0026quot;We don\u0026#39;t understand what state we\u0026#39;re in. Before we change anything, let\u0026#39;s gather data.\u0026quot; And then, critically: \u0026quot;Who\u0026#39;s writing this down?\u0026quot;\u003c/p\u003e\n\u003cp\u003eThat changed everything. Not because documentation mattered in the moment, but because it forced everyone to externalize their mental model. When you have to say out loud what you think is happening and someone writes it down, you think differently. You catch your own assumptions.\u003c/p\u003e\n\u003ch2\u003eWhat Broke Wasn\u0026#39;t the System\u003c/h2\u003e\n\u003cp\u003eAfter the incident, there\u0026#39;s always a postmortem. And almost always, the postmortem focuses on the technical failure.\u003c/p\u003e\n\u003cp\u003eIn that motor control incident at 3 AM, the technical root cause was clear: a race condition in the watchdog handling code. Under specific timing conditions, the watchdog could timeout while a critical section was locked, and the recovery path assumed clean state that didn\u0026#39;t exist. Textbook bug. We fixed it in forty lines of code.\u003c/p\u003e\n\u003cp\u003eBut that wasn\u0026#39;t what broke.\u003c/p\u003e\n\u003cp\u003eWhat broke was that the firmware had been written with assumptions about the execution environment that were true on the test bench and false in production. The test bench had consistent loop timing. Production had variable timing because of interrupt patterns we hadn\u0026#39;t characterized. We\u0026#39;d tested the watchdog recovery path, but we\u0026#39;d tested it by triggering the watchdog deliberately, not by letting it trigger from timing variance.\u003c/p\u003e\n\u003cp\u003eOkay, but that still sounds technical. Go deeper.\u003c/p\u003e\n\u003cp\u003eWhy didn\u0026#39;t we characterize the interrupt patterns in production? Because commissioning was scheduled tight, and characterization takes time, and we had twelve other systems to bring up. \u003c/p\u003e\n\u003cp\u003eWhy was commissioning scheduled tight? Because the project was already over budget, and the plant needed the new line operational before the next quarter.\u003c/p\u003e\n\u003cp\u003eWhy was the project over budget? Partly because the scope grew during implementation, partly because integration with legacy systems took longer than estimated, partly because we\u0026#39;d optimized the bid to win the contract.\u003c/p\u003e\n\u003cp\u003eWho decided that the commissioning schedule was more important than thorough characterization? No one, explicitly. It was a collective drift. A series of small decisions that each made sense locally but accumulated into a gap where critical testing didn\u0026#39;t happen.\u003c/p\u003e\n\u003cp\u003eHere\u0026#39;s what actually broke: the communication path between the firmware team and the commissioning team. We didn\u0026#39;t have a clear handoff protocol for \u0026quot;things that need to be validated in production that we can\u0026#39;t validate in the lab.\u0026quot; That\u0026#39;s not a technical problem. That\u0026#39;s an organizational design problem.\u003c/p\u003e\n\u003cp\u003eAnd beneath that: the incentive structure. Commissioning engineers were measured on schedule adherence. Firmware engineers were measured on feature completion. No one was measured on \u0026quot;probability we understand the system\u0026#39;s actual behavior in production.\u0026quot;\u003c/p\u003e\n\u003cp\u003eSo we didn\u0026#39;t do it. Not because anyone was negligent. Because the organization wasn\u0026#39;t designed to value it.\u003c/p\u003e\n\u003ch2\u003eWhy Technical Postmortems Often Miss the Point\u003c/h2\u003e\n\u003cp\u003eMost postmortems I\u0026#39;ve read follow a pattern:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIncident summary\u003c/strong\u003e: What happened and when\u003cbr\u003e\u003cstrong\u003eImpact\u003c/strong\u003e: Who was affected and how\u003cbr\u003e\u003cstrong\u003eRoot cause\u003c/strong\u003e: The technical failure\u003cbr\u003e\u003cstrong\u003eContributing factors\u003c/strong\u003e: Other technical issues\u003cbr\u003e\u003cstrong\u003eAction items\u003c/strong\u003e: Technical fixes\u003c/p\u003e\n\u003cp\u003eThis format is comfortable because it suggests the problem was technical and therefore fixable. Change the code, add a check, improve the test suite. Done.\u003c/p\u003e\n\u003cp\u003eBut look at what\u0026#39;s missing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy did the design review not catch this failure mode?\u003c/li\u003e\n\u003cli\u003eWhat information existed that wasn\u0026#39;t shared?\u003c/li\u003e\n\u003cli\u003eWhich tradeoffs were made consciously versus unconsciously?\u003c/li\u003e\n\u003cli\u003eWhat incentives led to those tradeoffs?\u003c/li\u003e\n\u003cli\u003eHow did the team\u0026#39;s understanding of the system differ from reality?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese questions are harder because the answers implicate decisions, not just code. They make people uncomfortable because they suggest the incident wasn\u0026#39;t just bad luck or an obscure bug‚Äîit was predictable given how the system was designed and how the organization operates.\u003c/p\u003e\n\u003cp\u003eI once sat in a postmortem for a system that failed because of insufficient input validation. The root cause was listed as \u0026quot;missing bounds check.\u0026quot; The action item was \u0026quot;add bounds checking to all inputs.\u0026quot;\u003c/p\u003e\n\u003cp\u003eBut the actual story was different. The bounds checking had been in the original design. It was removed during optimization because it added 2ms to the control loop latency, and the spec required sub-10ms response time. The decision to remove it was documented in a code review but not escalated. The person who reviewed it assumed someone else would catch it in integration testing. The integration testing was abbreviated because commissioning was behind schedule.\u003c/p\u003e\n\u003cp\u003eThe missing bounds check was the mechanism of failure. But what failed was a web of assumptions about who was responsible for verifying what, under what time pressure, with what communication overhead.\u003c/p\u003e\n\u003cp\u003eFixing the bounds check addressed the symptom. It didn\u0026#39;t address why a known-critical validation got removed without proper scrutiny.\u003c/p\u003e\n\u003ch2\u003e\u0026quot;The System Worked Exactly as Designed\u0026quot;\u003c/h2\u003e\n\u003cp\u003eThis phrase appears in postmortems as a conclusion. It\u0026#39;s meant to close the investigation: there was no malfunction, just an unanticipated scenario.\u003c/p\u003e\n\u003cp\u003eBut it\u0026#39;s the most dangerous phrase in incident analysis because it\u0026#39;s almost always true‚Äîand almost always misunderstood.\u003c/p\u003e\n\u003cp\u003eYes, the system worked as designed. The problem is that \u0026quot;the design\u0026quot; includes far more than you documented.\u003c/p\u003e\n\u003cp\u003eIt includes the implicit assumptions you made about the operating environment. The tradeoffs you accepted to meet the schedule. The risks you deprioritized because they seemed unlikely. The monitoring you didn\u0026#39;t implement because it wasn\u0026#39;t in scope. The operational knowledge that exists only in people\u0026#39;s heads.\u003c/p\u003e\n\u003cp\u003eWhen the motor controllers became unresponsive at 2:47 AM, they worked exactly as designed: the watchdog detected a timeout, triggered the recovery path, and entered a safe state. The recovery path was designed to reset the system to a known-good state. It did that.\u003c/p\u003e\n\u003cp\u003eWhat it didn\u0026#39;t do was account for the scenario where the \u0026quot;known-good state\u0026quot; wasn\u0026#39;t actually safe because other systems were depending on outputs that were now frozen at their last value. That wasn\u0026#39;t a bug in the recovery logic. That was a gap in the system-level design.\u003c/p\u003e\n\u003cp\u003eBut more than that: it was a gap in how we thought about the system. We\u0026#39;d designed the controller firmware in isolation, with defined interfaces. We\u0026#39;d assumed those interfaces were sufficient to capture the dependencies. They weren\u0026#39;t. The actual dependencies included timing relationships, implicit state machines in other systems, and operator expectations that weren\u0026#39;t written anywhere.\u003c/p\u003e\n\u003cp\u003eThe system worked exactly as designed. The design was incomplete.\u003c/p\u003e\n\u003ch2\u003eThe Difference Between Intent and Reality\u003c/h2\u003e\n\u003cp\u003eThere\u0026#39;s a moment during every significant incident where you realize: we built what we said we\u0026#39;d build. We just didn\u0026#39;t build what was needed.\u003c/p\u003e\n\u003cp\u003eNot because we were incompetent. Because reality is richer than specification.\u003c/p\u003e\n\u003cp\u003eThe spec said \u0026quot;handle watchdog timeout.\u0026quot; It didn\u0026#39;t say \u0026quot;handle watchdog timeout in a way that doesn\u0026#39;t leave dependent systems in ambiguous state.\u0026quot; That seemed implied. It wasn\u0026#39;t.\u003c/p\u003e\n\u003cp\u003eThe spec said \u0026quot;buffer sensor data during communication loss.\u0026quot; It didn\u0026#39;t say \u0026quot;buffer data in a way that\u0026#39;s meaningful when different sensors lose communication at different times.\u0026quot; We assumed synchronization. The assumption was wrong.\u003c/p\u003e\n\u003cp\u003eThe spec said \u0026quot;provide HMI feedback for fault conditions.\u0026quot; It didn\u0026#39;t say \u0026quot;provide feedback that operators can interpret correctly at 3 AM when multiple faults are active simultaneously.\u0026quot; We tested each fault individually. They don\u0026#39;t happen individually in production.\u003c/p\u003e\n\u003cp\u003eIntent and reality diverge in the space between what you specify and what you assume. And you don\u0026#39;t discover what you assumed until it\u0026#39;s tested in an environment that doesn\u0026#39;t share your assumptions.\u003c/p\u003e\n\u003ch2\u003eWhat Changes After\u003c/h2\u003e\n\u003cp\u003eThe first incident changes everything because it changes what \u0026quot;working\u0026quot; means.\u003c/p\u003e\n\u003cp\u003eBefore: working meant passing tests, meeting specifications, behaving correctly under expected conditions.\u003c/p\u003e\n\u003cp\u003eAfter: working means remaining understandable and recoverable when conditions aren\u0026#39;t expected. When you don\u0026#39;t have complete information. When the people responding aren\u0026#39;t the people who built it. When time is expensive.\u003c/p\u003e\n\u003cp\u003eYou start designing differently. Not necessarily better‚Äîsometimes the right design is still the simple one. But you design with the knowledge that the system will eventually teach you what you got wrong, and the question is whether you\u0026#39;ve made it possible to learn that lesson without catastrophic cost.\u003c/p\u003e\n\u003cp\u003eYou write different documentation. Not more documentation‚Äîmore documentation is often worse. But documentation that captures why, not just what. Documentation that names the assumptions. Documentation that future-you, woken up at 3 AM and trying to understand what state the system is in, will actually use.\u003c/p\u003e\n\u003cp\u003eYou test differently. Not just testing the happy path and the error paths. Testing the transitions. Testing what happens when error paths stack. Testing what happens when recovery takes longer than expected. Testing what happens when the test itself is wrong.\u003c/p\u003e\n\u003cp\u003eAnd you think about ownership differently. Not as credit, but as responsibility. Not as \u0026quot;I built this,\u0026quot; but as \u0026quot;I\u0026#39;m accountable for what this does in an environment I don\u0026#39;t control.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThat motor control system got fixed. The race condition was patched. We added instrumentation to characterize interrupt timing in production. We updated commissioning procedures to include the checks we\u0026#39;d skipped.\u003c/p\u003e\n\u003cp\u003eBut more than that: I stopped being surprised when systems behaved in ways I hadn\u0026#39;t anticipated. I started assuming they would. And I started designing for that assumption.\u003c/p\u003e\n\u003cp\u003eBecause the first incident taught me something no design review ever could: the system you build is not the system that runs in production.\u003c/p\u003e\n\u003cp\u003eProduction adds variables you didn\u0026#39;t model. Pressures you didn\u0026#39;t simulate. Dependencies you didn\u0026#39;t document. And people who need answers you don\u0026#39;t have yet.\u003c/p\u003e\n\u003cp\u003eThe question isn\u0026#39;t whether you\u0026#39;ll face that gap. The question is whether you\u0026#39;ve designed for it.\u003c/p\u003e\n\u003cp\u003eMost of the time, you haven\u0026#39;t.\u003c/p\u003e\n\u003cp\u003eAnd that\u0026#39;s okay‚Äîas long as you remember it.\u003c/p\u003e\n\u003cp\u003eThe first incident won\u0026#39;t be your last. But if you learn from it, the next one will be different.\u003c/p\u003e\n\u003cp\u003eNot easier. Just different.\u003c/p\u003e\n\u003cp\u003eAnd maybe, slowly, you\u0026#39;ll build systems that fail in ways that are less surprising, less catastrophic, and more recoverable.\u003c/p\u003e\n\u003cp\u003eThat\u0026#39;s not mastery. That\u0026#39;s just experience.\u003c/p\u003e\n\u003cp\u003eAnd it starts with that first call at 2:47 AM.\u003c/p\u003e\n","rawContent":"\nThe pager goes off at 2:47 AM. You're the firmware lead for a new motor control system that's been in production for six weeks. It's been running fine. No issues. And then: \"Line 3 emergency stop. Controllers unresponsive. Production halted.\"\n\nYou're on a call by 2:52. The plant manager is already there‚Äîin person, at the facility, at 3 AM. The line is still down. Every minute costs money you've only thought about abstractly until now. The operators are watching. Maintenance is standing by. And everyone is looking at your system.\n\nNot metaphorically looking. Actually standing in front of the cabinet, pointing at LEDs, asking what they mean.\n\nYou realize, with a clarity that's almost physical, that you don't know. You know what they're supposed to mean. You wrote the documentation. But right now, with the system in an unknown state, you're not sure. You're running mental simulations, trying to map LED patterns to states, wondering if the watchdog triggered or if it's a CAN bus fault or if something else entirely happened.\n\nThe plant manager asks: \"How long until we're back up?\"\n\nThis is the moment. Not the moment you become an experienced engineer. The moment you realize you weren't one yet.\n\n## When Ownership Becomes Real\n\nBefore your first real incident, ownership is theoretical. You own the firmware, sure. Your name is on the commit history. You wrote the architecture doc. You presented at the design review.\n\nBut that's ownership as authorship. Ownership as credit.\n\nReal ownership is different. Real ownership is: when this breaks, you get called. When it misbehaves, you explain why. When someone needs to make a decision about whether to push forward or shut down, they're looking at you for information that might not exist yet.\n\nThe shift happens fast. One moment you're proud of what you built. The next moment you're responsible for it in a way that makes pride seem quaint.\n\nI remember my first significant incident. A PLC I'd written firmware for started dropping Modbus requests unpredictably. Not often‚Äîmaybe one in ten thousand. Enough to be noticeable in production but rare enough that we'd never caught it in testing. The system had error handling. It would retry. But the retries added latency, and that latency cascaded into other systems, and suddenly a water treatment facility was dealing with tank levels that were drifting outside normal operating ranges.\n\nNo one was angry. Everyone was professional. But I could feel the weight of it: this wasn't my code anymore. It was their water treatment system. Their compliance requirements. Their operators who had to explain to management why levels were fluctuating.\n\nThe code was the same. The system was the same. But what it meant was different.\n\n## How Teams Behave Under Public Failure\n\nHere's something they don't teach you: teams change during incidents.\n\nThe usual dynamics suspend. The person who never speaks up in meetings suddenly has crucial information. The senior engineer who usually drives decisions steps back and lets the person closest to the problem lead. Or sometimes the opposite happens‚Äîsomeone who's normally collaborative becomes territorial, defensive.\n\nYou learn who stays calm and who doesn't. Who asks clarifying questions and who jumps to conclusions. Who admits when they don't know something and who deflects. Who thinks about the system and who thinks about blame.\n\nThis isn't about good people versus bad people. It's about how pressure reveals what people actually optimize for when the stakes are real.\n\nI've watched a junior technician with three months on the job diagnose a problem faster than anyone else because they'd been the one doing the daily rounds and noticed a pattern no one else had seen. I've watched a principal engineer who designed the original system defer to an operator who knew which breaker \"acted funny\" in a way no documentation captured.\n\nI've also watched teams spiral. Someone suggests a theory, and instead of testing it, everyone starts building on it. Someone makes a change without announcing it, and now two people are modifying the same system with different assumptions. Someone gets frustrated and starts blaming the vendor, the previous team, the budget constraints‚Äîanything except the current problem.\n\nThe best incident response I've seen wasn't the fastest. It was the most deliberate. Someone‚Äîit happened to be a controls engineer who'd been with the company for fifteen years‚Äîsaid: \"We don't understand what state we're in. Before we change anything, let's gather data.\" And then, critically: \"Who's writing this down?\"\n\nThat changed everything. Not because documentation mattered in the moment, but because it forced everyone to externalize their mental model. When you have to say out loud what you think is happening and someone writes it down, you think differently. You catch your own assumptions.\n\n## What Broke Wasn't the System\n\nAfter the incident, there's always a postmortem. And almost always, the postmortem focuses on the technical failure.\n\nIn that motor control incident at 3 AM, the technical root cause was clear: a race condition in the watchdog handling code. Under specific timing conditions, the watchdog could timeout while a critical section was locked, and the recovery path assumed clean state that didn't exist. Textbook bug. We fixed it in forty lines of code.\n\nBut that wasn't what broke.\n\nWhat broke was that the firmware had been written with assumptions about the execution environment that were true on the test bench and false in production. The test bench had consistent loop timing. Production had variable timing because of interrupt patterns we hadn't characterized. We'd tested the watchdog recovery path, but we'd tested it by triggering the watchdog deliberately, not by letting it trigger from timing variance.\n\nOkay, but that still sounds technical. Go deeper.\n\nWhy didn't we characterize the interrupt patterns in production? Because commissioning was scheduled tight, and characterization takes time, and we had twelve other systems to bring up. \n\nWhy was commissioning scheduled tight? Because the project was already over budget, and the plant needed the new line operational before the next quarter.\n\nWhy was the project over budget? Partly because the scope grew during implementation, partly because integration with legacy systems took longer than estimated, partly because we'd optimized the bid to win the contract.\n\nWho decided that the commissioning schedule was more important than thorough characterization? No one, explicitly. It was a collective drift. A series of small decisions that each made sense locally but accumulated into a gap where critical testing didn't happen.\n\nHere's what actually broke: the communication path between the firmware team and the commissioning team. We didn't have a clear handoff protocol for \"things that need to be validated in production that we can't validate in the lab.\" That's not a technical problem. That's an organizational design problem.\n\nAnd beneath that: the incentive structure. Commissioning engineers were measured on schedule adherence. Firmware engineers were measured on feature completion. No one was measured on \"probability we understand the system's actual behavior in production.\"\n\nSo we didn't do it. Not because anyone was negligent. Because the organization wasn't designed to value it.\n\n## Why Technical Postmortems Often Miss the Point\n\nMost postmortems I've read follow a pattern:\n\n**Incident summary**: What happened and when  \n**Impact**: Who was affected and how  \n**Root cause**: The technical failure  \n**Contributing factors**: Other technical issues  \n**Action items**: Technical fixes\n\nThis format is comfortable because it suggests the problem was technical and therefore fixable. Change the code, add a check, improve the test suite. Done.\n\nBut look at what's missing:\n\n- Why did the design review not catch this failure mode?\n- What information existed that wasn't shared?\n- Which tradeoffs were made consciously versus unconsciously?\n- What incentives led to those tradeoffs?\n- How did the team's understanding of the system differ from reality?\n\nThese questions are harder because the answers implicate decisions, not just code. They make people uncomfortable because they suggest the incident wasn't just bad luck or an obscure bug‚Äîit was predictable given how the system was designed and how the organization operates.\n\nI once sat in a postmortem for a system that failed because of insufficient input validation. The root cause was listed as \"missing bounds check.\" The action item was \"add bounds checking to all inputs.\"\n\nBut the actual story was different. The bounds checking had been in the original design. It was removed during optimization because it added 2ms to the control loop latency, and the spec required sub-10ms response time. The decision to remove it was documented in a code review but not escalated. The person who reviewed it assumed someone else would catch it in integration testing. The integration testing was abbreviated because commissioning was behind schedule.\n\nThe missing bounds check was the mechanism of failure. But what failed was a web of assumptions about who was responsible for verifying what, under what time pressure, with what communication overhead.\n\nFixing the bounds check addressed the symptom. It didn't address why a known-critical validation got removed without proper scrutiny.\n\n## \"The System Worked Exactly as Designed\"\n\nThis phrase appears in postmortems as a conclusion. It's meant to close the investigation: there was no malfunction, just an unanticipated scenario.\n\nBut it's the most dangerous phrase in incident analysis because it's almost always true‚Äîand almost always misunderstood.\n\nYes, the system worked as designed. The problem is that \"the design\" includes far more than you documented.\n\nIt includes the implicit assumptions you made about the operating environment. The tradeoffs you accepted to meet the schedule. The risks you deprioritized because they seemed unlikely. The monitoring you didn't implement because it wasn't in scope. The operational knowledge that exists only in people's heads.\n\nWhen the motor controllers became unresponsive at 2:47 AM, they worked exactly as designed: the watchdog detected a timeout, triggered the recovery path, and entered a safe state. The recovery path was designed to reset the system to a known-good state. It did that.\n\nWhat it didn't do was account for the scenario where the \"known-good state\" wasn't actually safe because other systems were depending on outputs that were now frozen at their last value. That wasn't a bug in the recovery logic. That was a gap in the system-level design.\n\nBut more than that: it was a gap in how we thought about the system. We'd designed the controller firmware in isolation, with defined interfaces. We'd assumed those interfaces were sufficient to capture the dependencies. They weren't. The actual dependencies included timing relationships, implicit state machines in other systems, and operator expectations that weren't written anywhere.\n\nThe system worked exactly as designed. The design was incomplete.\n\n## The Difference Between Intent and Reality\n\nThere's a moment during every significant incident where you realize: we built what we said we'd build. We just didn't build what was needed.\n\nNot because we were incompetent. Because reality is richer than specification.\n\nThe spec said \"handle watchdog timeout.\" It didn't say \"handle watchdog timeout in a way that doesn't leave dependent systems in ambiguous state.\" That seemed implied. It wasn't.\n\nThe spec said \"buffer sensor data during communication loss.\" It didn't say \"buffer data in a way that's meaningful when different sensors lose communication at different times.\" We assumed synchronization. The assumption was wrong.\n\nThe spec said \"provide HMI feedback for fault conditions.\" It didn't say \"provide feedback that operators can interpret correctly at 3 AM when multiple faults are active simultaneously.\" We tested each fault individually. They don't happen individually in production.\n\nIntent and reality diverge in the space between what you specify and what you assume. And you don't discover what you assumed until it's tested in an environment that doesn't share your assumptions.\n\n## What Changes After\n\nThe first incident changes everything because it changes what \"working\" means.\n\nBefore: working meant passing tests, meeting specifications, behaving correctly under expected conditions.\n\nAfter: working means remaining understandable and recoverable when conditions aren't expected. When you don't have complete information. When the people responding aren't the people who built it. When time is expensive.\n\nYou start designing differently. Not necessarily better‚Äîsometimes the right design is still the simple one. But you design with the knowledge that the system will eventually teach you what you got wrong, and the question is whether you've made it possible to learn that lesson without catastrophic cost.\n\nYou write different documentation. Not more documentation‚Äîmore documentation is often worse. But documentation that captures why, not just what. Documentation that names the assumptions. Documentation that future-you, woken up at 3 AM and trying to understand what state the system is in, will actually use.\n\nYou test differently. Not just testing the happy path and the error paths. Testing the transitions. Testing what happens when error paths stack. Testing what happens when recovery takes longer than expected. Testing what happens when the test itself is wrong.\n\nAnd you think about ownership differently. Not as credit, but as responsibility. Not as \"I built this,\" but as \"I'm accountable for what this does in an environment I don't control.\"\n\nThat motor control system got fixed. The race condition was patched. We added instrumentation to characterize interrupt timing in production. We updated commissioning procedures to include the checks we'd skipped.\n\nBut more than that: I stopped being surprised when systems behaved in ways I hadn't anticipated. I started assuming they would. And I started designing for that assumption.\n\nBecause the first incident taught me something no design review ever could: the system you build is not the system that runs in production.\n\nProduction adds variables you didn't model. Pressures you didn't simulate. Dependencies you didn't document. And people who need answers you don't have yet.\n\nThe question isn't whether you'll face that gap. The question is whether you've designed for it.\n\nMost of the time, you haven't.\n\nAnd that's okay‚Äîas long as you remember it.\n\nThe first incident won't be your last. But if you learn from it, the next one will be different.\n\nNot easier. Just different.\n\nAnd maybe, slowly, you'll build systems that fail in ways that are less surprising, less catastrophic, and more recoverable.\n\nThat's not mastery. That's just experience.\n\nAnd it starts with that first call at 2:47 AM."},{"slug":"the-happy-post-lie","title":"The Happy Path Is a Lie We Tell Ourselves","date":"2025-12-01","excerpt":"Design reviews reward optimism. Test environments validate assumptions. And somewhere between approval and deployment, we forget that we're building systems for a world that doesn't care about our diagrams.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/design-review.png","tags":["systems","design","production","failure"],"likes":0,"comments":0,"content":"\u003cp\u003eThree engineers sit in a design review for a new temperature monitoring system. The architecture is clean: sensors report to edge controllers, controllers aggregate to a gateway, gateway publishes to the cloud. Redundancy at every layer. Graceful degradation clearly marked on the diagram.\u003c/p\u003e\n\u003cp\u003eSomeone asks: \u0026quot;What happens if the gateway loses connectivity?\u0026quot;\u003c/p\u003e\n\u003cp\u003e\u0026quot;It buffers locally for up to 72 hours,\u0026quot; comes the answer. \u0026quot;More than enough for any reasonable outage.\u0026quot;\u003c/p\u003e\n\u003cp\u003eEveryone nods. The design is approved.\u003c/p\u003e\n\u003cp\u003eTwo years later, a cellular provider pushes a configuration update that breaks connectivity for five days. The gateway\u0026#39;s flash memory fills in eighteen hours. When connectivity returns, 96 hours of critical temperature data is gone. The system worked exactly as designed‚Äîit just wasn\u0026#39;t designed for this.\u003c/p\u003e\n\u003cp\u003eThe happy path is seductive because it lets us move forward. But it\u0026#39;s also a lie we tell ourselves, and design reviews are where that lie gets institutionalized.\u003c/p\u003e\n\u003ch2\u003eWhy Design Reviews Reward Optimism\u003c/h2\u003e\n\u003cp\u003eDesign reviews are supposed to find problems. In practice, they often do the opposite: they create consensus around the assumption that problems won\u0026#39;t happen.\u003c/p\u003e\n\u003cp\u003eHere\u0026#39;s how it works:\u003c/p\u003e\n\u003cp\u003eYou present a design. You show the nominal flow. You mark the error paths. You explain the redundancies. And then the questions come‚Äîbut they come in a specific form: \u0026quot;What if X fails?\u0026quot; You answer with your contingency for X. \u0026quot;What if Y happens?\u0026quot; You show your handling of Y.\u003c/p\u003e\n\u003cp\u003eEach question asked and answered creates the illusion that you\u0026#39;ve covered the space of possible failures. What\u0026#39;s harder to see is the space of failures that weren\u0026#39;t asked about‚Äînot because they\u0026#39;re impossible, but because they\u0026#39;re hard to imagine from a conference room.\u003c/p\u003e\n\u003cp\u003eThe cellular configuration update that breaks connectivity for five days isn\u0026#39;t a question anyone asks, because connectivity outages are supposed to last hours, not days. The sensor that doesn\u0026#39;t fail cleanly but drifts slowly enough to stay within acceptance bounds for months isn\u0026#39;t on the checklist, because sensors are supposed to either work or fail obviously. The operator who learns to bypass an interlock because it trips too often during valid operations isn\u0026#39;t in the room, because we\u0026#39;re reviewing technical design, not operational reality.\u003c/p\u003e\n\u003cp\u003eDesign reviews optimize for demonstrable coverage of known failure modes. They reward the ability to show that you\u0026#39;ve thought about things. But \u0026quot;thinking about things\u0026quot; is different from designing for them, and both are different from experiencing them.\u003c/p\u003e\n\u003cp\u003eThis creates a subtle bias: the designs that pass review most easily are the ones that look most robust on paper. Clean boundaries. Clear error handling. Documented assumptions. The designs that acknowledge fundamental uncertainties‚Äî\u0026quot;we don\u0026#39;t know how operators will actually use this,\u0026quot; \u0026quot;we can\u0026#39;t predict how this will interact with the legacy system at scale\u0026quot;‚Äîfeel incomplete. They sound like excuses.\u003c/p\u003e\n\u003cp\u003eSo we learn to present certainty. We learn to have answers. And slowly, the happy path becomes the only path we\u0026#39;re willing to defend.\u003c/p\u003e\n\u003ch2\u003eHow Organizations Institutionalize Blind Spots\u003c/h2\u003e\n\u003cp\u003eIndividual engineers know the happy path is optimistic. But organizations have a way of turning individual caution into collective blindness.\u003c/p\u003e\n\u003cp\u003eConsider what happens after that design review. The design is approved. It goes into a requirements document. The requirements become tasks. The tasks become sprints. And at each translation, something is lost.\u003c/p\u003e\n\u003cp\u003eThe subtle caveat‚Äî\u0026quot;this assumes network partitions are transient\u0026quot;‚Äîbecomes \u0026quot;handles network failures.\u0026quot; The hedge‚Äî\u0026quot;buffering capacity should be sufficient for typical outages\u0026quot;‚Äîbecomes \u0026quot;72-hour buffer.\u0026quot; The uncertainty‚Äî\u0026quot;we\u0026#39;ll need to monitor how this behaves under load\u0026quot;‚Äîbecomes a checkbox: \u0026quot;performance tested.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThis isn\u0026#39;t malice. It\u0026#39;s how organizations create actionable plans from ambiguous reality. But in the process, assumptions get promoted to facts, and facts get encoded into architecture.\u003c/p\u003e\n\u003cp\u003eWorse, once something is designed and built, it becomes expensive to question. Not just financially expensive‚Äîpolitically expensive. The team that built it has invested in it. Managers have reported progress on it. Customers have been promised it. To say \u0026quot;we need to rethink this\u0026quot; is to say all that investment might have been misdirected.\u003c/p\u003e\n\u003cp\u003eSo instead, we add compensating controls. We build monitoring. We write runbooks. We train operators. Each addition reinforces the original design rather than questioning it. We\u0026#39;re not asking \u0026quot;should this system exist in this form?\u0026quot; We\u0026#39;re asking \u0026quot;how do we make this system work?\u0026quot;\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ve sat in meetings where everyone in the room privately knew a system was brittle, but no one said it directly because the system was already in production and replacing it would be a six-month project. Instead, we talked about \u0026quot;hardening\u0026quot; and \u0026quot;resilience improvements\u0026quot;‚Äîlanguage that suggested we were making something robust rather than patching something fundamentally fragile.\u003c/p\u003e\n\u003cp\u003eThe organization\u0026#39;s immune system had learned to reject the observation that the system was designed wrong, because accepting that observation would require acknowledging that a lot of other decisions were also wrong.\u003c/p\u003e\n\u003ch2\u003eMost Failures Are Designed\u003c/h2\u003e\n\u003cp\u003eHere\u0026#39;s an uncomfortable truth: most production failures aren\u0026#39;t accidents. They\u0026#39;re not the result of bugs that slipped through testing or edge cases that no one thought of.\u003c/p\u003e\n\u003cp\u003eThey\u0026#39;re the inevitable outcome of decisions made under constraints.\u003c/p\u003e\n\u003cp\u003eThat temperature monitoring system that lost 96 hours of data? The decision to use flash memory with limited write endurance was made to hit a cost target. The decision to buffer for 72 hours was made based on historical uptime data from a different cellular provider. The decision not to implement hierarchical buffering (edge ‚Üí gateway ‚Üí cloud) was made to keep the architecture simple and shippable.\u003c/p\u003e\n\u003cp\u003eNone of those were wrong decisions in isolation. Given the constraints‚Äîbudget, schedule, what was known at the time‚Äîthey were defensible. Reasonable, even.\u003c/p\u003e\n\u003cp\u003eBut they were decisions, not accidents. And decisions have consequences that aren\u0026#39;t always visible until they compound.\u003c/p\u003e\n\u003cp\u003eThis is what I mean when I say most failures are designed. Not that anyone set out to build a fragile system, but that fragility is often the natural result of optimizing for other things: speed, cost, simplicity, familiarity.\u003c/p\u003e\n\u003cp\u003eThe difference between a bug and a decision is that bugs can be fixed. Decisions are encoded into the architecture. They become load-bearing assumptions. You can\u0026#39;t fix them without rethinking the system.\u003c/p\u003e\n\u003cp\u003eWhen an incident review concludes \u0026quot;the system worked as designed, but we didn\u0026#39;t anticipate this scenario,\u0026quot; what they\u0026#39;re really saying is: \u0026quot;we made tradeoffs, and this is what we traded away.\u0026quot;\u003c/p\u003e\n\u003cp\u003eThe question is whether we\u0026#39;re honest about what we\u0026#39;re trading. Most of the time, we\u0026#39;re not‚Äîbecause being honest would make it harder to get the design approved.\u003c/p\u003e\n\u003ch2\u003eWhy Test, Staging, and Simulation Always Mislead\u003c/h2\u003e\n\u003cp\u003eEvery environment before production is a curated experience.\u003c/p\u003e\n\u003cp\u003eTest environments use clean data. Staging uses a subset of production scale. Simulations use models that abstract away complexity. Even load testing is fundamentally artificial‚Äîyou generate the load, you choose when to apply it, you know what you\u0026#39;re testing for.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026#39;t a criticism of testing. Testing is essential. But it\u0026#39;s also fundamentally limited, and we consistently underestimate how limited it is.\u003c/p\u003e\n\u003cp\u003eHere\u0026#39;s what test environments can\u0026#39;t show you:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThey can\u0026#39;t show you emergent behavior.\u003c/strong\u003e That interaction between the VFD noise and the CAN bus that only manifests when both systems are under load and the ambient temperature is above 30¬∞C? Your test bench runs at 22¬∞C with one system at a time.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThey can\u0026#39;t show you operational reality.\u003c/strong\u003e The operator who learned that if you cycle the power on the HMI in a specific sequence, you can temporarily bypass an error condition that otherwise requires a maintenance window? They discovered that in production, under pressure, when the line manager was demanding a workaround.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThey can\u0026#39;t show you the full dependency graph.\u003c/strong\u003e You tested integration with the ERP system. You didn\u0026#39;t test integration with the ERP system while the network team is doing maintenance and traffic is rerouting through a secondary path with higher latency and the backup MES is handling requests because the primary is being patched.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThey can\u0026#39;t show you time.\u003c/strong\u003e That sensor calibration drift that takes six months to become problematic? Your acceptance test runs for six hours.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThey can\u0026#39;t show you organizational dynamics.\u003c/strong\u003e Who gets called when something ambiguous happens at 2 AM? Who has the authority to make the call to shut down the line? Who actually knows where the documentation is? None of that exists in staging.\u003c/p\u003e\n\u003cp\u003eProduction is the only environment where all the variables are real, all at once, without anyone curating the experience.\u003c/p\u003e\n\u003ch2\u003eWhat Production Pressure Reveals That Design Never Does\u003c/h2\u003e\n\u003cp\u003eThere\u0026#39;s a specific kind of knowledge that only emerges under production pressure, and it has nothing to do with technical design.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s the knowledge of what actually matters.\u003c/p\u003e\n\u003cp\u003eIn design, everything matters equally. Every requirement is important. Every failure mode is worth handling. Every dependency is documented. But in production, under time pressure, with costs accumulating, you discover very quickly what\u0026#39;s truly critical and what was just conceptually important.\u003c/p\u003e\n\u003cp\u003eYou discover that the \u0026quot;critical\u0026quot; alert that fires three times a day gets ignored, while the informal message in Slack from the night shift operator gets immediate attention because everyone knows that operator only speaks up when something is genuinely wrong.\u003c/p\u003e\n\u003cp\u003eYou discover that the official escalation path‚Äîsubmit a ticket, wait for triage, get assigned to the on-call engineer‚Äîis too slow, and there\u0026#39;s an unofficial path where certain people have certain phone numbers and that\u0026#39;s what actually gets used when things are breaking.\u003c/p\u003e\n\u003cp\u003eYou discover that the monitoring dashboard everyone insisted on building never gets looked at during incidents, but everyone opens the same three SSH sessions to check the same three log files because that\u0026#39;s where the useful information actually lives.\u003c/p\u003e\n\u003cp\u003eYou discover that the runbook that took weeks to write is useless because it assumes you have time to read it, and in production you don\u0026#39;t‚Äîso people fall back on intuition, pattern matching, and educated guesses.\u003c/p\u003e\n\u003cp\u003eNone of this is visible during design because design happens in an environment where time is less expensive and mistakes are reversible. Production pressure doesn\u0026#39;t just reveal technical problems. It reveals organizational truth.\u003c/p\u003e\n\u003cp\u003eIt shows you which abstractions hold up and which ones collapse. It shows you where authority actually lies versus where the org chart says it lies. It shows you which skills matter‚Äîand they\u0026#39;re often not the ones that got people promoted.\u003c/p\u003e\n\u003ch2\u003eWhat This Means for How We Design\u003c/h2\u003e\n\u003cp\u003eI\u0026#39;m not arguing that we should abandon design reviews or testing or staging environments. They serve a purpose. The problem is that we treat them as validation rather than exploration.\u003c/p\u003e\n\u003cp\u003eWe treat passing a design review as evidence that the design is good. It\u0026#39;s not. It\u0026#39;s evidence that the design is defensible given what we know now, in this room, without production pressure.\u003c/p\u003e\n\u003cp\u003eWe treat staging as a high-fidelity simulation of production. It\u0026#39;s not. It\u0026#39;s a curated environment that shares some properties with production and systematically excludes others.\u003c/p\u003e\n\u003cp\u003eIf we were honest about this, we\u0026#39;d design differently. We\u0026#39;d spend less energy trying to anticipate every failure mode and more energy building systems that can be understood and modified when inevitably surprising failures occur.\u003c/p\u003e\n\u003cp\u003eWe\u0026#39;d document not just what the system does, but what we assumed about the environment it operates in. Not as a CYA exercise, but as a genuine artifact that helps the next person understand what we were thinking‚Äîand where we were probably wrong.\u003c/p\u003e\n\u003cp\u003eWe\u0026#39;d treat the first six months in production not as \u0026quot;stabilization\u0026quot; but as \u0026quot;learning what we actually built\u0026quot;‚Äîbecause that\u0026#39;s what it is.\u003c/p\u003e\n\u003cp\u003eThe happy path isn\u0026#39;t useless. It\u0026#39;s necessary to ship anything at all. But it\u0026#39;s a lie we tell ourselves to make progress in the face of uncertainty.\u003c/p\u003e\n\u003cp\u003eThe question is whether we remember it\u0026#39;s a lie‚Äîor whether we start believing it.\u003c/p\u003e\n\u003cp\u003eBecause the system will tell the truth eventually.\u003c/p\u003e\n\u003cp\u003eIt always does.\u003c/p\u003e\n","rawContent":"\r\nThree engineers sit in a design review for a new temperature monitoring system. The architecture is clean: sensors report to edge controllers, controllers aggregate to a gateway, gateway publishes to the cloud. Redundancy at every layer. Graceful degradation clearly marked on the diagram.\r\n\r\nSomeone asks: \"What happens if the gateway loses connectivity?\"\r\n\r\n\"It buffers locally for up to 72 hours,\" comes the answer. \"More than enough for any reasonable outage.\"\r\n\r\nEveryone nods. The design is approved.\r\n\r\nTwo years later, a cellular provider pushes a configuration update that breaks connectivity for five days. The gateway's flash memory fills in eighteen hours. When connectivity returns, 96 hours of critical temperature data is gone. The system worked exactly as designed‚Äîit just wasn't designed for this.\r\n\r\nThe happy path is seductive because it lets us move forward. But it's also a lie we tell ourselves, and design reviews are where that lie gets institutionalized.\r\n\r\n## Why Design Reviews Reward Optimism\r\n\r\nDesign reviews are supposed to find problems. In practice, they often do the opposite: they create consensus around the assumption that problems won't happen.\r\n\r\nHere's how it works:\r\n\r\nYou present a design. You show the nominal flow. You mark the error paths. You explain the redundancies. And then the questions come‚Äîbut they come in a specific form: \"What if X fails?\" You answer with your contingency for X. \"What if Y happens?\" You show your handling of Y.\r\n\r\nEach question asked and answered creates the illusion that you've covered the space of possible failures. What's harder to see is the space of failures that weren't asked about‚Äînot because they're impossible, but because they're hard to imagine from a conference room.\r\n\r\nThe cellular configuration update that breaks connectivity for five days isn't a question anyone asks, because connectivity outages are supposed to last hours, not days. The sensor that doesn't fail cleanly but drifts slowly enough to stay within acceptance bounds for months isn't on the checklist, because sensors are supposed to either work or fail obviously. The operator who learns to bypass an interlock because it trips too often during valid operations isn't in the room, because we're reviewing technical design, not operational reality.\r\n\r\nDesign reviews optimize for demonstrable coverage of known failure modes. They reward the ability to show that you've thought about things. But \"thinking about things\" is different from designing for them, and both are different from experiencing them.\r\n\r\nThis creates a subtle bias: the designs that pass review most easily are the ones that look most robust on paper. Clean boundaries. Clear error handling. Documented assumptions. The designs that acknowledge fundamental uncertainties‚Äî\"we don't know how operators will actually use this,\" \"we can't predict how this will interact with the legacy system at scale\"‚Äîfeel incomplete. They sound like excuses.\r\n\r\nSo we learn to present certainty. We learn to have answers. And slowly, the happy path becomes the only path we're willing to defend.\r\n\r\n## How Organizations Institutionalize Blind Spots\r\n\r\nIndividual engineers know the happy path is optimistic. But organizations have a way of turning individual caution into collective blindness.\r\n\r\nConsider what happens after that design review. The design is approved. It goes into a requirements document. The requirements become tasks. The tasks become sprints. And at each translation, something is lost.\r\n\r\nThe subtle caveat‚Äî\"this assumes network partitions are transient\"‚Äîbecomes \"handles network failures.\" The hedge‚Äî\"buffering capacity should be sufficient for typical outages\"‚Äîbecomes \"72-hour buffer.\" The uncertainty‚Äî\"we'll need to monitor how this behaves under load\"‚Äîbecomes a checkbox: \"performance tested.\"\r\n\r\nThis isn't malice. It's how organizations create actionable plans from ambiguous reality. But in the process, assumptions get promoted to facts, and facts get encoded into architecture.\r\n\r\nWorse, once something is designed and built, it becomes expensive to question. Not just financially expensive‚Äîpolitically expensive. The team that built it has invested in it. Managers have reported progress on it. Customers have been promised it. To say \"we need to rethink this\" is to say all that investment might have been misdirected.\r\n\r\nSo instead, we add compensating controls. We build monitoring. We write runbooks. We train operators. Each addition reinforces the original design rather than questioning it. We're not asking \"should this system exist in this form?\" We're asking \"how do we make this system work?\"\r\n\r\nI've sat in meetings where everyone in the room privately knew a system was brittle, but no one said it directly because the system was already in production and replacing it would be a six-month project. Instead, we talked about \"hardening\" and \"resilience improvements\"‚Äîlanguage that suggested we were making something robust rather than patching something fundamentally fragile.\r\n\r\nThe organization's immune system had learned to reject the observation that the system was designed wrong, because accepting that observation would require acknowledging that a lot of other decisions were also wrong.\r\n\r\n## Most Failures Are Designed\r\n\r\nHere's an uncomfortable truth: most production failures aren't accidents. They're not the result of bugs that slipped through testing or edge cases that no one thought of.\r\n\r\nThey're the inevitable outcome of decisions made under constraints.\r\n\r\nThat temperature monitoring system that lost 96 hours of data? The decision to use flash memory with limited write endurance was made to hit a cost target. The decision to buffer for 72 hours was made based on historical uptime data from a different cellular provider. The decision not to implement hierarchical buffering (edge ‚Üí gateway ‚Üí cloud) was made to keep the architecture simple and shippable.\r\n\r\nNone of those were wrong decisions in isolation. Given the constraints‚Äîbudget, schedule, what was known at the time‚Äîthey were defensible. Reasonable, even.\r\n\r\nBut they were decisions, not accidents. And decisions have consequences that aren't always visible until they compound.\r\n\r\nThis is what I mean when I say most failures are designed. Not that anyone set out to build a fragile system, but that fragility is often the natural result of optimizing for other things: speed, cost, simplicity, familiarity.\r\n\r\nThe difference between a bug and a decision is that bugs can be fixed. Decisions are encoded into the architecture. They become load-bearing assumptions. You can't fix them without rethinking the system.\r\n\r\nWhen an incident review concludes \"the system worked as designed, but we didn't anticipate this scenario,\" what they're really saying is: \"we made tradeoffs, and this is what we traded away.\"\r\n\r\nThe question is whether we're honest about what we're trading. Most of the time, we're not‚Äîbecause being honest would make it harder to get the design approved.\r\n\r\n## Why Test, Staging, and Simulation Always Mislead\r\n\r\nEvery environment before production is a curated experience.\r\n\r\nTest environments use clean data. Staging uses a subset of production scale. Simulations use models that abstract away complexity. Even load testing is fundamentally artificial‚Äîyou generate the load, you choose when to apply it, you know what you're testing for.\r\n\r\nThis isn't a criticism of testing. Testing is essential. But it's also fundamentally limited, and we consistently underestimate how limited it is.\r\n\r\nHere's what test environments can't show you:\r\n\r\n**They can't show you emergent behavior.** That interaction between the VFD noise and the CAN bus that only manifests when both systems are under load and the ambient temperature is above 30¬∞C? Your test bench runs at 22¬∞C with one system at a time.\r\n\r\n**They can't show you operational reality.** The operator who learned that if you cycle the power on the HMI in a specific sequence, you can temporarily bypass an error condition that otherwise requires a maintenance window? They discovered that in production, under pressure, when the line manager was demanding a workaround.\r\n\r\n**They can't show you the full dependency graph.** You tested integration with the ERP system. You didn't test integration with the ERP system while the network team is doing maintenance and traffic is rerouting through a secondary path with higher latency and the backup MES is handling requests because the primary is being patched.\r\n\r\n**They can't show you time.** That sensor calibration drift that takes six months to become problematic? Your acceptance test runs for six hours.\r\n\r\n**They can't show you organizational dynamics.** Who gets called when something ambiguous happens at 2 AM? Who has the authority to make the call to shut down the line? Who actually knows where the documentation is? None of that exists in staging.\r\n\r\nProduction is the only environment where all the variables are real, all at once, without anyone curating the experience.\r\n\r\n## What Production Pressure Reveals That Design Never Does\r\n\r\nThere's a specific kind of knowledge that only emerges under production pressure, and it has nothing to do with technical design.\r\n\r\nIt's the knowledge of what actually matters.\r\n\r\nIn design, everything matters equally. Every requirement is important. Every failure mode is worth handling. Every dependency is documented. But in production, under time pressure, with costs accumulating, you discover very quickly what's truly critical and what was just conceptually important.\r\n\r\nYou discover that the \"critical\" alert that fires three times a day gets ignored, while the informal message in Slack from the night shift operator gets immediate attention because everyone knows that operator only speaks up when something is genuinely wrong.\r\n\r\nYou discover that the official escalation path‚Äîsubmit a ticket, wait for triage, get assigned to the on-call engineer‚Äîis too slow, and there's an unofficial path where certain people have certain phone numbers and that's what actually gets used when things are breaking.\r\n\r\nYou discover that the monitoring dashboard everyone insisted on building never gets looked at during incidents, but everyone opens the same three SSH sessions to check the same three log files because that's where the useful information actually lives.\r\n\r\nYou discover that the runbook that took weeks to write is useless because it assumes you have time to read it, and in production you don't‚Äîso people fall back on intuition, pattern matching, and educated guesses.\r\n\r\nNone of this is visible during design because design happens in an environment where time is less expensive and mistakes are reversible. Production pressure doesn't just reveal technical problems. It reveals organizational truth.\r\n\r\nIt shows you which abstractions hold up and which ones collapse. It shows you where authority actually lies versus where the org chart says it lies. It shows you which skills matter‚Äîand they're often not the ones that got people promoted.\r\n\r\n## What This Means for How We Design\r\n\r\nI'm not arguing that we should abandon design reviews or testing or staging environments. They serve a purpose. The problem is that we treat them as validation rather than exploration.\r\n\r\nWe treat passing a design review as evidence that the design is good. It's not. It's evidence that the design is defensible given what we know now, in this room, without production pressure.\r\n\r\nWe treat staging as a high-fidelity simulation of production. It's not. It's a curated environment that shares some properties with production and systematically excludes others.\r\n\r\nIf we were honest about this, we'd design differently. We'd spend less energy trying to anticipate every failure mode and more energy building systems that can be understood and modified when inevitably surprising failures occur.\r\n\r\nWe'd document not just what the system does, but what we assumed about the environment it operates in. Not as a CYA exercise, but as a genuine artifact that helps the next person understand what we were thinking‚Äîand where we were probably wrong.\r\n\r\nWe'd treat the first six months in production not as \"stabilization\" but as \"learning what we actually built\"‚Äîbecause that's what it is.\r\n\r\nThe happy path isn't useless. It's necessary to ship anything at all. But it's a lie we tell ourselves to make progress in the face of uncertainty.\r\n\r\nThe question is whether we remember it's a lie‚Äîor whether we start believing it.\r\n\r\nBecause the system will tell the truth eventually.\r\n\r\nIt always does."},{"slug":"the-anchor","title":"Every System Tells Its Truth in Production","date":"2025-11-01","excerpt":"Production strips away comfortable illusions. It's where assumptions stop being hypothetical and start charging interest‚Äîrevealing not just what we built, but what we actually designed.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/post-anchor-splash-0.png","tags":["systems","production","engineering","reliability"],"likes":0,"comments":0,"content":"\u003cp\u003eA control system runs a hydraulic system for eighteen months without incidence. Then one morning, the hydraulic fluid leaks out from a hose and the system looses pressure which causes the system response to be sluggish. No trips, No alarms fire. The control panel continues executing its cycle. But as the fluid reserviors drain out over the next few days, the system starts to misbehave, and operations begin to fail. The system is eventually down and maintenance is notified but the damage is done and operation is down resulting in costs.\u003c/p\u003e\n\u003cp\u003eThe control system worked exactly as designed. It just wasn\u0026#39;t designed for this.\u003c/p\u003e\n\u003ch1\u003eEvery System Tells Its Truth in Production\u003c/h1\u003e\n\u003cp\u003eMost systems look reasonable on the happy path. The designed path. These systems behave themselves in demos and pass the developer\u0026#39;s tests and even survive design reviews. Afterall, the developers have the best intentions in mind. And then they reach production and their intended customers. That is when the conversation take a turn.\u003c/p\u003e\n\u003cp\u003eProduction isn\u0026#39;t just an environment‚Äîit\u0026#39;s a crucible. Real machinery, real physics, real latency, real operators, real consequences. It\u0026#39;s where assumptions stop being hypothetical and start charging interest. Where the difference between 100ms and 200ms isn\u0026#39;t academic‚Äîit\u0026#39;s the difference between a controlled stop and an emergency shutdown.\u003c/p\u003e\n\u003cp\u003eEvery system tells its truth there.\u003c/p\u003e\n\u003ch1\u003eThe Comfortable Lie of the Happy Path\u003c/h1\u003e\n\u003cp\u003eThe happy path is a useful fiction - an essential posion if you may. We need it to get anything built at all and for the fundamental truth that it lets us describe the intent. A composite of requirements translated into user stories or fairy-tale descriptions of how the user will interact with the system. It lets us romance the reason for the desired behaviour and reassures us that when a checklist is all done, we can finally ship and raise a glass to a job well done.\u003c/p\u003e\n\u003cp\u003eIt starts with a concise set of words on a document and then evolves and over time, teams begin to mistake the happy path for reality. Diagrams become promises. Tests become guarantees. Architecture documents become a form of reassurance.\u003c/p\u003e\n\u003cp\u003eWhat gets less attention is everything living just outside that path: sensors that drift over months rather than fail outright or even sensors installed so that they jittern instead of producing a constant output, networks that don\u0026#39;t drop packets but add 50ms of jitter under load, operators who develop workarounds that bypass safety interlocks, physical processes that couple in ways the control model doesn\u0026#39;t account for.\u003c/p\u003e\n\u003cp\u003eThese are not edge cases. \u003cstrong\u003eThey are the system.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eWhat ‚ÄúTruth‚Äù Actually Means\u003c/h1\u003e\n\u003cp\u003eWhen I say a system tells its truth in production, I don\u0026#39;t mean it suddenly becomes malicious or broken. I mean it becomes honest about what it actually is‚Äînot what we documented it to be.\u003c/p\u003e\n\u003cp\u003eThink of it like this: in the lab, you\u0026#39;re having a conversation with your system. You command a valve to open and it complies. You trigger an interlock and it responds. You\u0026#39;re speaking to it in a controlled environment, with calibrated instruments, asking it controlled questions.\u003c/p\u003e\n\u003cp\u003eProduction is where the system starts talking back.\u003c/p\u003e\n\u003cp\u003eIt reveals what it actually depends on‚Äînot just the 24VDC supply you specified, but the quality of that supply under varying loads, the grounding scheme you inherited, the EMI from the VFD three panels over. It shows which failures it tolerates (a single missed Communication frame) and which it amplifies (a watchdog timeout that cascades into a full line stop). It exposes where responsibility is clear (who owns the PLC code) and where it diffuses (who decides when to replace aging sensors).\u003c/p\u003e\n\u003cp\u003eAnd here\u0026#39;s what makes this uncomfortable: production reveals not just technical design, but operational reality. Maintenance schedules that drift. Calibration records that age. Tribal knowledge about \u0026quot;the weird thing it does on cold starts.\u0026quot; Decision-making under the pressure of downtime costs.\u003c/p\u003e\n\u003cp\u003eNone of this is visible in a clean system architecture diagram.\u003c/p\u003e\n\u003ch1\u003e‚ÄúThe System Worked Exactly as Designed‚Äù\u003c/h1\u003e\n\u003cp\u003eThis sentence appears often in incident reports, usually as a way to end a conversation. But it\u0026#39;s almost always true‚Äîand deeply misunderstood.\u003c/p\u003e\n\u003cp\u003eMost failures are not the result of a single fault. They are the natural outcome of tradeoffs made under budget constraints, complexity accumulated across multiple integration phases, and risk accepted implicitly because making it explicit would have delayed commissioning.\u003c/p\u003e\n\u003cp\u003eThe system worked exactly as designed. The problem is that the design included far more than firmware, software and wiring diagrams. It included unspoken assumptions about how operators would interact with HMIs, implicit bets about which sensors wouldn\u0026#39;t drift simultaneously or would be installed correctly, and organizational patterns that made certain kinds of information invisible during shift handoffs.\u003c/p\u003e\n\u003cp\u003eThe control system with the leaky hydraulic fluid? It was designed to operate the hydraulics and ignore response time in favour of supporting different models of pumps. That was a conscious choice, documented and reviewed. What wasn\u0026#39;t designed was the specific failure mode where a sluggish response of the mechanics, reported plausible-but-wrong values that fell within acceptance bounds, or the maintenance team\u0026#39;s inability to correlate subtle performance degradation, or the absence of any pressure monitoring in the original specification.\u003cbr\u003eThe system told the truth about all of it.\u003c/p\u003e\n\u003ch1\u003eWhy Production Is Uncomfortable\u003c/h1\u003e\n\u003cp\u003eProduction removes the illusion of control.\u003c/p\u003e\n\u003cp\u003eYou no longer get to choose when failures occur, how visible they are, what else is happening simultaneously, or how much time you have to respond. It forces decisions to be made with incomplete telemetry, by fatigued operators, with production managers asking for ETAs.\u003c/p\u003e\n\u003cp\u003eThat is not a weakness of production. That is its value.\u003c/p\u003e\n\u003cp\u003eThis is where experience is formed‚Äînot by avoiding failures, but by learning what survives them. It\u0026#39;s where you discover which of your redundancies were weight-bearing and which were decorative. It\u0026#39;s where you learn the difference between a system that degrades gracefully and one that simply hasn\u0026#39;t encountered the right combination of conditions yet.\u003c/p\u003e\n\u003cp\u003eIn embedded and IIoT systems, this matters more. You can\u0026#39;t just roll back a deployment. You can\u0026#39;t hotfix firmware on a system that\u0026#39;s safety-critical without a maintenance window. Your \u0026quot;users\u0026quot; are 50-ton machines that cost $10,000 per hour of downtime. The feedback loop is measured in months, not minutes.\u003c/p\u003e\n\u003ch1\u003eThis Is Not a Blog About Tools\u003c/h1\u003e\n\u003cp\u003eTools matter. Microcontrollers matter. Protocols matter. Fieldbus architectures matter. Cloud architectures matter.\u003c/p\u003e\n\u003cp\u003eBut they are rarely the reason systems fail in production.\u003c/p\u003e\n\u003cp\u003eSystems fail because graceful degradation was an afterthought, ownership boundaries were unclear between firmware and mechanical teams, incentives were misaligned between commissioning speed and long-term reliability, risks were known but unspoken during design reviews, and complexity grew faster than documentation‚Äîor understanding.\u003c/p\u003e\n\u003cp\u003eThose are design problems. Judgment problems. Responsibility problems.\u003c/p\u003e\n\u003cp\u003eThat is what this space is about.\u003c/p\u003e\n\u003ch1\u003eSo Why This Exists?\u003c/h1\u003e\n\u003cp\u003eOff the Happy Path is a collection of observations, essays, and stories from production systems‚Äîembedded firmware, IIoT platforms, control systems, and the organizations that build and operate them.\u003c/p\u003e\n\u003cp\u003eIt is not a tutorial blog. It will not explain fundamentals that datasheets and standards already cover well. It will not optimize for engagement or outrage.\u003c/p\u003e\n\u003cp\u003eIt exists to name things that are usually felt but not discussed. To talk about systems as they behave, not as we wish they would. To capture lessons that tend to surface only after something breaks‚Äîand sometimes only after we\u0026#39;ve stopped to actually listen.\u003c/p\u003e\n\u003ch1\u003eA Final Thought\u003c/h1\u003e\n\u003cp\u003eIf a system has never surprised you in production, one of two things is true: either it has not been in production long enough, or you were not listening closely when it spoke.\u003c/p\u003e\n\u003cp\u003eBecause eventually, every system does. It tells its truth.\u003c/p\u003e\n\u003cp\u003eThe question is whether we\u0026#39;re ready to hear it.\u003c/p\u003e\n","rawContent":"\r\nA control system runs a hydraulic system for eighteen months without incidence. Then one morning, the hydraulic fluid leaks out from a hose and the system looses pressure which causes the system response to be sluggish. No trips, No alarms fire. The control panel continues executing its cycle. But as the fluid reserviors drain out over the next few days, the system starts to misbehave, and operations begin to fail. The system is eventually down and maintenance is notified but the damage is done and operation is down resulting in costs.\r\n\r\nThe control system worked exactly as designed. It just wasn't designed for this.\r\n\r\n# Every System Tells Its Truth in Production\r\n\r\nMost systems look reasonable on the happy path. The designed path. These systems behave themselves in demos and pass the developer's tests and even survive design reviews. Afterall, the developers have the best intentions in mind. And then they reach production and their intended customers. That is when the conversation take a turn.\r\n\r\nProduction isn't just an environment‚Äîit's a crucible. Real machinery, real physics, real latency, real operators, real consequences. It's where assumptions stop being hypothetical and start charging interest. Where the difference between 100ms and 200ms isn't academic‚Äîit's the difference between a controlled stop and an emergency shutdown.\r\n\r\nEvery system tells its truth there.\r\n\r\n# The Comfortable Lie of the Happy Path\r\n\r\nThe happy path is a useful fiction - an essential posion if you may. We need it to get anything built at all and for the fundamental truth that it lets us describe the intent. A composite of requirements translated into user stories or fairy-tale descriptions of how the user will interact with the system. It lets us romance the reason for the desired behaviour and reassures us that when a checklist is all done, we can finally ship and raise a glass to a job well done.\r\n\r\nIt starts with a concise set of words on a document and then evolves and over time, teams begin to mistake the happy path for reality. Diagrams become promises. Tests become guarantees. Architecture documents become a form of reassurance.\r\n\r\nWhat gets less attention is everything living just outside that path: sensors that drift over months rather than fail outright or even sensors installed so that they jittern instead of producing a constant output, networks that don't drop packets but add 50ms of jitter under load, operators who develop workarounds that bypass safety interlocks, physical processes that couple in ways the control model doesn't account for.\r\n\r\nThese are not edge cases. **They are the system.**\r\n\r\n# What ‚ÄúTruth‚Äù Actually Means\r\n\r\nWhen I say a system tells its truth in production, I don't mean it suddenly becomes malicious or broken. I mean it becomes honest about what it actually is‚Äînot what we documented it to be.\r\n\r\nThink of it like this: in the lab, you're having a conversation with your system. You command a valve to open and it complies. You trigger an interlock and it responds. You're speaking to it in a controlled environment, with calibrated instruments, asking it controlled questions.\r\n\r\nProduction is where the system starts talking back.\r\n\r\nIt reveals what it actually depends on‚Äînot just the 24VDC supply you specified, but the quality of that supply under varying loads, the grounding scheme you inherited, the EMI from the VFD three panels over. It shows which failures it tolerates (a single missed Communication frame) and which it amplifies (a watchdog timeout that cascades into a full line stop). It exposes where responsibility is clear (who owns the PLC code) and where it diffuses (who decides when to replace aging sensors).\r\n\r\nAnd here's what makes this uncomfortable: production reveals not just technical design, but operational reality. Maintenance schedules that drift. Calibration records that age. Tribal knowledge about \"the weird thing it does on cold starts.\" Decision-making under the pressure of downtime costs.\r\n\r\nNone of this is visible in a clean system architecture diagram.\r\n\r\n# ‚ÄúThe System Worked Exactly as Designed‚Äù\r\n\r\nThis sentence appears often in incident reports, usually as a way to end a conversation. But it's almost always true‚Äîand deeply misunderstood.\r\n\r\nMost failures are not the result of a single fault. They are the natural outcome of tradeoffs made under budget constraints, complexity accumulated across multiple integration phases, and risk accepted implicitly because making it explicit would have delayed commissioning.\r\n\r\nThe system worked exactly as designed. The problem is that the design included far more than firmware, software and wiring diagrams. It included unspoken assumptions about how operators would interact with HMIs, implicit bets about which sensors wouldn't drift simultaneously or would be installed correctly, and organizational patterns that made certain kinds of information invisible during shift handoffs.\r\n\r\nThe control system with the leaky hydraulic fluid? It was designed to operate the hydraulics and ignore response time in favour of supporting different models of pumps. That was a conscious choice, documented and reviewed. What wasn't designed was the specific failure mode where a sluggish response of the mechanics, reported plausible-but-wrong values that fell within acceptance bounds, or the maintenance team's inability to correlate subtle performance degradation, or the absence of any pressure monitoring in the original specification.\r\nThe system told the truth about all of it.\r\n\r\n# Why Production Is Uncomfortable\r\n\r\nProduction removes the illusion of control.\r\n\r\nYou no longer get to choose when failures occur, how visible they are, what else is happening simultaneously, or how much time you have to respond. It forces decisions to be made with incomplete telemetry, by fatigued operators, with production managers asking for ETAs.\r\n\r\nThat is not a weakness of production. That is its value.\r\n\r\nThis is where experience is formed‚Äînot by avoiding failures, but by learning what survives them. It's where you discover which of your redundancies were weight-bearing and which were decorative. It's where you learn the difference between a system that degrades gracefully and one that simply hasn't encountered the right combination of conditions yet.\r\n\r\nIn embedded and IIoT systems, this matters more. You can't just roll back a deployment. You can't hotfix firmware on a system that's safety-critical without a maintenance window. Your \"users\" are 50-ton machines that cost $10,000 per hour of downtime. The feedback loop is measured in months, not minutes.\r\n\r\n# This Is Not a Blog About Tools\r\n\r\nTools matter. Microcontrollers matter. Protocols matter. Fieldbus architectures matter. Cloud architectures matter.\r\n\r\nBut they are rarely the reason systems fail in production.\r\n\r\nSystems fail because graceful degradation was an afterthought, ownership boundaries were unclear between firmware and mechanical teams, incentives were misaligned between commissioning speed and long-term reliability, risks were known but unspoken during design reviews, and complexity grew faster than documentation‚Äîor understanding.\r\n\r\nThose are design problems. Judgment problems. Responsibility problems.\r\n\r\nThat is what this space is about.\r\n\r\n# So Why This Exists?\r\n\r\nOff the Happy Path is a collection of observations, essays, and stories from production systems‚Äîembedded firmware, IIoT platforms, control systems, and the organizations that build and operate them.\r\n\r\nIt is not a tutorial blog. It will not explain fundamentals that datasheets and standards already cover well. It will not optimize for engagement or outrage.\r\n\r\nIt exists to name things that are usually felt but not discussed. To talk about systems as they behave, not as we wish they would. To capture lessons that tend to surface only after something breaks‚Äîand sometimes only after we've stopped to actually listen.\r\n\r\n# A Final Thought\r\n\r\nIf a system has never surprised you in production, one of two things is true: either it has not been in production long enough, or you were not listening closely when it spoke.\r\n\r\nBecause eventually, every system does. It tells its truth.\r\n\r\nThe question is whether we're ready to hear it."}]},"__N_SSG":true},"page":"/","query":{},"buildId":"phl_WOpLyX2gCvGrSm1hh","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
{"pageProps":{"posts":[{"slug":"blog_post_control_systems","title":"Every Automated System Is a Control System","date":"2025-12-24","excerpt":"Automation doesn't remove control—it implements it. And whether that control lives in a PLC, a cloud service, or an organization chart doesn't change the physics involved.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/control-system.png","tags":["automation","control-systems","feedback","industrial"],"content":"<h1>Every Automated System Is a Control System</h1>\n<p>A distribution warehouse automated its loading dock safety system in 2021. Smart control panels coordinated hydraulic dock levelers, truck restraints, overhead doors, and inflatable seals. The sequence was critical: restrain the truck, extend the leveler, inflate the seal, then allow the door to open. Get the order wrong and you risk crushing equipment, damaging trailers, or creating a fall hazard for forklift operators.</p>\n<p>The system worked flawlessly for eighteen months. Then one Tuesday morning, Bay 7 locked out. The operator pressed the button sequence. Nothing. The panel showed conflicting states—door closed, but also a fault. Leveler down, but restraint active. The display flickered between valid states too quickly to read.</p>\n<p>The truck sat in the bay. The operator called maintenance. Maintenance called the panel vendor. No one could see what the system was seeing. No logs. No remote access. No diagnostic mode that didn&#39;t require physically opening the panel and connecting a laptop—which no one on-site knew how to do safely while the system was energized.</p>\n<p>Four hours later, a technician traced the problem: a door interlock sensor with a loose connection. Instead of a clean on/off signal, it was chattering—sending intermittent pulses that the control logic interpreted as rapid state changes. The safety system, doing exactly what it was designed to do, refused to proceed because the door state was ambiguous.</p>\n<p>The fix took ten minutes once diagnosed. The diagnosis took four hours and $800 in emergency service fees. The lost productivity cost more than that.</p>\n<p>The system worked exactly as designed. It just had no way to tell anyone what it was experiencing.</p>\n<p>Automation is often described as a way to remove humans from the loop. That framing is comforting—and wrong.</p>\n<p>Automation does not eliminate control. It implements it. Every automated system is, at its core, a control system: something observes a process, compares it to intent, and acts to reduce the difference.</p>\n<p>Whether that system lives in a PLC, a microcontroller, a cloud service, or an organization chart does not change the physics involved. Feedback, delay, noise, saturation, and stability still apply.</p>\n<p>Ignoring that reality is how automated systems quietly become dangerous.</p>\n<h2>Automation Is Control With Commitment</h2>\n<p>In industrial environments, automation is not an abstraction. It closes a loop around something physical.</p>\n<p>A valve moves. A motor spins. A door opens. A restraint engages. A process heats, cools, fills, or drains.</p>\n<p>Once automated, those actions happen faster than human reaction time and with more consistency than human judgment. That is the point.</p>\n<p>But it also means the consequences of design decisions arrive faster—and with less opportunity for intervention.</p>\n<p>A human operator running a system manually makes continuous micro-adjustments based on observation, intuition, and context that never makes it into documentation. They notice when a sensor &quot;feels sticky.&quot; They remember that the interlock on Bay 3 takes an extra half-second to settle. They know to wait for the hydraulic pump to stabilize before cycling again.</p>\n<p>Automation doesn&#39;t have intuition. It has the logic you gave it and the sensors you installed. It will faithfully execute that logic with those sensors, even when the result is obviously wrong to anyone watching.</p>\n<p>This is not a limitation of automation. It is the nature of it.</p>\n<p>Automation is control with commitment. Once the loop is closed, the system will do exactly what you encoded—no more, no less—long after you&#39;ve stopped paying attention. It won&#39;t second-guess itself. It won&#39;t notice that something feels off. It will enforce whatever rules you programmed, even when those rules produce unexpected behavior.</p>\n<p>That dock control panel didn&#39;t know the door sensor was chattering. It knew the door state was changing rapidly, which violated the safety interlock logic. So it did exactly what it was supposed to: refuse to proceed. The fact that the state changes were caused by a failing sensor rather than an actual unsafe condition was invisible to the control logic.</p>\n<p>The system couldn&#39;t distinguish between &quot;door rapidly opening and closing&quot; (dangerous, must prevent) and &quot;door sensor failing&quot; (annoying, should flag). Both looked the same from inside the control loop.</p>\n<h2>The Control Loop Is the Unit of Truth</h2>\n<p>It&#39;s tempting to talk about systems in terms of components: sensors, PLCs, networks, SCADA, cloud dashboards, HMI panels.</p>\n<p>But components don&#39;t fail in isolation. Loops do.</p>\n<p>A control loop tells you:</p>\n<ul>\n<li>What the system believes about the world</li>\n<li>How often it updates that belief  </li>\n<li>How aggressively it responds</li>\n<li>What happens when measurements are missing or wrong</li>\n<li>What it does when it reaches its limits</li>\n</ul>\n<p>If you want to understand an automated system, you don&#39;t start with the hardware list. You start by asking: where are the loops, and where aren&#39;t they?</p>\n<p>That dock safety system had all the right components. The sensors were rated for the environment. The control panel had proper safety logic. The interlocks were correctly wired. But the loop—the complete feedback path from sensor through logic to actuator and back—had no provision for ambiguous sensor states.</p>\n<p>The loop is what matters. Not because the components aren&#39;t important, but because components only make sense in the context of the loop they&#39;re part of.</p>\n<p>A binary sensor is perfect—unless environmental vibration causes intermittent connections. A safety interlock is necessary—unless it can&#39;t distinguish between unsafe conditions and sensor failures. A local control panel is reliable—unless diagnosing it requires knowledge and tools that aren&#39;t available on-site.</p>\n<p>This is why you can&#39;t design automation by picking components and wiring them together. You have to design loops: understand what the system needs to observe, determine how it should respond to both normal and abnormal signals, and verify that the complete feedback path behaves as intended—including degraded sensor conditions.</p>\n<p>Most automation problems are loop problems disguised as component problems.</p>\n<h2>Open-Loop Automation Is Optimism</h2>\n<p>Many automation failures are not caused by bugs. They are caused by loops that were never fully closed.</p>\n<p>That dock control system had excellent safety interlocks—sensors, logic, actuators, all integrated. But it had no diagnostic loop. No way for the system to observe its own behavior and communicate what it was experiencing. No telemetry. No event logging. No remote visibility.</p>\n<p>When the door sensor failed, the safety loop worked perfectly: detect ambiguous state, prevent operation, protect people and equipment. But there was no observability loop to help operators or technicians understand why.</p>\n<p>This pattern appears everywhere:</p>\n<ul>\n<li>Systems that actuate but don&#39;t verify outcome</li>\n<li>Alarms that fire but have no authority to act  </li>\n<li>Safety interlocks with no diagnostic telemetry</li>\n<li>Control panels that enforce logic but can&#39;t explain their state</li>\n<li>Analytics platforms that detect anomalies no one responds to</li>\n<li>Dashboards that inform no decisions</li>\n</ul>\n<p>These are open-loop systems wearing the appearance of control. They work beautifully in steady state. They fail spectacularly during transitions—startups, shutdowns, disturbances, sensor degradation, and edge cases—because there&#39;s no feedback to communicate what&#39;s actually happening.</p>\n<p>Open-loop automation is not engineering. It is optimism encoded in architecture diagrams.</p>\n<p>The difference between &quot;automated&quot; and &quot;automatic&quot; matters. Automated means machines do the work. Automatic means the machines adjust themselves when things change. One is a tool. The other is a control system.</p>\n<p>But there&#39;s a third category that&#39;s often forgotten: observable. A system can be automatic but opaque. It makes decisions, but you can&#39;t see why. It enforces rules, but you can&#39;t tell which rule is blocking you. It protects you from hazards, but also locks you out when a sensor hiccups—and offers no clue about which sensor or why.</p>\n<p>Observability isn&#39;t optional. It&#39;s a feedback loop for humans trying to understand what the automated system is doing.</p>\n<h2>Remote Monitoring Is a Supervisory Layer</h2>\n<p>Remote monitoring is often sold as visibility. Visibility is useful, but visibility alone does not create control.</p>\n<p>A remote system introduces latency, bandwidth constraints, loss of determinism, and unclear authority boundaries. At that point, you are no longer designing a control loop. You are designing a supervisory loop.</p>\n<p>Supervisory control is valid—but only when its role is clearly defined.</p>\n<p>Consider a hierarchical control structure: local control panels handle safety interlocks (milliseconds), site management systems coordinate operations (seconds to minutes), and cloud platforms aggregate diagnostics across facilities (minutes to hours). Each layer operates at its natural timescale. Authority is clear.</p>\n<p>The dock safety system didn&#39;t need cloud control. The safety decisions—when to allow the door to open, whether the truck is properly restrained—must happen locally, instantly, deterministically. Those decisions can&#39;t wait for network round-trips or cloud processing.</p>\n<p>But diagnostics? That&#39;s different. Diagnostics operate on a slower timescale. When a sensor starts behaving erratically, you don&#39;t need instant response. You need visibility—event logs, state histories, sensor signal quality metrics. Information that helps technicians diagnose problems without being on-site with specialized tools.</p>\n<p>The missing piece wasn&#39;t faster control. It was slower observation.</p>\n<p>Problems arise when these boundaries blur. When cloud systems try to make real-time control decisions. When local controllers wait for remote approval before acting. When &quot;monitoring&quot; systems are expected to prevent problems they can only observe after the fact.</p>\n<p>Latency turns control into observation. But observation without control is still valuable—if the system is designed to provide it.</p>\n<h2>When Remote Observability Meets Local Control</h2>\n<p>Cloud platforms feel powerful because they are flexible, scalable, and abstracted from hardware. None of that exempts them from control theory.</p>\n<p>Remote systems are best understood as high-latency, high-compute supervisory layers:</p>\n<ul>\n<li>Excellent for aggregation across multiple sites</li>\n<li>Excellent for diagnostics and trend analysis</li>\n<li>Excellent for coordination and maintenance planning</li>\n<li>Terrible for real-time safety decisions</li>\n</ul>\n<p>This isn&#39;t a criticism of remote architecture. It&#39;s a description of its physics. Light only travels so fast. Networks have congestion. Cellular connections drop. These aren&#39;t implementation details to be optimized away—they&#39;re fundamental constraints that must be designed for.</p>\n<p>The dock control panel could have had both: local, deterministic safety control (no network dependency, instant response, proven logic) plus remote diagnostic telemetry (event logs, sensor states, fault histories uploaded when connectivity is available).</p>\n<p>This requires answering specific questions during design:</p>\n<ul>\n<li>What decisions must be local and deterministic?</li>\n<li>What information would help diagnose problems remotely?</li>\n<li>What happens when connectivity is unavailable?</li>\n<li>How long can diagnostics be buffered before critical information is lost?</li>\n<li>Who receives the diagnostic data and what do they do with it?</li>\n</ul>\n<p>Most systems don&#39;t answer these questions explicitly. They treat remote connectivity as either essential (everything goes to the cloud) or unnecessary (everything stays local). The middle ground—local control with remote observability—requires more thought but solves real problems.</p>\n<p>That four-hour diagnostic delay? With basic telemetry, it could have been four minutes. Not by making the cloud control the dock—but by making the dock tell the cloud what it was experiencing. Event timestamps showing the door sensor cycling 200 times per second. State machine logs showing repeated transitions between incompatible states. Signal quality metrics showing noise on the interlock circuit.</p>\n<p>The technician could have arrived knowing exactly which sensor to check, maybe even with a replacement part already in hand.</p>\n<h2>Safety Is a Separate Loop</h2>\n<p>One of the earliest lessons in industrial control is also one of the most frequently forgotten: safety is not a feature of control—it is a separate control loop.</p>\n<p>Safety systems assume the primary control loop can fail. They assume sensors can lie. They assume logic can be wrong. They assume humans can make poor decisions.</p>\n<p>They exist precisely because automation works so well—and so relentlessly.</p>\n<p>A control system optimizes for performance: throughput, efficiency, convenience. A safety system optimizes for survival: keeping things within boundaries that prevent damage, injury, or environmental harm.</p>\n<p>These goals conflict. And when they&#39;re implemented in the same system, the conflict gets resolved through implicit tradeoffs that no one agreed to.</p>\n<p>The dock control panel did this right: safety logic was primary. When the door sensor gave ambiguous signals, the system chose the safe response—lockout—over the convenient response—ignore the noise and proceed anyway.</p>\n<p>But safety and observability are not the same thing. The system could safely refuse to operate while simultaneously logging why. It could maintain safety interlocks while transmitting diagnostic data. It could protect operators from hazards while giving technicians information to fix the hazard.</p>\n<p>Instead, it protected operators but left technicians blind.</p>\n<p>This is a common pattern: systems designed with robust safety logic but minimal diagnostic capability. The assumption is that if safety is handled, everything else is secondary. But &quot;everything else&quot; includes the ability to understand failures, plan maintenance, and avoid emergency service calls.</p>\n<p>Safety loops must be independent: separate sensors, separate logic, separate authority to shut things down. But observability loops should be pervasive: instrument everything, log state changes, buffer diagnostics locally, transmit when possible.</p>\n<p>These are not competing requirements. They&#39;re complementary.</p>\n<h2>Organizations Are Control Systems Too</h2>\n<p>This is where automation, technology, and management converge.</p>\n<p>Organizations sense through metrics and reports. They decide through meetings and approvals. They actuate through people and processes. They respond with delay and distortion.</p>\n<p>When feedback is slow, organizations oscillate—overcorrecting to problems that have already changed.</p>\n<p>When authority is unclear, control saturates—multiple people trying to correct the same thing in incompatible ways.</p>\n<p>When incentives are misaligned, the system drives itself into failure—optimizing locally while degrading globally.</p>\n<p>These are not cultural problems. They are control problems.</p>\n<p>That dock control panel? The decision to exclude remote diagnostics wasn&#39;t technical. The panel manufacturer offered telemetry as an option. The warehouse operator decided against it to save $200 per bay—$2,400 total across twelve bays.</p>\n<p>That was a reasonable decision given available information: the system had worked reliably elsewhere, diagnostic issues seemed unlikely, and budget was constrained.</p>\n<p>But no one in the decision-making loop had experienced the cost of a four-hour diagnostic delay. No one modeled the probability of sensor degradation over time. No one considered that the warehouse was remote enough that emergency service calls were expensive. The organizational feedback loop—the one that should learn from operational experience and adjust specifications—was open.</p>\n<p>The result: $2,400 saved during installation, $800 spent on the first service call, plus lost productivity, plus the knowledge that eleven other bays could fail the same way with the same diagnostic blindness.</p>\n<p>The technical system was local. The organizational learning system was also local—each facility made its own decisions based on its own experience, with no feedback loop to aggregate lessons learned across sites.</p>\n<p>This pattern repeats: automation projects where procurement optimizes for initial cost without visibility into operational cost. Remote monitoring proposals rejected because &quot;it&#39;s worked fine without it&quot; right up until it doesn&#39;t. Diagnostic capabilities treated as nice-to-have features instead of essential observability loops.</p>\n<p>The technical system can be well designed, but if the organizational system around it is open-loop, failures are inevitable.</p>\n<h2>The Pattern Repeats</h2>\n<p>This dock control failure is not unique. The specifics change—different equipment, different sensors, different facilities—but the pattern is constant.</p>\n<p>Systems are designed with good control logic but minimal observability. Safety is prioritized (correctly) over diagnostics. Remote connectivity is seen as an unnecessary cost until the first time it would have saved multiples of that cost.</p>\n<p>And the lesson gets learned locally, slowly, expensively—one emergency service call at a time.</p>\n<p>This is where control theory meets organizational reality. The technical feedback loop (sensor → logic → actuator) works fine. The diagnostic feedback loop (system behavior → human understanding → maintenance action) is open. And the organizational learning loop (operational experience → design decisions → better specifications) barely exists.</p>\n<p>Each loop matters. Each fails in predictable ways. And understanding one helps you recognize the others.</p>\n<h2>A Final Thought</h2>\n<p>Automation doesn&#39;t remove humans from systems. It changes where humans intervene—and how much information they have when they do.</p>\n<p>The dock control panel didn&#39;t need humans removed from the safety loop. It needed humans with the right information at the right time: operators with clear panel feedback about system state, technicians with diagnostic telemetry about sensor health, managers with operational data about failure patterns across sites.</p>\n<p>Well-designed systems respect that reality. They put fast control close to the process. They add observability where diagnosis matters. They keep safety separate from optimization. They ensure someone owns the complete loop—not just the components, not just the initial installation, but the long-term operational behavior.</p>\n<p>Poorly designed systems hide these distinctions until something breaks.</p>\n<p>Every automated system is a control system. Whether it behaves safely, stably, and predictably depends on whether its designers treated it like one.</p>\n<p>The question is not whether you have automation. The question is whether you have control—and whether you can see what that control is doing when things go wrong.</p>\n","rawContent":"\n# Every Automated System Is a Control System\n\nA distribution warehouse automated its loading dock safety system in 2021. Smart control panels coordinated hydraulic dock levelers, truck restraints, overhead doors, and inflatable seals. The sequence was critical: restrain the truck, extend the leveler, inflate the seal, then allow the door to open. Get the order wrong and you risk crushing equipment, damaging trailers, or creating a fall hazard for forklift operators.\n\nThe system worked flawlessly for eighteen months. Then one Tuesday morning, Bay 7 locked out. The operator pressed the button sequence. Nothing. The panel showed conflicting states—door closed, but also a fault. Leveler down, but restraint active. The display flickered between valid states too quickly to read.\n\nThe truck sat in the bay. The operator called maintenance. Maintenance called the panel vendor. No one could see what the system was seeing. No logs. No remote access. No diagnostic mode that didn't require physically opening the panel and connecting a laptop—which no one on-site knew how to do safely while the system was energized.\n\nFour hours later, a technician traced the problem: a door interlock sensor with a loose connection. Instead of a clean on/off signal, it was chattering—sending intermittent pulses that the control logic interpreted as rapid state changes. The safety system, doing exactly what it was designed to do, refused to proceed because the door state was ambiguous.\n\nThe fix took ten minutes once diagnosed. The diagnosis took four hours and $800 in emergency service fees. The lost productivity cost more than that.\n\nThe system worked exactly as designed. It just had no way to tell anyone what it was experiencing.\n\nAutomation is often described as a way to remove humans from the loop. That framing is comforting—and wrong.\n\nAutomation does not eliminate control. It implements it. Every automated system is, at its core, a control system: something observes a process, compares it to intent, and acts to reduce the difference.\n\nWhether that system lives in a PLC, a microcontroller, a cloud service, or an organization chart does not change the physics involved. Feedback, delay, noise, saturation, and stability still apply.\n\nIgnoring that reality is how automated systems quietly become dangerous.\n\n## Automation Is Control With Commitment\n\nIn industrial environments, automation is not an abstraction. It closes a loop around something physical.\n\nA valve moves. A motor spins. A door opens. A restraint engages. A process heats, cools, fills, or drains.\n\nOnce automated, those actions happen faster than human reaction time and with more consistency than human judgment. That is the point.\n\nBut it also means the consequences of design decisions arrive faster—and with less opportunity for intervention.\n\nA human operator running a system manually makes continuous micro-adjustments based on observation, intuition, and context that never makes it into documentation. They notice when a sensor \"feels sticky.\" They remember that the interlock on Bay 3 takes an extra half-second to settle. They know to wait for the hydraulic pump to stabilize before cycling again.\n\nAutomation doesn't have intuition. It has the logic you gave it and the sensors you installed. It will faithfully execute that logic with those sensors, even when the result is obviously wrong to anyone watching.\n\nThis is not a limitation of automation. It is the nature of it.\n\nAutomation is control with commitment. Once the loop is closed, the system will do exactly what you encoded—no more, no less—long after you've stopped paying attention. It won't second-guess itself. It won't notice that something feels off. It will enforce whatever rules you programmed, even when those rules produce unexpected behavior.\n\nThat dock control panel didn't know the door sensor was chattering. It knew the door state was changing rapidly, which violated the safety interlock logic. So it did exactly what it was supposed to: refuse to proceed. The fact that the state changes were caused by a failing sensor rather than an actual unsafe condition was invisible to the control logic.\n\nThe system couldn't distinguish between \"door rapidly opening and closing\" (dangerous, must prevent) and \"door sensor failing\" (annoying, should flag). Both looked the same from inside the control loop.\n\n## The Control Loop Is the Unit of Truth\n\nIt's tempting to talk about systems in terms of components: sensors, PLCs, networks, SCADA, cloud dashboards, HMI panels.\n\nBut components don't fail in isolation. Loops do.\n\nA control loop tells you:\n- What the system believes about the world\n- How often it updates that belief  \n- How aggressively it responds\n- What happens when measurements are missing or wrong\n- What it does when it reaches its limits\n\nIf you want to understand an automated system, you don't start with the hardware list. You start by asking: where are the loops, and where aren't they?\n\nThat dock safety system had all the right components. The sensors were rated for the environment. The control panel had proper safety logic. The interlocks were correctly wired. But the loop—the complete feedback path from sensor through logic to actuator and back—had no provision for ambiguous sensor states.\n\nThe loop is what matters. Not because the components aren't important, but because components only make sense in the context of the loop they're part of.\n\nA binary sensor is perfect—unless environmental vibration causes intermittent connections. A safety interlock is necessary—unless it can't distinguish between unsafe conditions and sensor failures. A local control panel is reliable—unless diagnosing it requires knowledge and tools that aren't available on-site.\n\nThis is why you can't design automation by picking components and wiring them together. You have to design loops: understand what the system needs to observe, determine how it should respond to both normal and abnormal signals, and verify that the complete feedback path behaves as intended—including degraded sensor conditions.\n\nMost automation problems are loop problems disguised as component problems.\n\n## Open-Loop Automation Is Optimism\n\nMany automation failures are not caused by bugs. They are caused by loops that were never fully closed.\n\nThat dock control system had excellent safety interlocks—sensors, logic, actuators, all integrated. But it had no diagnostic loop. No way for the system to observe its own behavior and communicate what it was experiencing. No telemetry. No event logging. No remote visibility.\n\nWhen the door sensor failed, the safety loop worked perfectly: detect ambiguous state, prevent operation, protect people and equipment. But there was no observability loop to help operators or technicians understand why.\n\nThis pattern appears everywhere:\n- Systems that actuate but don't verify outcome\n- Alarms that fire but have no authority to act  \n- Safety interlocks with no diagnostic telemetry\n- Control panels that enforce logic but can't explain their state\n- Analytics platforms that detect anomalies no one responds to\n- Dashboards that inform no decisions\n\nThese are open-loop systems wearing the appearance of control. They work beautifully in steady state. They fail spectacularly during transitions—startups, shutdowns, disturbances, sensor degradation, and edge cases—because there's no feedback to communicate what's actually happening.\n\nOpen-loop automation is not engineering. It is optimism encoded in architecture diagrams.\n\nThe difference between \"automated\" and \"automatic\" matters. Automated means machines do the work. Automatic means the machines adjust themselves when things change. One is a tool. The other is a control system.\n\nBut there's a third category that's often forgotten: observable. A system can be automatic but opaque. It makes decisions, but you can't see why. It enforces rules, but you can't tell which rule is blocking you. It protects you from hazards, but also locks you out when a sensor hiccups—and offers no clue about which sensor or why.\n\nObservability isn't optional. It's a feedback loop for humans trying to understand what the automated system is doing.\n\n## Remote Monitoring Is a Supervisory Layer\n\nRemote monitoring is often sold as visibility. Visibility is useful, but visibility alone does not create control.\n\nA remote system introduces latency, bandwidth constraints, loss of determinism, and unclear authority boundaries. At that point, you are no longer designing a control loop. You are designing a supervisory loop.\n\nSupervisory control is valid—but only when its role is clearly defined.\n\nConsider a hierarchical control structure: local control panels handle safety interlocks (milliseconds), site management systems coordinate operations (seconds to minutes), and cloud platforms aggregate diagnostics across facilities (minutes to hours). Each layer operates at its natural timescale. Authority is clear.\n\nThe dock safety system didn't need cloud control. The safety decisions—when to allow the door to open, whether the truck is properly restrained—must happen locally, instantly, deterministically. Those decisions can't wait for network round-trips or cloud processing.\n\nBut diagnostics? That's different. Diagnostics operate on a slower timescale. When a sensor starts behaving erratically, you don't need instant response. You need visibility—event logs, state histories, sensor signal quality metrics. Information that helps technicians diagnose problems without being on-site with specialized tools.\n\nThe missing piece wasn't faster control. It was slower observation.\n\nProblems arise when these boundaries blur. When cloud systems try to make real-time control decisions. When local controllers wait for remote approval before acting. When \"monitoring\" systems are expected to prevent problems they can only observe after the fact.\n\nLatency turns control into observation. But observation without control is still valuable—if the system is designed to provide it.\n\n## When Remote Observability Meets Local Control\n\nCloud platforms feel powerful because they are flexible, scalable, and abstracted from hardware. None of that exempts them from control theory.\n\nRemote systems are best understood as high-latency, high-compute supervisory layers:\n- Excellent for aggregation across multiple sites\n- Excellent for diagnostics and trend analysis\n- Excellent for coordination and maintenance planning\n- Terrible for real-time safety decisions\n\nThis isn't a criticism of remote architecture. It's a description of its physics. Light only travels so fast. Networks have congestion. Cellular connections drop. These aren't implementation details to be optimized away—they're fundamental constraints that must be designed for.\n\nThe dock control panel could have had both: local, deterministic safety control (no network dependency, instant response, proven logic) plus remote diagnostic telemetry (event logs, sensor states, fault histories uploaded when connectivity is available).\n\nThis requires answering specific questions during design:\n- What decisions must be local and deterministic?\n- What information would help diagnose problems remotely?\n- What happens when connectivity is unavailable?\n- How long can diagnostics be buffered before critical information is lost?\n- Who receives the diagnostic data and what do they do with it?\n\nMost systems don't answer these questions explicitly. They treat remote connectivity as either essential (everything goes to the cloud) or unnecessary (everything stays local). The middle ground—local control with remote observability—requires more thought but solves real problems.\n\nThat four-hour diagnostic delay? With basic telemetry, it could have been four minutes. Not by making the cloud control the dock—but by making the dock tell the cloud what it was experiencing. Event timestamps showing the door sensor cycling 200 times per second. State machine logs showing repeated transitions between incompatible states. Signal quality metrics showing noise on the interlock circuit.\n\nThe technician could have arrived knowing exactly which sensor to check, maybe even with a replacement part already in hand.\n\n## Safety Is a Separate Loop\n\nOne of the earliest lessons in industrial control is also one of the most frequently forgotten: safety is not a feature of control—it is a separate control loop.\n\nSafety systems assume the primary control loop can fail. They assume sensors can lie. They assume logic can be wrong. They assume humans can make poor decisions.\n\nThey exist precisely because automation works so well—and so relentlessly.\n\nA control system optimizes for performance: throughput, efficiency, convenience. A safety system optimizes for survival: keeping things within boundaries that prevent damage, injury, or environmental harm.\n\nThese goals conflict. And when they're implemented in the same system, the conflict gets resolved through implicit tradeoffs that no one agreed to.\n\nThe dock control panel did this right: safety logic was primary. When the door sensor gave ambiguous signals, the system chose the safe response—lockout—over the convenient response—ignore the noise and proceed anyway.\n\nBut safety and observability are not the same thing. The system could safely refuse to operate while simultaneously logging why. It could maintain safety interlocks while transmitting diagnostic data. It could protect operators from hazards while giving technicians information to fix the hazard.\n\nInstead, it protected operators but left technicians blind.\n\nThis is a common pattern: systems designed with robust safety logic but minimal diagnostic capability. The assumption is that if safety is handled, everything else is secondary. But \"everything else\" includes the ability to understand failures, plan maintenance, and avoid emergency service calls.\n\nSafety loops must be independent: separate sensors, separate logic, separate authority to shut things down. But observability loops should be pervasive: instrument everything, log state changes, buffer diagnostics locally, transmit when possible.\n\nThese are not competing requirements. They're complementary.\n\n## Organizations Are Control Systems Too\n\nThis is where automation, technology, and management converge.\n\nOrganizations sense through metrics and reports. They decide through meetings and approvals. They actuate through people and processes. They respond with delay and distortion.\n\nWhen feedback is slow, organizations oscillate—overcorrecting to problems that have already changed.\n\nWhen authority is unclear, control saturates—multiple people trying to correct the same thing in incompatible ways.\n\nWhen incentives are misaligned, the system drives itself into failure—optimizing locally while degrading globally.\n\nThese are not cultural problems. They are control problems.\n\nThat dock control panel? The decision to exclude remote diagnostics wasn't technical. The panel manufacturer offered telemetry as an option. The warehouse operator decided against it to save $200 per bay—$2,400 total across twelve bays.\n\nThat was a reasonable decision given available information: the system had worked reliably elsewhere, diagnostic issues seemed unlikely, and budget was constrained.\n\nBut no one in the decision-making loop had experienced the cost of a four-hour diagnostic delay. No one modeled the probability of sensor degradation over time. No one considered that the warehouse was remote enough that emergency service calls were expensive. The organizational feedback loop—the one that should learn from operational experience and adjust specifications—was open.\n\nThe result: $2,400 saved during installation, $800 spent on the first service call, plus lost productivity, plus the knowledge that eleven other bays could fail the same way with the same diagnostic blindness.\n\nThe technical system was local. The organizational learning system was also local—each facility made its own decisions based on its own experience, with no feedback loop to aggregate lessons learned across sites.\n\nThis pattern repeats: automation projects where procurement optimizes for initial cost without visibility into operational cost. Remote monitoring proposals rejected because \"it's worked fine without it\" right up until it doesn't. Diagnostic capabilities treated as nice-to-have features instead of essential observability loops.\n\nThe technical system can be well designed, but if the organizational system around it is open-loop, failures are inevitable.\n\n## The Pattern Repeats\n\nThis dock control failure is not unique. The specifics change—different equipment, different sensors, different facilities—but the pattern is constant.\n\nSystems are designed with good control logic but minimal observability. Safety is prioritized (correctly) over diagnostics. Remote connectivity is seen as an unnecessary cost until the first time it would have saved multiples of that cost.\n\nAnd the lesson gets learned locally, slowly, expensively—one emergency service call at a time.\n\nThis is where control theory meets organizational reality. The technical feedback loop (sensor → logic → actuator) works fine. The diagnostic feedback loop (system behavior → human understanding → maintenance action) is open. And the organizational learning loop (operational experience → design decisions → better specifications) barely exists.\n\nEach loop matters. Each fails in predictable ways. And understanding one helps you recognize the others.\n\n## A Final Thought\n\nAutomation doesn't remove humans from systems. It changes where humans intervene—and how much information they have when they do.\n\nThe dock control panel didn't need humans removed from the safety loop. It needed humans with the right information at the right time: operators with clear panel feedback about system state, technicians with diagnostic telemetry about sensor health, managers with operational data about failure patterns across sites.\n\nWell-designed systems respect that reality. They put fast control close to the process. They add observability where diagnosis matters. They keep safety separate from optimization. They ensure someone owns the complete loop—not just the components, not just the initial installation, but the long-term operational behavior.\n\nPoorly designed systems hide these distinctions until something breaks.\n\nEvery automated system is a control system. Whether it behaves safely, stably, and predictably depends on whether its designers treated it like one.\n\nThe question is not whether you have automation. The question is whether you have control—and whether you can see what that control is doing when things go wrong."},{"slug":"blog_post_design_reviews","title":"What Experience Looks Like in Design Reviews","date":"2025-12-21","excerpt":"Inexperienced engineers talk a lot in design reviews. Experienced ones often don't. This is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/design-review2.png","tags":["design","experience","reviews","engineering"],"likes":0,"comments":0,"content":"<p>Inexperienced engineers tend to talk a lot in design reviews. Experienced ones often don&#39;t.</p>\n<p>This is frequently misinterpreted as disengagement. It isn&#39;t. It&#39;s pattern recognition at work.</p>\n<p>A junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There&#39;s error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.</p>\n<p>The team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.</p>\n<p>Thirty minutes in, the senior controls engineer—who&#39;s been silent the entire time—asks: &quot;What happens when the press cycles faster than the sensor settling time?&quot;</p>\n<p>Pause.</p>\n<p>&quot;The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.&quot;</p>\n<p>&quot;What happens when production pushes it to 75 strokes per minute to meet a deadline?&quot;</p>\n<p>Longer pause.</p>\n<p>That question wasn&#39;t on any slide. It wasn&#39;t in the requirements. But everyone in the room who&#39;s been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between &quot;rated for&quot; and &quot;tested at.&quot;</p>\n<p>Experience changes what you listen for.</p>\n<h2>The Questions That Matter Change Over Time</h2>\n<p>Early in a career, design reviews focus on how something works.</p>\n<p>Is the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?</p>\n<p>These are legitimate questions. They matter. But with experience, attention shifts to different territory:</p>\n<ul>\n<li>What assumptions are being made about the environment?</li>\n<li>Which constraints are fixed, and which are imagined?</li>\n<li>What happens when this fails at the worst possible time?</li>\n<li>Who will be awake at 3 AM when it does?</li>\n<li>How will we know it&#39;s failing before it&#39;s catastrophic?</li>\n</ul>\n<p>These questions don&#39;t show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.</p>\n<p>I remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.</p>\n<p>What I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values—and a tank overheated. The system worked exactly as designed. The design just didn&#39;t account for sensors that fail by lying rather than going silent.</p>\n<p>An experienced engineer would have asked: &quot;What does a failing sensor look like to this algorithm?&quot; Not because they&#39;re smarter, but because they&#39;ve seen that failure mode before. Probably more than once.</p>\n<h2>Silence Is a Signal</h2>\n<p>Experienced engineers are often quiet during the early part of a review.</p>\n<p>They let the design present itself. They listen for what is emphasized—and what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?</p>\n<p>When they finally speak, it is rarely to propose an alternative design. It is to test the edges:</p>\n<p>&quot;What happens if Modbus requests start taking 200ms instead of 50ms?&quot;</p>\n<p>&quot;How do we know this component is failing versus just slow?&quot;</p>\n<p>&quot;Who owns the configuration updates when this is in production?&quot;</p>\n<p>&quot;What&#39;s the plan when this needs to change and you&#39;re not here?&quot;</p>\n<p>These are not clever questions. They are uncomfortable ones. They don&#39;t have satisfying technical answers because they&#39;re not really technical questions—they&#39;re questions about risk, ownership, and operational reality.</p>\n<p>The inexperienced reviewer says: &quot;Have you considered using CRC32 instead of CRC16 for better error detection?&quot;</p>\n<p>The experienced reviewer says: &quot;What happens when the checksum passes but the data is still wrong?&quot;</p>\n<p>Both questions are about data integrity. Only one is about what actually happens in production.</p>\n<h2>Experience Recognizes Deferred Decisions</h2>\n<p>Many designs appear solid because they defer hard decisions:</p>\n<ul>\n<li>Retry logic described as &quot;exponential backoff with appropriate limits&quot; but never defined</li>\n<li>Operational ownership assumed but not assigned</li>\n<li>Failure handling described vaguely as &quot;graceful degradation&quot;</li>\n<li>Scaling concerns waved away with &quot;we&#39;ll monitor and adjust&quot;</li>\n<li>Integration complexity acknowledged but postponed to &quot;phase two&quot;</li>\n</ul>\n<p>Experience recognizes deferral instantly—not because deferral is always wrong, but because deferred decisions always come back with interest.</p>\n<p>&quot;We&#39;ll tune the timeouts in production&quot; sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.</p>\n<p>&quot;We&#39;ll add more detailed logging if needed&quot; sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.</p>\n<p>&quot;We&#39;ll clarify ownership during deployment&quot; sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.</p>\n<p>Production is very patient about collecting on these debts.</p>\n<p>I watched a team defer a decision about how to handle partial sensor array failures. &quot;We&#39;ll see how it behaves in production and tune the algorithm.&quot; Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn&#39;t designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.</p>\n<p>The cost of deferring that decision was higher than the cost of making it wrong during design would have been.</p>\n<h2>The Absence of Overconfidence</h2>\n<p>One of the clearest markers of experience is restraint.</p>\n<p>Experienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will—and the ways it might not.</p>\n<p>This is sometimes mistaken for pessimism. It isn&#39;t. It is respect for complexity.</p>\n<p>Confidence says: &quot;This will handle sensor failures.&quot;</p>\n<p>Experience says: &quot;This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We&#39;ve added bounds checking for the first case. The other two will require operational awareness.&quot;</p>\n<p>Confidence says: &quot;This will scale to 100 nodes.&quot;</p>\n<p>Experience says: &quot;This will scale to 100 nodes if network latency stays below 50ms and nodes don&#39;t join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We&#39;ll see it as increased jitter in the timing measurements.&quot;</p>\n<p>The difference is not about being negative. It&#39;s about being specific about what has been designed for and what has been assumed away.</p>\n<p>When someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn&#39;t understood the problem space well enough to see the edges.</p>\n<p>Most problems are not trivial.</p>\n<h2>Ownership Reveals Maturity</h2>\n<p>Designs that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.</p>\n<p>Experienced reviewers push on this immediately.</p>\n<p>Who owns this in production? Who gets paged when it misbehaves? Who can say &quot;no&quot; when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?</p>\n<p>If ownership is unclear, the design is incomplete—no matter how elegant the code.</p>\n<p>I&#39;ve seen systems designed by committee where every component had a different owner, and the interfaces between components were &quot;shared responsibility.&quot; Shared responsibility is another way of saying &quot;no one is responsible.&quot;</p>\n<p>When that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system&#39;s behavior was the outcome of those negotiations, not of any coherent design intent.</p>\n<p>The experienced engineer asks: &quot;Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?&quot;</p>\n<p>If the answer is &quot;well, it depends on which part is having issues,&quot; the ownership model is broken.</p>\n<p>Ownership isn&#39;t about credit. It&#39;s about who carries the mental model of the system&#39;s actual behavior—not its documented behavior, its actual behavior—and has the authority to act on that understanding.</p>\n<h2>Experience Is Often Mistaken for Negativity</h2>\n<p>The most experienced person in the room is often labeled &quot;difficult&quot; at least once.</p>\n<p>They ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn&#39;t include time for learning what was gotten wrong.</p>\n<p>This is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.</p>\n<p>What they are really doing is defending the future team from the present team&#39;s optimism.</p>\n<p>That is rarely popular in the moment. It is deeply appreciated later—usually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn&#39;t want to answer during the review has become an urgent problem with no good solutions.</p>\n<p>I&#39;ve been that &quot;difficult&quot; person. I&#39;ve asked questions that made presenters defensive. I&#39;ve pointed out gaps that felt like criticisms. I&#39;ve watched the energy in the room shift from enthusiasm to frustration.</p>\n<p>And I&#39;ve also gotten emails, months later, from those same presenters: &quot;Remember when you asked about [that thing]? It happened. We were ready because we&#39;d thought about it. Thank you.&quot;</p>\n<p>The experienced reviewer is not trying to stop the design. They are trying to make it survivable.</p>\n<h2>Why Design Reviews Fail</h2>\n<p>Most failed design reviews fail quietly.</p>\n<p>They approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.</p>\n<p>And then production teaches the lesson that the review avoided.</p>\n<p>Design reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.</p>\n<p>Good design reviews do not prevent failure. They prevent surprise.</p>\n<p>A good review leaves everyone with a shared understanding of:</p>\n<ul>\n<li>What has been designed for</li>\n<li>What has been assumed away</li>\n<li>Where the edges are</li>\n<li>Who owns what happens at those edges</li>\n</ul>\n<p>It does not guarantee success. It makes failure less catastrophic and more recoverable.</p>\n<h2>A Final Thought</h2>\n<p>Experience does not make you smarter in design reviews. It makes you more selective.</p>\n<p>You stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you&#39;re gone.</p>\n<p>That shift is subtle. And once you see it, you can&#39;t unsee it.</p>\n<p>You start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions—not because you&#39;re difficult, but because you&#39;ve seen what happens when no one asks them.</p>\n<p>And you realize that the best design reviews are not the ones where everyone agrees.</p>\n<p>They are the ones where everyone leaves slightly uncomfortable, but with clarity about why.</p>\n","rawContent":"\nInexperienced engineers tend to talk a lot in design reviews. Experienced ones often don't.\n\nThis is frequently misinterpreted as disengagement. It isn't. It's pattern recognition at work.\n\nA junior firmware engineer presents a new data acquisition system for an industrial press. The architecture is clean: sensors feed into an STM32, which aggregates readings and publishes over Modbus RTU to a supervisory PLC. There's error detection, retry logic, and graceful degradation. The presentation is thorough. Twenty slides. Clear diagrams.\n\nThe team asks questions. The junior engineer has answers. Protocol timing? Handled. Sensor failure modes? Covered. Integration with existing systems? Documented.\n\nThirty minutes in, the senior controls engineer—who's been silent the entire time—asks: \"What happens when the press cycles faster than the sensor settling time?\"\n\nPause.\n\n\"The sensors are rated for 100Hz sampling. The press cycles at 60 strokes per minute. We have margin.\"\n\n\"What happens when production pushes it to 75 strokes per minute to meet a deadline?\"\n\nLonger pause.\n\nThat question wasn't on any slide. It wasn't in the requirements. But everyone in the room who's been in production long enough knows: the system will eventually be pushed beyond its design envelope. Not because anyone is reckless, but because production pressures always find the gap between \"rated for\" and \"tested at.\"\n\nExperience changes what you listen for.\n\n## The Questions That Matter Change Over Time\n\nEarly in a career, design reviews focus on how something works.\n\nIs the state machine correct? Is the timing analysis sound? Is the protocol implementation compliant? Are the error paths covered?\n\nThese are legitimate questions. They matter. But with experience, attention shifts to different territory:\n\n- What assumptions are being made about the environment?\n- Which constraints are fixed, and which are imagined?\n- What happens when this fails at the worst possible time?\n- Who will be awake at 3 AM when it does?\n- How will we know it's failing before it's catastrophic?\n\nThese questions don't show up on architecture diagrams. They emerge from having seen systems behave badly in familiar ways.\n\nI remember reviewing a temperature control system early in my career. I focused on the PID tuning, the sensor accuracy, the control loop frequency. All technically sound.\n\nWhat I missed: the system assumed sensor readings were always valid. When a sensor eventually failed by reporting plausible but frozen values, the control loop dutifully maintained those values—and a tank overheated. The system worked exactly as designed. The design just didn't account for sensors that fail by lying rather than going silent.\n\nAn experienced engineer would have asked: \"What does a failing sensor look like to this algorithm?\" Not because they're smarter, but because they've seen that failure mode before. Probably more than once.\n\n## Silence Is a Signal\n\nExperienced engineers are often quiet during the early part of a review.\n\nThey let the design present itself. They listen for what is emphasized—and what is rushed past. They note which risks are acknowledged and which are avoided. They pay attention to the energy in the room: where does the presenter get confident, and where do they get vague?\n\nWhen they finally speak, it is rarely to propose an alternative design. It is to test the edges:\n\n\"What happens if Modbus requests start taking 200ms instead of 50ms?\"\n\n\"How do we know this component is failing versus just slow?\"\n\n\"Who owns the configuration updates when this is in production?\"\n\n\"What's the plan when this needs to change and you're not here?\"\n\nThese are not clever questions. They are uncomfortable ones. They don't have satisfying technical answers because they're not really technical questions—they're questions about risk, ownership, and operational reality.\n\nThe inexperienced reviewer says: \"Have you considered using CRC32 instead of CRC16 for better error detection?\"\n\nThe experienced reviewer says: \"What happens when the checksum passes but the data is still wrong?\"\n\nBoth questions are about data integrity. Only one is about what actually happens in production.\n\n## Experience Recognizes Deferred Decisions\n\nMany designs appear solid because they defer hard decisions:\n\n- Retry logic described as \"exponential backoff with appropriate limits\" but never defined\n- Operational ownership assumed but not assigned\n- Failure handling described vaguely as \"graceful degradation\"\n- Scaling concerns waved away with \"we'll monitor and adjust\"\n- Integration complexity acknowledged but postponed to \"phase two\"\n\nExperience recognizes deferral instantly—not because deferral is always wrong, but because deferred decisions always come back with interest.\n\n\"We'll tune the timeouts in production\" sounds reasonable. What it means is: we will discover the correct timeout values by finding out which values cause problems. The production system and its operators will pay the tuition for that education.\n\n\"We'll add more detailed logging if needed\" sounds pragmatic. What it means is: when this fails mysteriously, we will not have the information we need to understand why, and we will add logging in a panic, and that logging will probably be wrong the first time.\n\n\"We'll clarify ownership during deployment\" sounds like responsible planning. What it means is: when something breaks at 2 AM, there will be a delay while people figure out who is supposed to be responding, and that delay will be expensive in ways no one budgeted for.\n\nProduction is very patient about collecting on these debts.\n\nI watched a team defer a decision about how to handle partial sensor array failures. \"We'll see how it behaves in production and tune the algorithm.\" Three months later, a single failed sensor caused the system to oscillate because the algorithm wasn't designed for asymmetric input. The fix required a firmware update that needed a maintenance window. The maintenance window took six weeks to schedule. The oscillation cost measurable efficiency every day until then.\n\nThe cost of deferring that decision was higher than the cost of making it wrong during design would have been.\n\n## The Absence of Overconfidence\n\nOne of the clearest markers of experience is restraint.\n\nExperienced engineers are careful with certainty. They do not promise that something will work. They describe the conditions under which it probably will—and the ways it might not.\n\nThis is sometimes mistaken for pessimism. It isn't. It is respect for complexity.\n\nConfidence says: \"This will handle sensor failures.\"\n\nExperience says: \"This will handle sensors that fail by going silent. Sensors that fail by drifting slowly will look like environmental changes. Sensors that fail by reporting intermittently valid data will be harder to detect. We've added bounds checking for the first case. The other two will require operational awareness.\"\n\nConfidence says: \"This will scale to 100 nodes.\"\n\nExperience says: \"This will scale to 100 nodes if network latency stays below 50ms and nodes don't join simultaneously. If we exceed those conditions, the synchronization protocol will degrade. We'll see it as increased jitter in the timing measurements.\"\n\nThe difference is not about being negative. It's about being specific about what has been designed for and what has been assumed away.\n\nWhen someone presents with absolute confidence, experienced reviewers get nervous. Absolute confidence means either the problem is trivial, or the presenter hasn't understood the problem space well enough to see the edges.\n\nMost problems are not trivial.\n\n## Ownership Reveals Maturity\n\nDesigns that lack clear ownership often pass reviews easily. They are polite. They offend no one. They carefully avoid assigning responsibility for anything uncomfortable.\n\nExperienced reviewers push on this immediately.\n\nWho owns this in production? Who gets paged when it misbehaves? Who can say \"no\" when someone wants to add a feature that compromises the design? Who has the authority to simplify it later when it proves too complex?\n\nIf ownership is unclear, the design is incomplete—no matter how elegant the code.\n\nI've seen systems designed by committee where every component had a different owner, and the interfaces between components were \"shared responsibility.\" Shared responsibility is another way of saying \"no one is responsible.\"\n\nWhen that system started having integration issues, no single person had the authority to make decisions about tradeoffs. Every change required negotiation. The system's behavior was the outcome of those negotiations, not of any coherent design intent.\n\nThe experienced engineer asks: \"Who can wake up at 3 AM, look at this system in an unknown state, and make a judgment call about whether to restart it or leave it alone?\"\n\nIf the answer is \"well, it depends on which part is having issues,\" the ownership model is broken.\n\nOwnership isn't about credit. It's about who carries the mental model of the system's actual behavior—not its documented behavior, its actual behavior—and has the authority to act on that understanding.\n\n## Experience Is Often Mistaken for Negativity\n\nThe most experienced person in the room is often labeled \"difficult\" at least once.\n\nThey ask why something must exist at all. They question timelines that assume perfect execution. They introduce failure scenarios no one wants to think about. They point out that the schedule doesn't include time for learning what was gotten wrong.\n\nThis is uncomfortable. It slows down momentum. It introduces doubt into presentations that felt confident.\n\nWhat they are really doing is defending the future team from the present team's optimism.\n\nThat is rarely popular in the moment. It is deeply appreciated later—usually around 3 AM, when the pager goes off and someone realizes that the uncomfortable question they didn't want to answer during the review has become an urgent problem with no good solutions.\n\nI've been that \"difficult\" person. I've asked questions that made presenters defensive. I've pointed out gaps that felt like criticisms. I've watched the energy in the room shift from enthusiasm to frustration.\n\nAnd I've also gotten emails, months later, from those same presenters: \"Remember when you asked about [that thing]? It happened. We were ready because we'd thought about it. Thank you.\"\n\nThe experienced reviewer is not trying to stop the design. They are trying to make it survivable.\n\n## Why Design Reviews Fail\n\nMost failed design reviews fail quietly.\n\nThey approve systems that look reasonable, feel familiar, and fit within current organizational comfort. Everyone leaves the meeting satisfied. The design moves forward.\n\nAnd then production teaches the lesson that the review avoided.\n\nDesign reviews fail when they optimize for approval rather than understanding. When uncomfortable questions are seen as obstruction. When experience is interpreted as negativity. When the goal is to get the design approved rather than to understand its edges.\n\nGood design reviews do not prevent failure. They prevent surprise.\n\nA good review leaves everyone with a shared understanding of:\n- What has been designed for\n- What has been assumed away\n- Where the edges are\n- Who owns what happens at those edges\n\nIt does not guarantee success. It makes failure less catastrophic and more recoverable.\n\n## A Final Thought\n\nExperience does not make you smarter in design reviews. It makes you more selective.\n\nYou stop optimizing for correctness and start optimizing for survivability. You stop asking whether a system can work and start asking whether it can endure. You stop evaluating designs in isolation and start evaluating them in context: the operational environment, the organizational structure, the people who will maintain it when you're gone.\n\nThat shift is subtle. And once you see it, you can't unsee it.\n\nYou start noticing the patterns: the deferred decisions, the vague ownership, the overconfidence, the assumptions baked into diagrams. You start asking the uncomfortable questions—not because you're difficult, but because you've seen what happens when no one asks them.\n\nAnd you realize that the best design reviews are not the ones where everyone agrees.\n\nThey are the ones where everyone leaves slightly uncomfortable, but with clarity about why."},{"slug":"blog_post_first_incident","title":"The First Incident Changes Everything","date":"2025-12-20","excerpt":"There's a moment when ownership stops being theoretical and becomes visceral. It usually happens at 3 AM, with a production line stopped and people asking questions you don't have answers to yet.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/first-incident.png","tags":["incidents","ownership","production","systems"],"likes":0,"comments":0,"content":"<p>The pager goes off at 2:47 AM. You&#39;re the firmware lead for a new motor control system that&#39;s been in production for six weeks. It&#39;s been running fine. No issues. And then: &quot;Line 3 emergency stop. Controllers unresponsive. Production halted.&quot;</p>\n<p>You&#39;re on a call by 2:52. The plant manager is already there—in person, at the facility, at 3 AM. The line is still down. Every minute costs money you&#39;ve only thought about abstractly until now. The operators are watching. Maintenance is standing by. And everyone is looking at your system.</p>\n<p>Not metaphorically looking. Actually standing in front of the cabinet, pointing at LEDs, asking what they mean.</p>\n<p>You realize, with a clarity that&#39;s almost physical, that you don&#39;t know. You know what they&#39;re supposed to mean. You wrote the documentation. But right now, with the system in an unknown state, you&#39;re not sure. You&#39;re running mental simulations, trying to map LED patterns to states, wondering if the watchdog triggered or if it&#39;s a CAN bus fault or if something else entirely happened.</p>\n<p>The plant manager asks: &quot;How long until we&#39;re back up?&quot;</p>\n<p>This is the moment. Not the moment you become an experienced engineer. The moment you realize you weren&#39;t one yet.</p>\n<h2>When Ownership Becomes Real</h2>\n<p>Before your first real incident, ownership is theoretical. You own the firmware, sure. Your name is on the commit history. You wrote the architecture doc. You presented at the design review.</p>\n<p>But that&#39;s ownership as authorship. Ownership as credit.</p>\n<p>Real ownership is different. Real ownership is: when this breaks, you get called. When it misbehaves, you explain why. When someone needs to make a decision about whether to push forward or shut down, they&#39;re looking at you for information that might not exist yet.</p>\n<p>The shift happens fast. One moment you&#39;re proud of what you built. The next moment you&#39;re responsible for it in a way that makes pride seem quaint.</p>\n<p>I remember my first significant incident. A PLC I&#39;d written firmware for started dropping Modbus requests unpredictably. Not often—maybe one in ten thousand. Enough to be noticeable in production but rare enough that we&#39;d never caught it in testing. The system had error handling. It would retry. But the retries added latency, and that latency cascaded into other systems, and suddenly a water treatment facility was dealing with tank levels that were drifting outside normal operating ranges.</p>\n<p>No one was angry. Everyone was professional. But I could feel the weight of it: this wasn&#39;t my code anymore. It was their water treatment system. Their compliance requirements. Their operators who had to explain to management why levels were fluctuating.</p>\n<p>The code was the same. The system was the same. But what it meant was different.</p>\n<h2>How Teams Behave Under Public Failure</h2>\n<p>Here&#39;s something they don&#39;t teach you: teams change during incidents.</p>\n<p>The usual dynamics suspend. The person who never speaks up in meetings suddenly has crucial information. The senior engineer who usually drives decisions steps back and lets the person closest to the problem lead. Or sometimes the opposite happens—someone who&#39;s normally collaborative becomes territorial, defensive.</p>\n<p>You learn who stays calm and who doesn&#39;t. Who asks clarifying questions and who jumps to conclusions. Who admits when they don&#39;t know something and who deflects. Who thinks about the system and who thinks about blame.</p>\n<p>This isn&#39;t about good people versus bad people. It&#39;s about how pressure reveals what people actually optimize for when the stakes are real.</p>\n<p>I&#39;ve watched a junior technician with three months on the job diagnose a problem faster than anyone else because they&#39;d been the one doing the daily rounds and noticed a pattern no one else had seen. I&#39;ve watched a principal engineer who designed the original system defer to an operator who knew which breaker &quot;acted funny&quot; in a way no documentation captured.</p>\n<p>I&#39;ve also watched teams spiral. Someone suggests a theory, and instead of testing it, everyone starts building on it. Someone makes a change without announcing it, and now two people are modifying the same system with different assumptions. Someone gets frustrated and starts blaming the vendor, the previous team, the budget constraints—anything except the current problem.</p>\n<p>The best incident response I&#39;ve seen wasn&#39;t the fastest. It was the most deliberate. Someone—it happened to be a controls engineer who&#39;d been with the company for fifteen years—said: &quot;We don&#39;t understand what state we&#39;re in. Before we change anything, let&#39;s gather data.&quot; And then, critically: &quot;Who&#39;s writing this down?&quot;</p>\n<p>That changed everything. Not because documentation mattered in the moment, but because it forced everyone to externalize their mental model. When you have to say out loud what you think is happening and someone writes it down, you think differently. You catch your own assumptions.</p>\n<h2>What Broke Wasn&#39;t the System</h2>\n<p>After the incident, there&#39;s always a postmortem. And almost always, the postmortem focuses on the technical failure.</p>\n<p>In that motor control incident at 3 AM, the technical root cause was clear: a race condition in the watchdog handling code. Under specific timing conditions, the watchdog could timeout while a critical section was locked, and the recovery path assumed clean state that didn&#39;t exist. Textbook bug. We fixed it in forty lines of code.</p>\n<p>But that wasn&#39;t what broke.</p>\n<p>What broke was that the firmware had been written with assumptions about the execution environment that were true on the test bench and false in production. The test bench had consistent loop timing. Production had variable timing because of interrupt patterns we hadn&#39;t characterized. We&#39;d tested the watchdog recovery path, but we&#39;d tested it by triggering the watchdog deliberately, not by letting it trigger from timing variance.</p>\n<p>Okay, but that still sounds technical. Go deeper.</p>\n<p>Why didn&#39;t we characterize the interrupt patterns in production? Because commissioning was scheduled tight, and characterization takes time, and we had twelve other systems to bring up. </p>\n<p>Why was commissioning scheduled tight? Because the project was already over budget, and the plant needed the new line operational before the next quarter.</p>\n<p>Why was the project over budget? Partly because the scope grew during implementation, partly because integration with legacy systems took longer than estimated, partly because we&#39;d optimized the bid to win the contract.</p>\n<p>Who decided that the commissioning schedule was more important than thorough characterization? No one, explicitly. It was a collective drift. A series of small decisions that each made sense locally but accumulated into a gap where critical testing didn&#39;t happen.</p>\n<p>Here&#39;s what actually broke: the communication path between the firmware team and the commissioning team. We didn&#39;t have a clear handoff protocol for &quot;things that need to be validated in production that we can&#39;t validate in the lab.&quot; That&#39;s not a technical problem. That&#39;s an organizational design problem.</p>\n<p>And beneath that: the incentive structure. Commissioning engineers were measured on schedule adherence. Firmware engineers were measured on feature completion. No one was measured on &quot;probability we understand the system&#39;s actual behavior in production.&quot;</p>\n<p>So we didn&#39;t do it. Not because anyone was negligent. Because the organization wasn&#39;t designed to value it.</p>\n<h2>Why Technical Postmortems Often Miss the Point</h2>\n<p>Most postmortems I&#39;ve read follow a pattern:</p>\n<p><strong>Incident summary</strong>: What happened and when<br><strong>Impact</strong>: Who was affected and how<br><strong>Root cause</strong>: The technical failure<br><strong>Contributing factors</strong>: Other technical issues<br><strong>Action items</strong>: Technical fixes</p>\n<p>This format is comfortable because it suggests the problem was technical and therefore fixable. Change the code, add a check, improve the test suite. Done.</p>\n<p>But look at what&#39;s missing:</p>\n<ul>\n<li>Why did the design review not catch this failure mode?</li>\n<li>What information existed that wasn&#39;t shared?</li>\n<li>Which tradeoffs were made consciously versus unconsciously?</li>\n<li>What incentives led to those tradeoffs?</li>\n<li>How did the team&#39;s understanding of the system differ from reality?</li>\n</ul>\n<p>These questions are harder because the answers implicate decisions, not just code. They make people uncomfortable because they suggest the incident wasn&#39;t just bad luck or an obscure bug—it was predictable given how the system was designed and how the organization operates.</p>\n<p>I once sat in a postmortem for a system that failed because of insufficient input validation. The root cause was listed as &quot;missing bounds check.&quot; The action item was &quot;add bounds checking to all inputs.&quot;</p>\n<p>But the actual story was different. The bounds checking had been in the original design. It was removed during optimization because it added 2ms to the control loop latency, and the spec required sub-10ms response time. The decision to remove it was documented in a code review but not escalated. The person who reviewed it assumed someone else would catch it in integration testing. The integration testing was abbreviated because commissioning was behind schedule.</p>\n<p>The missing bounds check was the mechanism of failure. But what failed was a web of assumptions about who was responsible for verifying what, under what time pressure, with what communication overhead.</p>\n<p>Fixing the bounds check addressed the symptom. It didn&#39;t address why a known-critical validation got removed without proper scrutiny.</p>\n<h2>&quot;The System Worked Exactly as Designed&quot;</h2>\n<p>This phrase appears in postmortems as a conclusion. It&#39;s meant to close the investigation: there was no malfunction, just an unanticipated scenario.</p>\n<p>But it&#39;s the most dangerous phrase in incident analysis because it&#39;s almost always true—and almost always misunderstood.</p>\n<p>Yes, the system worked as designed. The problem is that &quot;the design&quot; includes far more than you documented.</p>\n<p>It includes the implicit assumptions you made about the operating environment. The tradeoffs you accepted to meet the schedule. The risks you deprioritized because they seemed unlikely. The monitoring you didn&#39;t implement because it wasn&#39;t in scope. The operational knowledge that exists only in people&#39;s heads.</p>\n<p>When the motor controllers became unresponsive at 2:47 AM, they worked exactly as designed: the watchdog detected a timeout, triggered the recovery path, and entered a safe state. The recovery path was designed to reset the system to a known-good state. It did that.</p>\n<p>What it didn&#39;t do was account for the scenario where the &quot;known-good state&quot; wasn&#39;t actually safe because other systems were depending on outputs that were now frozen at their last value. That wasn&#39;t a bug in the recovery logic. That was a gap in the system-level design.</p>\n<p>But more than that: it was a gap in how we thought about the system. We&#39;d designed the controller firmware in isolation, with defined interfaces. We&#39;d assumed those interfaces were sufficient to capture the dependencies. They weren&#39;t. The actual dependencies included timing relationships, implicit state machines in other systems, and operator expectations that weren&#39;t written anywhere.</p>\n<p>The system worked exactly as designed. The design was incomplete.</p>\n<h2>The Difference Between Intent and Reality</h2>\n<p>There&#39;s a moment during every significant incident where you realize: we built what we said we&#39;d build. We just didn&#39;t build what was needed.</p>\n<p>Not because we were incompetent. Because reality is richer than specification.</p>\n<p>The spec said &quot;handle watchdog timeout.&quot; It didn&#39;t say &quot;handle watchdog timeout in a way that doesn&#39;t leave dependent systems in ambiguous state.&quot; That seemed implied. It wasn&#39;t.</p>\n<p>The spec said &quot;buffer sensor data during communication loss.&quot; It didn&#39;t say &quot;buffer data in a way that&#39;s meaningful when different sensors lose communication at different times.&quot; We assumed synchronization. The assumption was wrong.</p>\n<p>The spec said &quot;provide HMI feedback for fault conditions.&quot; It didn&#39;t say &quot;provide feedback that operators can interpret correctly at 3 AM when multiple faults are active simultaneously.&quot; We tested each fault individually. They don&#39;t happen individually in production.</p>\n<p>Intent and reality diverge in the space between what you specify and what you assume. And you don&#39;t discover what you assumed until it&#39;s tested in an environment that doesn&#39;t share your assumptions.</p>\n<h2>What Changes After</h2>\n<p>The first incident changes everything because it changes what &quot;working&quot; means.</p>\n<p>Before: working meant passing tests, meeting specifications, behaving correctly under expected conditions.</p>\n<p>After: working means remaining understandable and recoverable when conditions aren&#39;t expected. When you don&#39;t have complete information. When the people responding aren&#39;t the people who built it. When time is expensive.</p>\n<p>You start designing differently. Not necessarily better—sometimes the right design is still the simple one. But you design with the knowledge that the system will eventually teach you what you got wrong, and the question is whether you&#39;ve made it possible to learn that lesson without catastrophic cost.</p>\n<p>You write different documentation. Not more documentation—more documentation is often worse. But documentation that captures why, not just what. Documentation that names the assumptions. Documentation that future-you, woken up at 3 AM and trying to understand what state the system is in, will actually use.</p>\n<p>You test differently. Not just testing the happy path and the error paths. Testing the transitions. Testing what happens when error paths stack. Testing what happens when recovery takes longer than expected. Testing what happens when the test itself is wrong.</p>\n<p>And you think about ownership differently. Not as credit, but as responsibility. Not as &quot;I built this,&quot; but as &quot;I&#39;m accountable for what this does in an environment I don&#39;t control.&quot;</p>\n<p>That motor control system got fixed. The race condition was patched. We added instrumentation to characterize interrupt timing in production. We updated commissioning procedures to include the checks we&#39;d skipped.</p>\n<p>But more than that: I stopped being surprised when systems behaved in ways I hadn&#39;t anticipated. I started assuming they would. And I started designing for that assumption.</p>\n<p>Because the first incident taught me something no design review ever could: the system you build is not the system that runs in production.</p>\n<p>Production adds variables you didn&#39;t model. Pressures you didn&#39;t simulate. Dependencies you didn&#39;t document. And people who need answers you don&#39;t have yet.</p>\n<p>The question isn&#39;t whether you&#39;ll face that gap. The question is whether you&#39;ve designed for it.</p>\n<p>Most of the time, you haven&#39;t.</p>\n<p>And that&#39;s okay—as long as you remember it.</p>\n<p>The first incident won&#39;t be your last. But if you learn from it, the next one will be different.</p>\n<p>Not easier. Just different.</p>\n<p>And maybe, slowly, you&#39;ll build systems that fail in ways that are less surprising, less catastrophic, and more recoverable.</p>\n<p>That&#39;s not mastery. That&#39;s just experience.</p>\n<p>And it starts with that first call at 2:47 AM.</p>\n","rawContent":"\nThe pager goes off at 2:47 AM. You're the firmware lead for a new motor control system that's been in production for six weeks. It's been running fine. No issues. And then: \"Line 3 emergency stop. Controllers unresponsive. Production halted.\"\n\nYou're on a call by 2:52. The plant manager is already there—in person, at the facility, at 3 AM. The line is still down. Every minute costs money you've only thought about abstractly until now. The operators are watching. Maintenance is standing by. And everyone is looking at your system.\n\nNot metaphorically looking. Actually standing in front of the cabinet, pointing at LEDs, asking what they mean.\n\nYou realize, with a clarity that's almost physical, that you don't know. You know what they're supposed to mean. You wrote the documentation. But right now, with the system in an unknown state, you're not sure. You're running mental simulations, trying to map LED patterns to states, wondering if the watchdog triggered or if it's a CAN bus fault or if something else entirely happened.\n\nThe plant manager asks: \"How long until we're back up?\"\n\nThis is the moment. Not the moment you become an experienced engineer. The moment you realize you weren't one yet.\n\n## When Ownership Becomes Real\n\nBefore your first real incident, ownership is theoretical. You own the firmware, sure. Your name is on the commit history. You wrote the architecture doc. You presented at the design review.\n\nBut that's ownership as authorship. Ownership as credit.\n\nReal ownership is different. Real ownership is: when this breaks, you get called. When it misbehaves, you explain why. When someone needs to make a decision about whether to push forward or shut down, they're looking at you for information that might not exist yet.\n\nThe shift happens fast. One moment you're proud of what you built. The next moment you're responsible for it in a way that makes pride seem quaint.\n\nI remember my first significant incident. A PLC I'd written firmware for started dropping Modbus requests unpredictably. Not often—maybe one in ten thousand. Enough to be noticeable in production but rare enough that we'd never caught it in testing. The system had error handling. It would retry. But the retries added latency, and that latency cascaded into other systems, and suddenly a water treatment facility was dealing with tank levels that were drifting outside normal operating ranges.\n\nNo one was angry. Everyone was professional. But I could feel the weight of it: this wasn't my code anymore. It was their water treatment system. Their compliance requirements. Their operators who had to explain to management why levels were fluctuating.\n\nThe code was the same. The system was the same. But what it meant was different.\n\n## How Teams Behave Under Public Failure\n\nHere's something they don't teach you: teams change during incidents.\n\nThe usual dynamics suspend. The person who never speaks up in meetings suddenly has crucial information. The senior engineer who usually drives decisions steps back and lets the person closest to the problem lead. Or sometimes the opposite happens—someone who's normally collaborative becomes territorial, defensive.\n\nYou learn who stays calm and who doesn't. Who asks clarifying questions and who jumps to conclusions. Who admits when they don't know something and who deflects. Who thinks about the system and who thinks about blame.\n\nThis isn't about good people versus bad people. It's about how pressure reveals what people actually optimize for when the stakes are real.\n\nI've watched a junior technician with three months on the job diagnose a problem faster than anyone else because they'd been the one doing the daily rounds and noticed a pattern no one else had seen. I've watched a principal engineer who designed the original system defer to an operator who knew which breaker \"acted funny\" in a way no documentation captured.\n\nI've also watched teams spiral. Someone suggests a theory, and instead of testing it, everyone starts building on it. Someone makes a change without announcing it, and now two people are modifying the same system with different assumptions. Someone gets frustrated and starts blaming the vendor, the previous team, the budget constraints—anything except the current problem.\n\nThe best incident response I've seen wasn't the fastest. It was the most deliberate. Someone—it happened to be a controls engineer who'd been with the company for fifteen years—said: \"We don't understand what state we're in. Before we change anything, let's gather data.\" And then, critically: \"Who's writing this down?\"\n\nThat changed everything. Not because documentation mattered in the moment, but because it forced everyone to externalize their mental model. When you have to say out loud what you think is happening and someone writes it down, you think differently. You catch your own assumptions.\n\n## What Broke Wasn't the System\n\nAfter the incident, there's always a postmortem. And almost always, the postmortem focuses on the technical failure.\n\nIn that motor control incident at 3 AM, the technical root cause was clear: a race condition in the watchdog handling code. Under specific timing conditions, the watchdog could timeout while a critical section was locked, and the recovery path assumed clean state that didn't exist. Textbook bug. We fixed it in forty lines of code.\n\nBut that wasn't what broke.\n\nWhat broke was that the firmware had been written with assumptions about the execution environment that were true on the test bench and false in production. The test bench had consistent loop timing. Production had variable timing because of interrupt patterns we hadn't characterized. We'd tested the watchdog recovery path, but we'd tested it by triggering the watchdog deliberately, not by letting it trigger from timing variance.\n\nOkay, but that still sounds technical. Go deeper.\n\nWhy didn't we characterize the interrupt patterns in production? Because commissioning was scheduled tight, and characterization takes time, and we had twelve other systems to bring up. \n\nWhy was commissioning scheduled tight? Because the project was already over budget, and the plant needed the new line operational before the next quarter.\n\nWhy was the project over budget? Partly because the scope grew during implementation, partly because integration with legacy systems took longer than estimated, partly because we'd optimized the bid to win the contract.\n\nWho decided that the commissioning schedule was more important than thorough characterization? No one, explicitly. It was a collective drift. A series of small decisions that each made sense locally but accumulated into a gap where critical testing didn't happen.\n\nHere's what actually broke: the communication path between the firmware team and the commissioning team. We didn't have a clear handoff protocol for \"things that need to be validated in production that we can't validate in the lab.\" That's not a technical problem. That's an organizational design problem.\n\nAnd beneath that: the incentive structure. Commissioning engineers were measured on schedule adherence. Firmware engineers were measured on feature completion. No one was measured on \"probability we understand the system's actual behavior in production.\"\n\nSo we didn't do it. Not because anyone was negligent. Because the organization wasn't designed to value it.\n\n## Why Technical Postmortems Often Miss the Point\n\nMost postmortems I've read follow a pattern:\n\n**Incident summary**: What happened and when  \n**Impact**: Who was affected and how  \n**Root cause**: The technical failure  \n**Contributing factors**: Other technical issues  \n**Action items**: Technical fixes\n\nThis format is comfortable because it suggests the problem was technical and therefore fixable. Change the code, add a check, improve the test suite. Done.\n\nBut look at what's missing:\n\n- Why did the design review not catch this failure mode?\n- What information existed that wasn't shared?\n- Which tradeoffs were made consciously versus unconsciously?\n- What incentives led to those tradeoffs?\n- How did the team's understanding of the system differ from reality?\n\nThese questions are harder because the answers implicate decisions, not just code. They make people uncomfortable because they suggest the incident wasn't just bad luck or an obscure bug—it was predictable given how the system was designed and how the organization operates.\n\nI once sat in a postmortem for a system that failed because of insufficient input validation. The root cause was listed as \"missing bounds check.\" The action item was \"add bounds checking to all inputs.\"\n\nBut the actual story was different. The bounds checking had been in the original design. It was removed during optimization because it added 2ms to the control loop latency, and the spec required sub-10ms response time. The decision to remove it was documented in a code review but not escalated. The person who reviewed it assumed someone else would catch it in integration testing. The integration testing was abbreviated because commissioning was behind schedule.\n\nThe missing bounds check was the mechanism of failure. But what failed was a web of assumptions about who was responsible for verifying what, under what time pressure, with what communication overhead.\n\nFixing the bounds check addressed the symptom. It didn't address why a known-critical validation got removed without proper scrutiny.\n\n## \"The System Worked Exactly as Designed\"\n\nThis phrase appears in postmortems as a conclusion. It's meant to close the investigation: there was no malfunction, just an unanticipated scenario.\n\nBut it's the most dangerous phrase in incident analysis because it's almost always true—and almost always misunderstood.\n\nYes, the system worked as designed. The problem is that \"the design\" includes far more than you documented.\n\nIt includes the implicit assumptions you made about the operating environment. The tradeoffs you accepted to meet the schedule. The risks you deprioritized because they seemed unlikely. The monitoring you didn't implement because it wasn't in scope. The operational knowledge that exists only in people's heads.\n\nWhen the motor controllers became unresponsive at 2:47 AM, they worked exactly as designed: the watchdog detected a timeout, triggered the recovery path, and entered a safe state. The recovery path was designed to reset the system to a known-good state. It did that.\n\nWhat it didn't do was account for the scenario where the \"known-good state\" wasn't actually safe because other systems were depending on outputs that were now frozen at their last value. That wasn't a bug in the recovery logic. That was a gap in the system-level design.\n\nBut more than that: it was a gap in how we thought about the system. We'd designed the controller firmware in isolation, with defined interfaces. We'd assumed those interfaces were sufficient to capture the dependencies. They weren't. The actual dependencies included timing relationships, implicit state machines in other systems, and operator expectations that weren't written anywhere.\n\nThe system worked exactly as designed. The design was incomplete.\n\n## The Difference Between Intent and Reality\n\nThere's a moment during every significant incident where you realize: we built what we said we'd build. We just didn't build what was needed.\n\nNot because we were incompetent. Because reality is richer than specification.\n\nThe spec said \"handle watchdog timeout.\" It didn't say \"handle watchdog timeout in a way that doesn't leave dependent systems in ambiguous state.\" That seemed implied. It wasn't.\n\nThe spec said \"buffer sensor data during communication loss.\" It didn't say \"buffer data in a way that's meaningful when different sensors lose communication at different times.\" We assumed synchronization. The assumption was wrong.\n\nThe spec said \"provide HMI feedback for fault conditions.\" It didn't say \"provide feedback that operators can interpret correctly at 3 AM when multiple faults are active simultaneously.\" We tested each fault individually. They don't happen individually in production.\n\nIntent and reality diverge in the space between what you specify and what you assume. And you don't discover what you assumed until it's tested in an environment that doesn't share your assumptions.\n\n## What Changes After\n\nThe first incident changes everything because it changes what \"working\" means.\n\nBefore: working meant passing tests, meeting specifications, behaving correctly under expected conditions.\n\nAfter: working means remaining understandable and recoverable when conditions aren't expected. When you don't have complete information. When the people responding aren't the people who built it. When time is expensive.\n\nYou start designing differently. Not necessarily better—sometimes the right design is still the simple one. But you design with the knowledge that the system will eventually teach you what you got wrong, and the question is whether you've made it possible to learn that lesson without catastrophic cost.\n\nYou write different documentation. Not more documentation—more documentation is often worse. But documentation that captures why, not just what. Documentation that names the assumptions. Documentation that future-you, woken up at 3 AM and trying to understand what state the system is in, will actually use.\n\nYou test differently. Not just testing the happy path and the error paths. Testing the transitions. Testing what happens when error paths stack. Testing what happens when recovery takes longer than expected. Testing what happens when the test itself is wrong.\n\nAnd you think about ownership differently. Not as credit, but as responsibility. Not as \"I built this,\" but as \"I'm accountable for what this does in an environment I don't control.\"\n\nThat motor control system got fixed. The race condition was patched. We added instrumentation to characterize interrupt timing in production. We updated commissioning procedures to include the checks we'd skipped.\n\nBut more than that: I stopped being surprised when systems behaved in ways I hadn't anticipated. I started assuming they would. And I started designing for that assumption.\n\nBecause the first incident taught me something no design review ever could: the system you build is not the system that runs in production.\n\nProduction adds variables you didn't model. Pressures you didn't simulate. Dependencies you didn't document. And people who need answers you don't have yet.\n\nThe question isn't whether you'll face that gap. The question is whether you've designed for it.\n\nMost of the time, you haven't.\n\nAnd that's okay—as long as you remember it.\n\nThe first incident won't be your last. But if you learn from it, the next one will be different.\n\nNot easier. Just different.\n\nAnd maybe, slowly, you'll build systems that fail in ways that are less surprising, less catastrophic, and more recoverable.\n\nThat's not mastery. That's just experience.\n\nAnd it starts with that first call at 2:47 AM."},{"slug":"the-happy-post-lie","title":"The Happy Path Is a Lie We Tell Ourselves","date":"2025-12-01","excerpt":"Design reviews reward optimism. Test environments validate assumptions. And somewhere between approval and deployment, we forget that we're building systems for a world that doesn't care about our diagrams.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/design-review.png","tags":["systems","design","production","failure"],"likes":0,"comments":0,"content":"<p>Three engineers sit in a design review for a new temperature monitoring system. The architecture is clean: sensors report to edge controllers, controllers aggregate to a gateway, gateway publishes to the cloud. Redundancy at every layer. Graceful degradation clearly marked on the diagram.</p>\n<p>Someone asks: &quot;What happens if the gateway loses connectivity?&quot;</p>\n<p>&quot;It buffers locally for up to 72 hours,&quot; comes the answer. &quot;More than enough for any reasonable outage.&quot;</p>\n<p>Everyone nods. The design is approved.</p>\n<p>Two years later, a cellular provider pushes a configuration update that breaks connectivity for five days. The gateway&#39;s flash memory fills in eighteen hours. When connectivity returns, 96 hours of critical temperature data is gone. The system worked exactly as designed—it just wasn&#39;t designed for this.</p>\n<p>The happy path is seductive because it lets us move forward. But it&#39;s also a lie we tell ourselves, and design reviews are where that lie gets institutionalized.</p>\n<h2>Why Design Reviews Reward Optimism</h2>\n<p>Design reviews are supposed to find problems. In practice, they often do the opposite: they create consensus around the assumption that problems won&#39;t happen.</p>\n<p>Here&#39;s how it works:</p>\n<p>You present a design. You show the nominal flow. You mark the error paths. You explain the redundancies. And then the questions come—but they come in a specific form: &quot;What if X fails?&quot; You answer with your contingency for X. &quot;What if Y happens?&quot; You show your handling of Y.</p>\n<p>Each question asked and answered creates the illusion that you&#39;ve covered the space of possible failures. What&#39;s harder to see is the space of failures that weren&#39;t asked about—not because they&#39;re impossible, but because they&#39;re hard to imagine from a conference room.</p>\n<p>The cellular configuration update that breaks connectivity for five days isn&#39;t a question anyone asks, because connectivity outages are supposed to last hours, not days. The sensor that doesn&#39;t fail cleanly but drifts slowly enough to stay within acceptance bounds for months isn&#39;t on the checklist, because sensors are supposed to either work or fail obviously. The operator who learns to bypass an interlock because it trips too often during valid operations isn&#39;t in the room, because we&#39;re reviewing technical design, not operational reality.</p>\n<p>Design reviews optimize for demonstrable coverage of known failure modes. They reward the ability to show that you&#39;ve thought about things. But &quot;thinking about things&quot; is different from designing for them, and both are different from experiencing them.</p>\n<p>This creates a subtle bias: the designs that pass review most easily are the ones that look most robust on paper. Clean boundaries. Clear error handling. Documented assumptions. The designs that acknowledge fundamental uncertainties—&quot;we don&#39;t know how operators will actually use this,&quot; &quot;we can&#39;t predict how this will interact with the legacy system at scale&quot;—feel incomplete. They sound like excuses.</p>\n<p>So we learn to present certainty. We learn to have answers. And slowly, the happy path becomes the only path we&#39;re willing to defend.</p>\n<h2>How Organizations Institutionalize Blind Spots</h2>\n<p>Individual engineers know the happy path is optimistic. But organizations have a way of turning individual caution into collective blindness.</p>\n<p>Consider what happens after that design review. The design is approved. It goes into a requirements document. The requirements become tasks. The tasks become sprints. And at each translation, something is lost.</p>\n<p>The subtle caveat—&quot;this assumes network partitions are transient&quot;—becomes &quot;handles network failures.&quot; The hedge—&quot;buffering capacity should be sufficient for typical outages&quot;—becomes &quot;72-hour buffer.&quot; The uncertainty—&quot;we&#39;ll need to monitor how this behaves under load&quot;—becomes a checkbox: &quot;performance tested.&quot;</p>\n<p>This isn&#39;t malice. It&#39;s how organizations create actionable plans from ambiguous reality. But in the process, assumptions get promoted to facts, and facts get encoded into architecture.</p>\n<p>Worse, once something is designed and built, it becomes expensive to question. Not just financially expensive—politically expensive. The team that built it has invested in it. Managers have reported progress on it. Customers have been promised it. To say &quot;we need to rethink this&quot; is to say all that investment might have been misdirected.</p>\n<p>So instead, we add compensating controls. We build monitoring. We write runbooks. We train operators. Each addition reinforces the original design rather than questioning it. We&#39;re not asking &quot;should this system exist in this form?&quot; We&#39;re asking &quot;how do we make this system work?&quot;</p>\n<p>I&#39;ve sat in meetings where everyone in the room privately knew a system was brittle, but no one said it directly because the system was already in production and replacing it would be a six-month project. Instead, we talked about &quot;hardening&quot; and &quot;resilience improvements&quot;—language that suggested we were making something robust rather than patching something fundamentally fragile.</p>\n<p>The organization&#39;s immune system had learned to reject the observation that the system was designed wrong, because accepting that observation would require acknowledging that a lot of other decisions were also wrong.</p>\n<h2>Most Failures Are Designed</h2>\n<p>Here&#39;s an uncomfortable truth: most production failures aren&#39;t accidents. They&#39;re not the result of bugs that slipped through testing or edge cases that no one thought of.</p>\n<p>They&#39;re the inevitable outcome of decisions made under constraints.</p>\n<p>That temperature monitoring system that lost 96 hours of data? The decision to use flash memory with limited write endurance was made to hit a cost target. The decision to buffer for 72 hours was made based on historical uptime data from a different cellular provider. The decision not to implement hierarchical buffering (edge → gateway → cloud) was made to keep the architecture simple and shippable.</p>\n<p>None of those were wrong decisions in isolation. Given the constraints—budget, schedule, what was known at the time—they were defensible. Reasonable, even.</p>\n<p>But they were decisions, not accidents. And decisions have consequences that aren&#39;t always visible until they compound.</p>\n<p>This is what I mean when I say most failures are designed. Not that anyone set out to build a fragile system, but that fragility is often the natural result of optimizing for other things: speed, cost, simplicity, familiarity.</p>\n<p>The difference between a bug and a decision is that bugs can be fixed. Decisions are encoded into the architecture. They become load-bearing assumptions. You can&#39;t fix them without rethinking the system.</p>\n<p>When an incident review concludes &quot;the system worked as designed, but we didn&#39;t anticipate this scenario,&quot; what they&#39;re really saying is: &quot;we made tradeoffs, and this is what we traded away.&quot;</p>\n<p>The question is whether we&#39;re honest about what we&#39;re trading. Most of the time, we&#39;re not—because being honest would make it harder to get the design approved.</p>\n<h2>Why Test, Staging, and Simulation Always Mislead</h2>\n<p>Every environment before production is a curated experience.</p>\n<p>Test environments use clean data. Staging uses a subset of production scale. Simulations use models that abstract away complexity. Even load testing is fundamentally artificial—you generate the load, you choose when to apply it, you know what you&#39;re testing for.</p>\n<p>This isn&#39;t a criticism of testing. Testing is essential. But it&#39;s also fundamentally limited, and we consistently underestimate how limited it is.</p>\n<p>Here&#39;s what test environments can&#39;t show you:</p>\n<p><strong>They can&#39;t show you emergent behavior.</strong> That interaction between the VFD noise and the CAN bus that only manifests when both systems are under load and the ambient temperature is above 30°C? Your test bench runs at 22°C with one system at a time.</p>\n<p><strong>They can&#39;t show you operational reality.</strong> The operator who learned that if you cycle the power on the HMI in a specific sequence, you can temporarily bypass an error condition that otherwise requires a maintenance window? They discovered that in production, under pressure, when the line manager was demanding a workaround.</p>\n<p><strong>They can&#39;t show you the full dependency graph.</strong> You tested integration with the ERP system. You didn&#39;t test integration with the ERP system while the network team is doing maintenance and traffic is rerouting through a secondary path with higher latency and the backup MES is handling requests because the primary is being patched.</p>\n<p><strong>They can&#39;t show you time.</strong> That sensor calibration drift that takes six months to become problematic? Your acceptance test runs for six hours.</p>\n<p><strong>They can&#39;t show you organizational dynamics.</strong> Who gets called when something ambiguous happens at 2 AM? Who has the authority to make the call to shut down the line? Who actually knows where the documentation is? None of that exists in staging.</p>\n<p>Production is the only environment where all the variables are real, all at once, without anyone curating the experience.</p>\n<h2>What Production Pressure Reveals That Design Never Does</h2>\n<p>There&#39;s a specific kind of knowledge that only emerges under production pressure, and it has nothing to do with technical design.</p>\n<p>It&#39;s the knowledge of what actually matters.</p>\n<p>In design, everything matters equally. Every requirement is important. Every failure mode is worth handling. Every dependency is documented. But in production, under time pressure, with costs accumulating, you discover very quickly what&#39;s truly critical and what was just conceptually important.</p>\n<p>You discover that the &quot;critical&quot; alert that fires three times a day gets ignored, while the informal message in Slack from the night shift operator gets immediate attention because everyone knows that operator only speaks up when something is genuinely wrong.</p>\n<p>You discover that the official escalation path—submit a ticket, wait for triage, get assigned to the on-call engineer—is too slow, and there&#39;s an unofficial path where certain people have certain phone numbers and that&#39;s what actually gets used when things are breaking.</p>\n<p>You discover that the monitoring dashboard everyone insisted on building never gets looked at during incidents, but everyone opens the same three SSH sessions to check the same three log files because that&#39;s where the useful information actually lives.</p>\n<p>You discover that the runbook that took weeks to write is useless because it assumes you have time to read it, and in production you don&#39;t—so people fall back on intuition, pattern matching, and educated guesses.</p>\n<p>None of this is visible during design because design happens in an environment where time is less expensive and mistakes are reversible. Production pressure doesn&#39;t just reveal technical problems. It reveals organizational truth.</p>\n<p>It shows you which abstractions hold up and which ones collapse. It shows you where authority actually lies versus where the org chart says it lies. It shows you which skills matter—and they&#39;re often not the ones that got people promoted.</p>\n<h2>What This Means for How We Design</h2>\n<p>I&#39;m not arguing that we should abandon design reviews or testing or staging environments. They serve a purpose. The problem is that we treat them as validation rather than exploration.</p>\n<p>We treat passing a design review as evidence that the design is good. It&#39;s not. It&#39;s evidence that the design is defensible given what we know now, in this room, without production pressure.</p>\n<p>We treat staging as a high-fidelity simulation of production. It&#39;s not. It&#39;s a curated environment that shares some properties with production and systematically excludes others.</p>\n<p>If we were honest about this, we&#39;d design differently. We&#39;d spend less energy trying to anticipate every failure mode and more energy building systems that can be understood and modified when inevitably surprising failures occur.</p>\n<p>We&#39;d document not just what the system does, but what we assumed about the environment it operates in. Not as a CYA exercise, but as a genuine artifact that helps the next person understand what we were thinking—and where we were probably wrong.</p>\n<p>We&#39;d treat the first six months in production not as &quot;stabilization&quot; but as &quot;learning what we actually built&quot;—because that&#39;s what it is.</p>\n<p>The happy path isn&#39;t useless. It&#39;s necessary to ship anything at all. But it&#39;s a lie we tell ourselves to make progress in the face of uncertainty.</p>\n<p>The question is whether we remember it&#39;s a lie—or whether we start believing it.</p>\n<p>Because the system will tell the truth eventually.</p>\n<p>It always does.</p>\n","rawContent":"\r\nThree engineers sit in a design review for a new temperature monitoring system. The architecture is clean: sensors report to edge controllers, controllers aggregate to a gateway, gateway publishes to the cloud. Redundancy at every layer. Graceful degradation clearly marked on the diagram.\r\n\r\nSomeone asks: \"What happens if the gateway loses connectivity?\"\r\n\r\n\"It buffers locally for up to 72 hours,\" comes the answer. \"More than enough for any reasonable outage.\"\r\n\r\nEveryone nods. The design is approved.\r\n\r\nTwo years later, a cellular provider pushes a configuration update that breaks connectivity for five days. The gateway's flash memory fills in eighteen hours. When connectivity returns, 96 hours of critical temperature data is gone. The system worked exactly as designed—it just wasn't designed for this.\r\n\r\nThe happy path is seductive because it lets us move forward. But it's also a lie we tell ourselves, and design reviews are where that lie gets institutionalized.\r\n\r\n## Why Design Reviews Reward Optimism\r\n\r\nDesign reviews are supposed to find problems. In practice, they often do the opposite: they create consensus around the assumption that problems won't happen.\r\n\r\nHere's how it works:\r\n\r\nYou present a design. You show the nominal flow. You mark the error paths. You explain the redundancies. And then the questions come—but they come in a specific form: \"What if X fails?\" You answer with your contingency for X. \"What if Y happens?\" You show your handling of Y.\r\n\r\nEach question asked and answered creates the illusion that you've covered the space of possible failures. What's harder to see is the space of failures that weren't asked about—not because they're impossible, but because they're hard to imagine from a conference room.\r\n\r\nThe cellular configuration update that breaks connectivity for five days isn't a question anyone asks, because connectivity outages are supposed to last hours, not days. The sensor that doesn't fail cleanly but drifts slowly enough to stay within acceptance bounds for months isn't on the checklist, because sensors are supposed to either work or fail obviously. The operator who learns to bypass an interlock because it trips too often during valid operations isn't in the room, because we're reviewing technical design, not operational reality.\r\n\r\nDesign reviews optimize for demonstrable coverage of known failure modes. They reward the ability to show that you've thought about things. But \"thinking about things\" is different from designing for them, and both are different from experiencing them.\r\n\r\nThis creates a subtle bias: the designs that pass review most easily are the ones that look most robust on paper. Clean boundaries. Clear error handling. Documented assumptions. The designs that acknowledge fundamental uncertainties—\"we don't know how operators will actually use this,\" \"we can't predict how this will interact with the legacy system at scale\"—feel incomplete. They sound like excuses.\r\n\r\nSo we learn to present certainty. We learn to have answers. And slowly, the happy path becomes the only path we're willing to defend.\r\n\r\n## How Organizations Institutionalize Blind Spots\r\n\r\nIndividual engineers know the happy path is optimistic. But organizations have a way of turning individual caution into collective blindness.\r\n\r\nConsider what happens after that design review. The design is approved. It goes into a requirements document. The requirements become tasks. The tasks become sprints. And at each translation, something is lost.\r\n\r\nThe subtle caveat—\"this assumes network partitions are transient\"—becomes \"handles network failures.\" The hedge—\"buffering capacity should be sufficient for typical outages\"—becomes \"72-hour buffer.\" The uncertainty—\"we'll need to monitor how this behaves under load\"—becomes a checkbox: \"performance tested.\"\r\n\r\nThis isn't malice. It's how organizations create actionable plans from ambiguous reality. But in the process, assumptions get promoted to facts, and facts get encoded into architecture.\r\n\r\nWorse, once something is designed and built, it becomes expensive to question. Not just financially expensive—politically expensive. The team that built it has invested in it. Managers have reported progress on it. Customers have been promised it. To say \"we need to rethink this\" is to say all that investment might have been misdirected.\r\n\r\nSo instead, we add compensating controls. We build monitoring. We write runbooks. We train operators. Each addition reinforces the original design rather than questioning it. We're not asking \"should this system exist in this form?\" We're asking \"how do we make this system work?\"\r\n\r\nI've sat in meetings where everyone in the room privately knew a system was brittle, but no one said it directly because the system was already in production and replacing it would be a six-month project. Instead, we talked about \"hardening\" and \"resilience improvements\"—language that suggested we were making something robust rather than patching something fundamentally fragile.\r\n\r\nThe organization's immune system had learned to reject the observation that the system was designed wrong, because accepting that observation would require acknowledging that a lot of other decisions were also wrong.\r\n\r\n## Most Failures Are Designed\r\n\r\nHere's an uncomfortable truth: most production failures aren't accidents. They're not the result of bugs that slipped through testing or edge cases that no one thought of.\r\n\r\nThey're the inevitable outcome of decisions made under constraints.\r\n\r\nThat temperature monitoring system that lost 96 hours of data? The decision to use flash memory with limited write endurance was made to hit a cost target. The decision to buffer for 72 hours was made based on historical uptime data from a different cellular provider. The decision not to implement hierarchical buffering (edge → gateway → cloud) was made to keep the architecture simple and shippable.\r\n\r\nNone of those were wrong decisions in isolation. Given the constraints—budget, schedule, what was known at the time—they were defensible. Reasonable, even.\r\n\r\nBut they were decisions, not accidents. And decisions have consequences that aren't always visible until they compound.\r\n\r\nThis is what I mean when I say most failures are designed. Not that anyone set out to build a fragile system, but that fragility is often the natural result of optimizing for other things: speed, cost, simplicity, familiarity.\r\n\r\nThe difference between a bug and a decision is that bugs can be fixed. Decisions are encoded into the architecture. They become load-bearing assumptions. You can't fix them without rethinking the system.\r\n\r\nWhen an incident review concludes \"the system worked as designed, but we didn't anticipate this scenario,\" what they're really saying is: \"we made tradeoffs, and this is what we traded away.\"\r\n\r\nThe question is whether we're honest about what we're trading. Most of the time, we're not—because being honest would make it harder to get the design approved.\r\n\r\n## Why Test, Staging, and Simulation Always Mislead\r\n\r\nEvery environment before production is a curated experience.\r\n\r\nTest environments use clean data. Staging uses a subset of production scale. Simulations use models that abstract away complexity. Even load testing is fundamentally artificial—you generate the load, you choose when to apply it, you know what you're testing for.\r\n\r\nThis isn't a criticism of testing. Testing is essential. But it's also fundamentally limited, and we consistently underestimate how limited it is.\r\n\r\nHere's what test environments can't show you:\r\n\r\n**They can't show you emergent behavior.** That interaction between the VFD noise and the CAN bus that only manifests when both systems are under load and the ambient temperature is above 30°C? Your test bench runs at 22°C with one system at a time.\r\n\r\n**They can't show you operational reality.** The operator who learned that if you cycle the power on the HMI in a specific sequence, you can temporarily bypass an error condition that otherwise requires a maintenance window? They discovered that in production, under pressure, when the line manager was demanding a workaround.\r\n\r\n**They can't show you the full dependency graph.** You tested integration with the ERP system. You didn't test integration with the ERP system while the network team is doing maintenance and traffic is rerouting through a secondary path with higher latency and the backup MES is handling requests because the primary is being patched.\r\n\r\n**They can't show you time.** That sensor calibration drift that takes six months to become problematic? Your acceptance test runs for six hours.\r\n\r\n**They can't show you organizational dynamics.** Who gets called when something ambiguous happens at 2 AM? Who has the authority to make the call to shut down the line? Who actually knows where the documentation is? None of that exists in staging.\r\n\r\nProduction is the only environment where all the variables are real, all at once, without anyone curating the experience.\r\n\r\n## What Production Pressure Reveals That Design Never Does\r\n\r\nThere's a specific kind of knowledge that only emerges under production pressure, and it has nothing to do with technical design.\r\n\r\nIt's the knowledge of what actually matters.\r\n\r\nIn design, everything matters equally. Every requirement is important. Every failure mode is worth handling. Every dependency is documented. But in production, under time pressure, with costs accumulating, you discover very quickly what's truly critical and what was just conceptually important.\r\n\r\nYou discover that the \"critical\" alert that fires three times a day gets ignored, while the informal message in Slack from the night shift operator gets immediate attention because everyone knows that operator only speaks up when something is genuinely wrong.\r\n\r\nYou discover that the official escalation path—submit a ticket, wait for triage, get assigned to the on-call engineer—is too slow, and there's an unofficial path where certain people have certain phone numbers and that's what actually gets used when things are breaking.\r\n\r\nYou discover that the monitoring dashboard everyone insisted on building never gets looked at during incidents, but everyone opens the same three SSH sessions to check the same three log files because that's where the useful information actually lives.\r\n\r\nYou discover that the runbook that took weeks to write is useless because it assumes you have time to read it, and in production you don't—so people fall back on intuition, pattern matching, and educated guesses.\r\n\r\nNone of this is visible during design because design happens in an environment where time is less expensive and mistakes are reversible. Production pressure doesn't just reveal technical problems. It reveals organizational truth.\r\n\r\nIt shows you which abstractions hold up and which ones collapse. It shows you where authority actually lies versus where the org chart says it lies. It shows you which skills matter—and they're often not the ones that got people promoted.\r\n\r\n## What This Means for How We Design\r\n\r\nI'm not arguing that we should abandon design reviews or testing or staging environments. They serve a purpose. The problem is that we treat them as validation rather than exploration.\r\n\r\nWe treat passing a design review as evidence that the design is good. It's not. It's evidence that the design is defensible given what we know now, in this room, without production pressure.\r\n\r\nWe treat staging as a high-fidelity simulation of production. It's not. It's a curated environment that shares some properties with production and systematically excludes others.\r\n\r\nIf we were honest about this, we'd design differently. We'd spend less energy trying to anticipate every failure mode and more energy building systems that can be understood and modified when inevitably surprising failures occur.\r\n\r\nWe'd document not just what the system does, but what we assumed about the environment it operates in. Not as a CYA exercise, but as a genuine artifact that helps the next person understand what we were thinking—and where we were probably wrong.\r\n\r\nWe'd treat the first six months in production not as \"stabilization\" but as \"learning what we actually built\"—because that's what it is.\r\n\r\nThe happy path isn't useless. It's necessary to ship anything at all. But it's a lie we tell ourselves to make progress in the face of uncertainty.\r\n\r\nThe question is whether we remember it's a lie—or whether we start believing it.\r\n\r\nBecause the system will tell the truth eventually.\r\n\r\nIt always does."},{"slug":"the-anchor","title":"Every System Tells Its Truth in Production","date":"2025-11-01","excerpt":"Production strips away comfortable illusions. It's where assumptions stop being hypothetical and start charging interest—revealing not just what we built, but what we actually designed.","author":{"name":"Inderpreet Singh","avatar":"IS"},"image":"/images/posts/post-anchor-splash-0.png","tags":["systems","production","engineering","reliability"],"likes":0,"comments":0,"content":"<p>A control system runs a hydraulic system for eighteen months without incidence. Then one morning, the hydraulic fluid leaks out from a hose and the system looses pressure which causes the system response to be sluggish. No trips, No alarms fire. The control panel continues executing its cycle. But as the fluid reserviors drain out over the next few days, the system starts to misbehave, and operations begin to fail. The system is eventually down and maintenance is notified but the damage is done and operation is down resulting in costs.</p>\n<p>The control system worked exactly as designed. It just wasn&#39;t designed for this.</p>\n<h1>Every System Tells Its Truth in Production</h1>\n<p>Most systems look reasonable on the happy path. The designed path. These systems behave themselves in demos and pass the developer&#39;s tests and even survive design reviews. Afterall, the developers have the best intentions in mind. And then they reach production and their intended customers. That is when the conversation take a turn.</p>\n<p>Production isn&#39;t just an environment—it&#39;s a crucible. Real machinery, real physics, real latency, real operators, real consequences. It&#39;s where assumptions stop being hypothetical and start charging interest. Where the difference between 100ms and 200ms isn&#39;t academic—it&#39;s the difference between a controlled stop and an emergency shutdown.</p>\n<p>Every system tells its truth there.</p>\n<h1>The Comfortable Lie of the Happy Path</h1>\n<p>The happy path is a useful fiction - an essential posion if you may. We need it to get anything built at all and for the fundamental truth that it lets us describe the intent. A composite of requirements translated into user stories or fairy-tale descriptions of how the user will interact with the system. It lets us romance the reason for the desired behaviour and reassures us that when a checklist is all done, we can finally ship and raise a glass to a job well done.</p>\n<p>It starts with a concise set of words on a document and then evolves and over time, teams begin to mistake the happy path for reality. Diagrams become promises. Tests become guarantees. Architecture documents become a form of reassurance.</p>\n<p>What gets less attention is everything living just outside that path: sensors that drift over months rather than fail outright or even sensors installed so that they jittern instead of producing a constant output, networks that don&#39;t drop packets but add 50ms of jitter under load, operators who develop workarounds that bypass safety interlocks, physical processes that couple in ways the control model doesn&#39;t account for.</p>\n<p>These are not edge cases. <strong>They are the system.</strong></p>\n<h1>What “Truth” Actually Means</h1>\n<p>When I say a system tells its truth in production, I don&#39;t mean it suddenly becomes malicious or broken. I mean it becomes honest about what it actually is—not what we documented it to be.</p>\n<p>Think of it like this: in the lab, you&#39;re having a conversation with your system. You command a valve to open and it complies. You trigger an interlock and it responds. You&#39;re speaking to it in a controlled environment, with calibrated instruments, asking it controlled questions.</p>\n<p>Production is where the system starts talking back.</p>\n<p>It reveals what it actually depends on—not just the 24VDC supply you specified, but the quality of that supply under varying loads, the grounding scheme you inherited, the EMI from the VFD three panels over. It shows which failures it tolerates (a single missed Communication frame) and which it amplifies (a watchdog timeout that cascades into a full line stop). It exposes where responsibility is clear (who owns the PLC code) and where it diffuses (who decides when to replace aging sensors).</p>\n<p>And here&#39;s what makes this uncomfortable: production reveals not just technical design, but operational reality. Maintenance schedules that drift. Calibration records that age. Tribal knowledge about &quot;the weird thing it does on cold starts.&quot; Decision-making under the pressure of downtime costs.</p>\n<p>None of this is visible in a clean system architecture diagram.</p>\n<h1>“The System Worked Exactly as Designed”</h1>\n<p>This sentence appears often in incident reports, usually as a way to end a conversation. But it&#39;s almost always true—and deeply misunderstood.</p>\n<p>Most failures are not the result of a single fault. They are the natural outcome of tradeoffs made under budget constraints, complexity accumulated across multiple integration phases, and risk accepted implicitly because making it explicit would have delayed commissioning.</p>\n<p>The system worked exactly as designed. The problem is that the design included far more than firmware, software and wiring diagrams. It included unspoken assumptions about how operators would interact with HMIs, implicit bets about which sensors wouldn&#39;t drift simultaneously or would be installed correctly, and organizational patterns that made certain kinds of information invisible during shift handoffs.</p>\n<p>The control system with the leaky hydraulic fluid? It was designed to operate the hydraulics and ignore response time in favour of supporting different models of pumps. That was a conscious choice, documented and reviewed. What wasn&#39;t designed was the specific failure mode where a sluggish response of the mechanics, reported plausible-but-wrong values that fell within acceptance bounds, or the maintenance team&#39;s inability to correlate subtle performance degradation, or the absence of any pressure monitoring in the original specification.<br>The system told the truth about all of it.</p>\n<h1>Why Production Is Uncomfortable</h1>\n<p>Production removes the illusion of control.</p>\n<p>You no longer get to choose when failures occur, how visible they are, what else is happening simultaneously, or how much time you have to respond. It forces decisions to be made with incomplete telemetry, by fatigued operators, with production managers asking for ETAs.</p>\n<p>That is not a weakness of production. That is its value.</p>\n<p>This is where experience is formed—not by avoiding failures, but by learning what survives them. It&#39;s where you discover which of your redundancies were weight-bearing and which were decorative. It&#39;s where you learn the difference between a system that degrades gracefully and one that simply hasn&#39;t encountered the right combination of conditions yet.</p>\n<p>In embedded and IIoT systems, this matters more. You can&#39;t just roll back a deployment. You can&#39;t hotfix firmware on a system that&#39;s safety-critical without a maintenance window. Your &quot;users&quot; are 50-ton machines that cost $10,000 per hour of downtime. The feedback loop is measured in months, not minutes.</p>\n<h1>This Is Not a Blog About Tools</h1>\n<p>Tools matter. Microcontrollers matter. Protocols matter. Fieldbus architectures matter. Cloud architectures matter.</p>\n<p>But they are rarely the reason systems fail in production.</p>\n<p>Systems fail because graceful degradation was an afterthought, ownership boundaries were unclear between firmware and mechanical teams, incentives were misaligned between commissioning speed and long-term reliability, risks were known but unspoken during design reviews, and complexity grew faster than documentation—or understanding.</p>\n<p>Those are design problems. Judgment problems. Responsibility problems.</p>\n<p>That is what this space is about.</p>\n<h1>So Why This Exists?</h1>\n<p>Off the Happy Path is a collection of observations, essays, and stories from production systems—embedded firmware, IIoT platforms, control systems, and the organizations that build and operate them.</p>\n<p>It is not a tutorial blog. It will not explain fundamentals that datasheets and standards already cover well. It will not optimize for engagement or outrage.</p>\n<p>It exists to name things that are usually felt but not discussed. To talk about systems as they behave, not as we wish they would. To capture lessons that tend to surface only after something breaks—and sometimes only after we&#39;ve stopped to actually listen.</p>\n<h1>A Final Thought</h1>\n<p>If a system has never surprised you in production, one of two things is true: either it has not been in production long enough, or you were not listening closely when it spoke.</p>\n<p>Because eventually, every system does. It tells its truth.</p>\n<p>The question is whether we&#39;re ready to hear it.</p>\n","rawContent":"\r\nA control system runs a hydraulic system for eighteen months without incidence. Then one morning, the hydraulic fluid leaks out from a hose and the system looses pressure which causes the system response to be sluggish. No trips, No alarms fire. The control panel continues executing its cycle. But as the fluid reserviors drain out over the next few days, the system starts to misbehave, and operations begin to fail. The system is eventually down and maintenance is notified but the damage is done and operation is down resulting in costs.\r\n\r\nThe control system worked exactly as designed. It just wasn't designed for this.\r\n\r\n# Every System Tells Its Truth in Production\r\n\r\nMost systems look reasonable on the happy path. The designed path. These systems behave themselves in demos and pass the developer's tests and even survive design reviews. Afterall, the developers have the best intentions in mind. And then they reach production and their intended customers. That is when the conversation take a turn.\r\n\r\nProduction isn't just an environment—it's a crucible. Real machinery, real physics, real latency, real operators, real consequences. It's where assumptions stop being hypothetical and start charging interest. Where the difference between 100ms and 200ms isn't academic—it's the difference between a controlled stop and an emergency shutdown.\r\n\r\nEvery system tells its truth there.\r\n\r\n# The Comfortable Lie of the Happy Path\r\n\r\nThe happy path is a useful fiction - an essential posion if you may. We need it to get anything built at all and for the fundamental truth that it lets us describe the intent. A composite of requirements translated into user stories or fairy-tale descriptions of how the user will interact with the system. It lets us romance the reason for the desired behaviour and reassures us that when a checklist is all done, we can finally ship and raise a glass to a job well done.\r\n\r\nIt starts with a concise set of words on a document and then evolves and over time, teams begin to mistake the happy path for reality. Diagrams become promises. Tests become guarantees. Architecture documents become a form of reassurance.\r\n\r\nWhat gets less attention is everything living just outside that path: sensors that drift over months rather than fail outright or even sensors installed so that they jittern instead of producing a constant output, networks that don't drop packets but add 50ms of jitter under load, operators who develop workarounds that bypass safety interlocks, physical processes that couple in ways the control model doesn't account for.\r\n\r\nThese are not edge cases. **They are the system.**\r\n\r\n# What “Truth” Actually Means\r\n\r\nWhen I say a system tells its truth in production, I don't mean it suddenly becomes malicious or broken. I mean it becomes honest about what it actually is—not what we documented it to be.\r\n\r\nThink of it like this: in the lab, you're having a conversation with your system. You command a valve to open and it complies. You trigger an interlock and it responds. You're speaking to it in a controlled environment, with calibrated instruments, asking it controlled questions.\r\n\r\nProduction is where the system starts talking back.\r\n\r\nIt reveals what it actually depends on—not just the 24VDC supply you specified, but the quality of that supply under varying loads, the grounding scheme you inherited, the EMI from the VFD three panels over. It shows which failures it tolerates (a single missed Communication frame) and which it amplifies (a watchdog timeout that cascades into a full line stop). It exposes where responsibility is clear (who owns the PLC code) and where it diffuses (who decides when to replace aging sensors).\r\n\r\nAnd here's what makes this uncomfortable: production reveals not just technical design, but operational reality. Maintenance schedules that drift. Calibration records that age. Tribal knowledge about \"the weird thing it does on cold starts.\" Decision-making under the pressure of downtime costs.\r\n\r\nNone of this is visible in a clean system architecture diagram.\r\n\r\n# “The System Worked Exactly as Designed”\r\n\r\nThis sentence appears often in incident reports, usually as a way to end a conversation. But it's almost always true—and deeply misunderstood.\r\n\r\nMost failures are not the result of a single fault. They are the natural outcome of tradeoffs made under budget constraints, complexity accumulated across multiple integration phases, and risk accepted implicitly because making it explicit would have delayed commissioning.\r\n\r\nThe system worked exactly as designed. The problem is that the design included far more than firmware, software and wiring diagrams. It included unspoken assumptions about how operators would interact with HMIs, implicit bets about which sensors wouldn't drift simultaneously or would be installed correctly, and organizational patterns that made certain kinds of information invisible during shift handoffs.\r\n\r\nThe control system with the leaky hydraulic fluid? It was designed to operate the hydraulics and ignore response time in favour of supporting different models of pumps. That was a conscious choice, documented and reviewed. What wasn't designed was the specific failure mode where a sluggish response of the mechanics, reported plausible-but-wrong values that fell within acceptance bounds, or the maintenance team's inability to correlate subtle performance degradation, or the absence of any pressure monitoring in the original specification.\r\nThe system told the truth about all of it.\r\n\r\n# Why Production Is Uncomfortable\r\n\r\nProduction removes the illusion of control.\r\n\r\nYou no longer get to choose when failures occur, how visible they are, what else is happening simultaneously, or how much time you have to respond. It forces decisions to be made with incomplete telemetry, by fatigued operators, with production managers asking for ETAs.\r\n\r\nThat is not a weakness of production. That is its value.\r\n\r\nThis is where experience is formed—not by avoiding failures, but by learning what survives them. It's where you discover which of your redundancies were weight-bearing and which were decorative. It's where you learn the difference between a system that degrades gracefully and one that simply hasn't encountered the right combination of conditions yet.\r\n\r\nIn embedded and IIoT systems, this matters more. You can't just roll back a deployment. You can't hotfix firmware on a system that's safety-critical without a maintenance window. Your \"users\" are 50-ton machines that cost $10,000 per hour of downtime. The feedback loop is measured in months, not minutes.\r\n\r\n# This Is Not a Blog About Tools\r\n\r\nTools matter. Microcontrollers matter. Protocols matter. Fieldbus architectures matter. Cloud architectures matter.\r\n\r\nBut they are rarely the reason systems fail in production.\r\n\r\nSystems fail because graceful degradation was an afterthought, ownership boundaries were unclear between firmware and mechanical teams, incentives were misaligned between commissioning speed and long-term reliability, risks were known but unspoken during design reviews, and complexity grew faster than documentation—or understanding.\r\n\r\nThose are design problems. Judgment problems. Responsibility problems.\r\n\r\nThat is what this space is about.\r\n\r\n# So Why This Exists?\r\n\r\nOff the Happy Path is a collection of observations, essays, and stories from production systems—embedded firmware, IIoT platforms, control systems, and the organizations that build and operate them.\r\n\r\nIt is not a tutorial blog. It will not explain fundamentals that datasheets and standards already cover well. It will not optimize for engagement or outrage.\r\n\r\nIt exists to name things that are usually felt but not discussed. To talk about systems as they behave, not as we wish they would. To capture lessons that tend to surface only after something breaks—and sometimes only after we've stopped to actually listen.\r\n\r\n# A Final Thought\r\n\r\nIf a system has never surprised you in production, one of two things is true: either it has not been in production long enough, or you were not listening closely when it spoke.\r\n\r\nBecause eventually, every system does. It tells its truth.\r\n\r\nThe question is whether we're ready to hear it."}]},"__N_SSG":true}